# ==============================================================================
# EJERCICIO 1.1: CLASIFICACIÓN NO SUPERVISADA (K-MEANS)
# ==============================================================================

cat("\n\n********************************************************************\n")
cat(" EJERCICIO 1.1: CLUSTERING K-MEANS (CALIFICACIONES TEORÍA/LABORATORIO)\n")
cat("********************************************************************\n")

# ------------------------------------------------------------------------------
# 1. DEFINICIÓN DE LOS DATOS
# ------------------------------------------------------------------------------

# Se crea una matriz de 2 filas y 8 columnas con los datos proporcionados.
# Cada columna representa un estudiante:
#   - Fila 1: Nota de Teoría
#   - Fila 2: Nota de Laboratorio
m <- matrix(c(4, 4, 3, 5,
              1, 2, 5, 5,
              0, 1, 2, 2,
              4, 5, 2, 1),
            nrow = 2, ncol = 8)

cat(">> Matriz original (variables en filas, observaciones en columnas):\n")
print(m)

# Se transpone la matriz para que cada fila sea una observación
# y cada columna una variable (formato estándar para kmeans()).
m <- t(m)
cat("\n>> Matriz de datos tras la transposición (8 observaciones x 2 variables):\n")
print(m)

# ------------------------------------------------------------------------------
# 2. DEFINICIÓN DE LOS CENTROS INICIALES
# ------------------------------------------------------------------------------

# Se crea la matriz de centros iniciales (2 centros, 2 dimensiones).
# En este caso indicamos explícitamente dos puntos en el plano (Teoría, Laboratorio):
#   Centro 1: (0, 1)
#   Centro 2: (2, 2)
c <- matrix(c(0, 1,
              2, 2),
            nrow = 2, ncol = 2)

cat("\n>> Matriz de centros iniciales (antes de transponer):\n")
print(c)

# Se transpone la matriz de centros para que cada fila sea un centro.
c <- t(c)
cat("\n>> Centros iniciales usados por kmeans() (cada fila = un centro):\n")
print(c)

# ------------------------------------------------------------------------------
# 3. EJECUCIÓN DEL ALGORITMO K-MEANS
# ------------------------------------------------------------------------------

cat("\n>> Ejecutando kmeans() con 2 clústeres e iter.max = 4...\n")

# Ejecución de k-means usando:
#   - x = m         : matriz de datos (8 observaciones x 2 variables)
#   - centers = c   : matriz con los 2 centros iniciales
#   - iter.max = 4  : límite máximo de iteraciones del algoritmo
clasificacionss <- kmeans(m, centers = c, iter.max = 4)

cat("\n>> Resumen del objeto devuelto por kmeans():\n")
print(clasificacionss)

cat("\n>> Vector de pertenencia a clústeres (clasificacionss$cluster):\n")
print(clasificacionss$cluster)

cat("\n>> Centros finales de los clústeres (clasificacionss$centers):\n")
print(clasificacionss$centers)

# ------------------------------------------------------------------------------
# 4. AÑADIR LA ASIGNACIÓN DE CLÚSTER A LOS DATOS
# ------------------------------------------------------------------------------

# Se añade la columna con el número de clúster asignado a cada observación.
# El resultado es una matriz donde:
#   - Columna 1: número de clúster (1 o 2)
#   - Columnas 2 y 3: notas de Teoría y Laboratorio.
m <- cbind(clasificacionss$cluster, m)

cat("\n>> Datos con la columna de clúster añadida:\n")
colnames(m) <- c("Cluster", "Teoria", "Laboratorio")
print(m)

# ------------------------------------------------------------------------------
# 5. SEPARACIÓN DE LOS DATOS POR CLÚSTER
# ------------------------------------------------------------------------------

# Se filtran las observaciones del clúster 1
mc1 <- subset(m, m[, 1] == 1)
cat("\n>> Observaciones pertenecientes al clúster 1 (con columna de clúster):\n")
print(mc1)

# Se filtran las observaciones del clúster 2
mc2 <- subset(m, m[, 1] == 2)
cat("\n>> Observaciones pertenecientes al clúster 2 (con columna de clúster):\n")
print(mc2)

# ------------------------------------------------------------------------------
# 6. ELIMINAR LA COLUMNA DE CLÚSTER (DEJAR SOLO LAS COORDENADAS ORIGINALES)
# ------------------------------------------------------------------------------

# Se eliminan las columnas de clúster (dejamos solo las coordenadas originales).
mc1 <- mc1[, -1]
mc2 <- mc2[, -1]

cat("\n>> Clúster 1 (solo coordenadas Teoría/Laboratorio):\n")
print(mc1)

cat("\n>> Clúster 2 (solo coordenadas Teoría/Laboratorio):\n")
print(mc2)

# ==============================================================================
# EJERCICIO 1.3: CLASIFICACIÓN SUPERVISADA (Árboles de Decisión) - Guiado
# ==============================================================================
cat("\n\n********************************************************************\n")
cat(" EJERCICIO 1.3: ÁRBOLES DE DECISIÓN (CALIFICACIONES)\n")
cat("********************************************************************\n")

# if(!require(rpart)) install.packages("rpart")

# 1. Carga de Librería
library(rpart)

# 2. Carga de Datos

# Creación manual 
calificaciones <- data.frame(
  Teoria = c("A", "A", "D", "D", "B", "C", "B", "C", "B"),
  Laboratorio = c("A", "B", "D", "D", "C", "B", "B", "D", "A"),
  Practicas = c("B", "D", "C", "A", "B", "B", "A", "C", "C"),
  C.G = c("Ap", "Ss", "Ss", "Ss", "Ss", "Ap", "Ap", "Ss", "Ss")
)

cat(">> Datos de entrenamiento:\n")
print(calificaciones)

# 3. Entrenamiento del Modelo
# Usamos rpart.control(minsplit = 1) para que el árbol crezca incluso con pocos datos,
# tal como se hizo en clase para ver el ajuste completo.
clasificacion <- rpart(C.G ~ ., 
                       data = calificaciones, 
                       method = "class", 
                       control = rpart.control(minsplit = 1))

# 4. Resultados
cat("\n>> Detalles del Árbol Generado:\n")
print(clasificacion)

# ==============================================================================
# Ejercicio 2.2: CLUSTERING JERÁRQUICO: MIN, MAX, AVG Y EFECTO DEL JITTER
# ==============================================================================

# ------------------------------------------------------------------------------
# CARGA DE LIBRERÍA Y DEFINICIÓN DE LOS DATOS
# ------------------------------------------------------------------------------

# Se carga la librería que contiene la función agglomerative_clustering
# install.packages("UAHDataScienceUC")  # Descomentar si no está instalado
library(UAHDataScienceUC)

# Se define una matriz con 15 observaciones bidimensionales (15 filas, 2 columnas).
# Cada fila representa un punto en el plano (x, y).
datos <- matrix(c(
  3.5, 4.5,
  0.75, 3.25,
  0, 3,
  1.75, 0.75,
  3, 3.75,
  3.75, 4.5,
  1.25, 0.75,
  0.25, 3,
  3.5, 4.25,
  1.5, 0.5,
  1, 1,
  3, 4,
  0.5, 3,
  2, 0.25,
  0, 2.5
), ncol = 2, byrow = TRUE)

# Se asignan nombres de fila "1", "2", ..., "15" para identificar fácilmente
# cada observación en salidas y gráficos.
rownames(datos) <- as.character(1:nrow(datos))

# ------------------------------------------------------------------------------
# GENERACIÓN DE JITTER (RUIDO PEQUEÑO) SOBRE LOS DATOS
# ------------------------------------------------------------------------------

# Se fija la semilla para que la generación de ruido sea reproducible
# (cada ejecución con esta semilla produce el mismo ruido).
set.seed(123)

# Se crea una matriz de ruido del mismo tamaño que 'datos',
# con valores uniformes entre -0.03 y 0.03 en cada coordenada.
ruido <- matrix(runif(length(datos), -0.03, 0.03), ncol = 2)

# Se suman los pequeños ruidos a los datos originales, obteniendo 'datos_jitter'.
# Esto simula jitter: ligeras perturbaciones de la posición de cada punto.
datos_jitter <- datos + ruido

# ============================================
# MIN (single-link)
# ============================================

# ------------------------------------------------------------------------------
# CLUSTERING JERÁRQUICO CON ENLACE MÍNIMO (SIN JITTER)
# ------------------------------------------------------------------------------

# Se aplica clustering jerárquico aglomerativo con:
# - data = datos (puntos originales)
# - proximity = "single" (enlace mínimo: distancia entre clústeres
#   definida como la mínima distancia entre pares de puntos de ambos clústeres)
# - distance_method = "euclidean" (distancia euclídea)
# - learn y waiting se dejan para controlar el comportamiento interno de la función.
cl_min_original <- agglomerative_clustering(
  data = datos,
  proximity = "single",
  distance_method = "euclidean",
  learn = TRUE,
  waiting = FALSE
)

# ------------------------------------------------------------------------------
# CLUSTERING JERÁRQUICO CON ENLACE MÍNIMO (CON JITTER)
# ------------------------------------------------------------------------------

# Se repite el mismo procedimiento, pero sobre los datos perturbados 'datos_jitter'.
# Esto permite que grupos de 3 o más valores con la misma proximidad puedan ser
# distinguidos los clusters que se vayan formando.
cl_min_jitter <- agglomerative_clustering(
  data = datos_jitter,
  proximity = "single",
  distance_method = "euclidean",
  learn = TRUE,
  waiting = FALSE
)

# ------------------------------------------------------------------------------
# OBTENCIÓN DE PARTICIONES EN 3 CLÚSTERES (MIN)
# ------------------------------------------------------------------------------

# Se convierte el objeto devuelto por agglomerative_clustering a un objeto 'hclust'
# y se corta el dendrograma en k = 3 clústeres para los datos originales.
# El resultado es un vector que indica a qué clúster pertenece cada observación.
cutree(as.hclust(cl_min_original), k = 3)

# Se hace lo mismo para el clustering con jitter, permitiendo comparar
# las asignaciones de clúster entre la versión original y la perturbada.
# De esta forma se puede visualizar si el ruido añadido a los datos ha perjudicado
# el resultado esperado. Para que todo esté correcto, los datos deben continuar 
# estando en el cluster que se obtuvo para los datos originales.
cutree(as.hclust(cl_min_jitter),   k = 3)

# ============================================
# MAX (complete-link)
# ============================================

# ------------------------------------------------------------------------------
# CLUSTERING JERÁRQUICO CON ENLACE MÁXIMO (SIN JITTER)
# ------------------------------------------------------------------------------

# Se aplica clustering jerárquico con:
# - proximity = "complete" (enlace completo: la distancia entre clústeres
#   es la mayor distancia entre pares de puntos de ambos clústeres).
# Este criterio tiende a formar clústeres más compactos y bien separados,
# menos sensibles a cadenas de puntos que el single-link.
cl_max_original <- agglomerative_clustering(
  data = datos,
  proximity = "complete",
  distance_method = "euclidean",
  learn = TRUE,
  waiting = FALSE
)

# ============================================
# AVG (average-link)
# ============================================

# ------------------------------------------------------------------------------
# CLUSTERING JERÁRQUICO CON ENLACE PROMEDIO (SIN JITTER)
# ------------------------------------------------------------------------------

# Se aplica clustering jerárquico con:
# - proximity = "average" (enlace promedio: la distancia entre clústeres
#   es la media de las distancias entre todos los pares de puntos de ambos clústeres).
# Este método suele dar un compromiso entre single-link y complete-link,
# generando clústeres ni tan encadenados ni tan compactos como complete-link.
cl_avg_original <- agglomerative_clustering(
  data = datos,
  proximity = "average",
  distance_method = "euclidean",
  learn = TRUE,
  waiting = FALSE
)

# ============================================
# VISUALIZACIÓN DE LOS DENDROGRAMAS
# ============================================

# ------------------------------------------------------------------------------
# PREPARACIÓN DEL DISPOSITIVO GRÁFICO
# ------------------------------------------------------------------------------

# Se cierra cualquier dispositivo gráfico abierto previamente,
# para evitar conflictos con nuevas ventanas de gráficos.
dev.off()

# ------------------------------------------------------------------------------
# DENDROGRAMA PARA MAX (complete-link)
# ------------------------------------------------------------------------------

# Se abre una nueva ventana gráfica (en Windows) para mostrar el dendrograma
# del clustering con enlace completo sobre los datos originales.
windows()
plot(as.hclust(cl_max_original), main = "MAX (complete-link)")

# ------------------------------------------------------------------------------
# DENDROGRAMA PARA AVG (average-link)
# ------------------------------------------------------------------------------

# Se abre otra ventana gráfica para el dendrograma de enlace promedio
# con los datos originales.
windows()
plot(as.hclust(cl_avg_original), main = "AVG (average-link)")

# ------------------------------------------------------------------------------
# DENDROGRAMA PARA MIN (single-link) CON JITTER
# ------------------------------------------------------------------------------

# Finalmente, se abre una tercera ventana gráfica para mostrar el dendrograma
# del clustering con enlace mínimo aplicado a los datos con jitter.
windows()
plot(as.hclust(cl_min_jitter), main = "MIN (single-link) con jitter")



# ==============================================================================
# EJERCICIO 2.3: CLASIFICACIÓN SUPERVISADA MANUAL (Árboles de Decisión)
# Implementación del algoritmo de partición recursiva usando Índice de Gini.
# ==============================================================================
cat("\n\n********************************************************************\n")
cat(" EJERCICIO 2.3: ÁRBOL DE DECISIÓN MANUAL (VEHÍCULOS)\n")
cat("********************************************************************\n")

# --- 1. Definición del Dataset ---
# Datos de entrenamiento (10 observaciones)
datos_vehiculos <- data.frame(
  Carnet = c("B", "A", "N", "B", "B", "B", "N", "B", "B", "N"),
  Ruedas = c(4, 2, 2, 6, 4, 4, 2, 2, 6, 2),
  Pasajeros = c(5, 2, 1, 4, 6, 4, 2, 1, 2, 1),
  Vehiculo = c("Coche", "Moto", "Bicicleta", "Camion", "Coche", "Coche", 
               "Bicicleta", "Moto", "Camion", "Bicicleta"),
  stringsAsFactors = FALSE
)

print(">> Conjunto de datos inicial:")
print(datos_vehiculos)

# ------------------------------------------------------------------------------
# FUNCIONES AUXILIARES (Lógica del Algoritmo)
# ------------------------------------------------------------------------------

# Función: Calcular Impureza Gini de un conjunto de etiquetas
# Gini(S) = 1 - sum(p_i^2)
calcular_gini <- function(clases) {
  n <- length(clases)
  if (n == 0) return(0)
  
  conteos <- table(clases)
  proporciones <- conteos / n
  
  gini <- 1 - sum(proporciones^2)
  return(gini)
}

# Función: Evaluar Ganancia de Información de un atributo
# Ganancia = Gini(Padre) - Gini_Ponderado(Hijos)
evaluar_division <- function(datos, atributo, clase_objetivo) {
  valores <- unique(datos[[atributo]])
  n_total <- nrow(datos)
  
  # 1. Impureza del nodo actual (Padre)
  gini_inicial <- calcular_gini(datos[[clase_objetivo]])
  gini_ponderado <- 0
  
  cat(sprintf("\n   [Analizando atributo: %s]\n", atributo))
  
  # 2. Calcular impureza ponderada de los hijos
  for (val in valores) {
    subconjunto <- datos[datos[[atributo]] == val, ]
    n_sub <- nrow(subconjunto)
    gini_sub <- calcular_gini(subconjunto[[clase_objetivo]])
    
    gini_ponderado <- gini_ponderado + (n_sub / n_total) * gini_sub
    
    clases_rama <- paste(unique(subconjunto[[clase_objetivo]]), collapse=",")
    cat(sprintf("     -> Rama '%s' (n=%d): Clases={%s} | Gini=%.3f\n", 
                val, n_sub, clases_rama, gini_sub))
  }
  
  # 3. Ganancia
  ganancia <- gini_inicial - gini_ponderado
  cat(sprintf("   => Ganancia Total: %.3f (Gini Ponderado: %.3f)\n", 
              ganancia, gini_ponderado))
  
  return(ganancia)
}

# ------------------------------------------------------------------------------
# CONSTRUCCIÓN DEL ÁRBOL (Ejecución Paso a Paso)
# ------------------------------------------------------------------------------

# --- PASO 1: NODO RAÍZ ---
cat("\n\n=== PASO 1: SELECCIÓN DEL NODO RAÍZ ===\n")
# Evaluamos todos los atributos candidatos
g_carnet <- evaluar_division(datos_vehiculos, "Carnet", "Vehiculo")
g_ruedas <- evaluar_division(datos_vehiculos, "Ruedas", "Vehiculo")
g_pasajeros <- evaluar_division(datos_vehiculos, "Pasajeros", "Vehiculo")

cat("\n>> DECISIÓN PASO 1: El atributo 'Ruedas' tiene la mayor ganancia (0.500).\n")
cat("   Estructura actual:\n")
cat("   - Ruedas = 4: Coche (Gini 0.0 -> Hoja)\n")
cat("   - Ruedas = 6: Camion (Gini 0.0 -> Hoja)\n")
cat("   - Ruedas = 2: Mezcla {Moto, Bicicleta} (Gini > 0 -> DIVIDIR)\n")


# --- PASO 2: SUBDIVISIÓN (Rama Ruedas = 2) ---
cat("\n\n=== PASO 2: ANÁLISIS RAMA 'RUEDAS = 2' ===\n")

# Filtramos el dataset
datos_ruedas2 <- datos_vehiculos[datos_vehiculos$Ruedas == 2, ]
print("Subconjunto de datos (Ruedas=2):")
print(datos_ruedas2)

# Evaluamos atributos restantes
g2_carnet <- evaluar_division(datos_ruedas2, "Carnet", "Vehiculo")
g2_pasajeros <- evaluar_division(datos_ruedas2, "Pasajeros", "Vehiculo")

cat("\n>> DECISIÓN PASO 2: El atributo 'Carnet' tiene la mayor ganancia.\n")
cat("   Separa perfectamente las clases restantes:\n")
cat("   - Carnet A -> Moto\n")
cat("   - Carnet B -> Moto\n")
cat("   - Carnet N -> Bicicleta\n")

# ------------------------------------------------------------------------------
# RESUMEN FINAL
# ------------------------------------------------------------------------------
cat("\n==========================================================\n")
cat(" ÁRBOL DE DECISIÓN FINAL (GENERADO MANUALMENTE)\n")
cat("==========================================================\n")
cat("1. ¿Ruedas?\n")
cat("   |-- 4: Coche\n")
cat("   |-- 6: Camion\n")
cat("   |-- 2: ¿Tipo de Carnet?\n")
cat("       |-- A: Moto\n")
cat("       |-- B: Moto\n")
cat("       |-- N: Bicicleta\n")
cat("==========================================================\n")