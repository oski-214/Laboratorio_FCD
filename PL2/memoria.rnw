%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PREÁMBULO DE LATEX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper]{article}

% --- Paquetes Esenciales ---
\usepackage[utf8]{inputenc} % Codificación de entrada
\usepackage[T1]{fontenc}    % Codificación de fuentes
\usepackage[spanish]{babel} % Idioma español
\usepackage{graphicx}       % Para insertar imágenes
\usepackage[a4paper,margin=2.5cm]{geometry} % Márgenes
\usepackage{Sweave}         % Estilos para Sweave

% --- Paquetes Adicionales Útiles ---
\usepackage{amsmath}        % Mejoras para matemáticas
\usepackage{amsfonts}       % Fuentes matemáticas
\usepackage{hyperref}       % Links clicables
\usepackage{float}          % Controlar posición de flotantes (figuras, tablas)
\usepackage{csquotes}       % Para citas textuales
\usepackage[backend=biber,style=ieee]{biblatex} % Bibliografía

\setlength{\parskip}{\baselineskip} % Un espacio entre párrafos

% \addbibresource{bibliografia.bib} % Descomenta esto y crea tu archivo .bib

% --- Configuración del Documento ---
\title{Memoria: Memoria PL2 - Fundamentos de la Ciencia de Datos}
\author{Oscar Morán, Asier Álamo, Edgar Alexis Conforme, Lucía Díaz}
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INICIO DEL DOCUMENTO
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}


% --- Portada ---
\begin{titlepage}
    \centering
    \vspace*{\fill}
    {\LARGE Universidad de Alcalá \\[0.5cm]}
    {\Large Escuela Politécnica Superior \\[1cm]}
    {\Huge \textbf{PL2 - Fundamentos de la Ciencia de Datos} \\[1.5cm]}
    \Large\textbf{Autor:} \\[0.3cm]
    {\Large Oscar Morán, Asier Álamo, Edgar Alexis Conforme, Lucía Díaz \\[2cm]}
    \textbf{Fecha:} \\[0.3cm]
    {\Large \today} \\[1.5cm]
    \vspace*{\fill}
\end{titlepage}


\newpage
\tableofcontents % Tabla de contenido automática
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ejercicio 1.1: Clasificación no supervisada (K-Means)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Descripción del conjunto de datos}

El conjunto de datos está formado por \textbf{8 observaciones}, cada una de ellas correspondiente a un estudiante. Para cada estudiante se registran dos calificaciones numéricas:

\begin{itemize}
  \item \textbf{Teoría}: nota obtenida en la parte teórica de la asignatura.
  \item \textbf{Laboratorio}: nota obtenida en las prácticas de laboratorio.
\end{itemize}

Ambas variables son cuantitativas (en una escala discreta de 0 a 5) y se tratan como coordenadas de un punto en el plano bidimensional \((\text{Teoría}, \text{Laboratorio})\). De este modo, cada estudiante queda representado por un par ordenado que permite aplicar directamente técnicas de clustering en \(\mathbb{R}^2\).

Las 8 observaciones utilizadas en el ejercicio son las siguientes:

\[
\{(4,4), (3,5), (1,2), (5,5), (0,1), (2,2), (4,5), (2,1)\}
\]

En la implementación en R, estos datos se almacenan inicialmente en una matriz de dimensión \(2 \times 8\), donde cada columna representa un estudiante y cada fila una de las dos variables (\textit{Teoría} y \textit{Laboratorio}). Posteriormente, la matriz se transpone para obtener una estructura de \(8 \times 2\), en la que cada fila corresponde a un estudiante y cada columna a una variable, formato estándar para aplicar el algoritmo \texttt{kmeans()}.

El propósito del ejercicio es agrupar automáticamente a los estudiantes en clústeres de rendimiento similares, basándose únicamente en sus calificaciones de Teoría y Laboratorio, sin utilizar información de etiquetas previas (clasificación no supervisada).


\subsection{Fundamento teórico}

El objetivo del ejercicio es aplicar un método de \textbf{clasificación no supervisada} para agrupar observaciones similares sin emplear una variable clase conocida.

\paragraph{Concepto de clustering por particionado.}
Los métodos de particionado buscan dividir un conjunto de $n$ observaciones en $K$ grupos disjuntos (clústeres), de forma que:

\begin{itemize}
  \item Las observaciones dentro de un mismo clúster sean lo más \textbf{similares} posible.
  \item Las observaciones pertenecientes a clústeres distintos sean lo más \textbf{diferentes} posible.
\end{itemize}

En el caso de K-Means, la similitud se mide mediante la \textbf{distancia euclídea} y cada clúster se representa por su \textbf{centroide}, que es el vector medio de todas las observaciones asignadas a ese clúster.

\paragraph{Algoritmo K-Means.}
El algoritmo opera iterativamente siguiendo una serie de pasos:

\begin{enumerate}
  \item \textbf{Inicialización}: Se seleccionan $K$ centros iniciales. En este ejercicio, los centros se proporcionan explícitamente en forma de matriz.
  \item \textbf{Asignación}: Cada observación se asigna al clúster cuyo centroide esté más próximo en distancia euclídea.
  \item \textbf{Actualización}: Para cada clúster, se recalcula su centroide como la media de todas las observaciones asignadas a él.
  \item \textbf{Iteración}: Se repiten los pasos de asignación y actualización hasta que los centroides dejan de cambiar de manera significativa o se alcanza el número máximo de iteraciones.
\end{enumerate}

\paragraph{Puntos clave del algoritmo.}
\begin{itemize}
  \item K-Means siempre produce una partición en exactamente $K$ clústeres, a diferencia de los métodos jerárquicos.
  \item La elección de los centros iniciales puede influir en el resultado final.
  \item El algoritmo siempre converge, aunque no necesariamente al óptimo global.
  \item Es especialmente adecuado para datos numéricos en espacios métricos como $\mathbb{R}^2$.
\end{itemize}

\subsection{Implementación en R y resultados}

A continuación se presenta el código utilizado para aplicar el algoritmo K-Means al conjunto de datos. Todas las funciones empleadas pertenecen al paquete base \texttt{stats}, cargado automáticamente por R.

<<ejercicio1_2_kmeans, echo=TRUE, results=verbatim>>=
# Se crea una matriz de 2 filas y 8 columnas con los datos proporcionados
m <- matrix(c(4,4,3,5,
              1,2,5,5,
              0,1,2,2,
              4,5,2,1), 
            2, 8)

# Se transpone la matriz para que cada fila sea una observación
(m <- t(m))

# Se crea la matriz de centros iniciales (2 centros, 2 dimensiones)
c <- matrix(c(0,1,
              2,2), 
            2, 2)

# Se transponen los centros (cada fila será un centro)
(c <- t(c))

# Ejecución de k-means usando:
# - m como datos
# - c como centros iniciales (2 clusters)
# - iter.max = 4 (el límite de iteraciones)
(clasificacionss <- kmeans(m, c, 4))

# Se añade la columna con el número de cluster asignado a cada observación
m <- cbind(clasificacionss$cluster, m)

# Se filtran las observaciones del cluster 1
mc1 <- subset(m, m[,1] == 1)
mc1

# Se filtran las observaciones del cluster 2
mc2 <- subset(m, m[,1] == 2)
mc2

# Se elimina la columna del cluster (dejamos solo los valores originales)
(mc1 <- mc1[, -1])
(mc2 <- mc2[, -1])
@

\paragraph{Explicación detallada de las funciones y parámetros utilizados.}

\begin{itemize}

\item \textbf{\texttt{matrix(data, nrow, ncol)}}  
Construye una matriz a partir de un vector. En este ejercicio:

\begin{itemize}
  \item \textbf{\texttt{data}} contiene las calificaciones de 8 estudiantes.
  \item \textbf{\texttt{nrow = 2}} indica que hay dos variables (Teoría, Laboratorio).
  \item \textbf{\texttt{ncol = 8}} indica que hay 8 observaciones.
\end{itemize}

R rellena la matriz por columnas, por lo que posteriormente es necesario transponerla.

\item \textbf{\texttt{t(x)}}  
Devuelve la transpuesta de la matriz.  
Se utiliza para que cada fila represente una observación, formato requerido por \texttt{kmeans()}.

\item \textbf{\texttt{kmeans(x, centers, iter.max)}} — Paquete \texttt{stats}  

Aplica el algoritmo K-Means de particionado sobre un conjunto de datos numéricos.

\begin{itemize}
  \item \textbf{\texttt{x = m}}  
  Es el objeto que contiene los datos. En nuestro caso:
  \begin{itemize}
    \item \texttt{m} es una \textbf{matriz numérica} de dimensión \(8 \times 2\).
    \item Cada \textbf{fila} representa una observación (un estudiante).
    \item Cada \textbf{columna} representa una variable (\textit{Teoría} y \textit{Laboratorio}).
  \end{itemize}
  La función calcula distancias euclídeas entre filas de \texttt{m} para decidir a qué clúster pertenece cada observación.

  \item \textbf{\texttt{centers = c}}  
  Controla cómo se inicializan los centroides. Puede ser:
  \begin{itemize}
    \item Un \textbf{número entero} \(K\): R elige aleatoriamente \(K\) filas de \texttt{x} como centros iniciales.
    \item Una \textbf{matriz numérica} con \(K\) filas: cada fila se toma como centro inicial de un clúster.
  \end{itemize}
  En este ejercicio usamos la segunda opción:
  \begin{itemize}
    \item \texttt{c} es una matriz de dimensión \(2 \times 2\).
    \item Cada fila de \texttt{c} es un centro inicial en el plano \((\text{Teoría}, \text{Laboratorio})\).
  \end{itemize}
  Esto hace que el algoritmo sea \textbf{determinista}: siempre empieza en los mismos puntos y, por tanto, siempre produce la misma partición.

  \item \textbf{\texttt{iter.max = 4}}  
  Es el \textbf{número máximo de iteraciones} del algoritmo.  
  Cada iteración realiza dos pasos:
  \begin{enumerate}
    \item Reasignar cada observación al centro más cercano (en distancia euclídea).
    \item Recalcular los centroides como la media de las observaciones de cada clúster.
  \end{enumerate}
  El algoritmo se detiene cuando:
  \begin{itemize}
    \item los centros dejan de cambiar (o cambian muy poco), o
    \item se ha alcanzado el límite de \texttt{iter.max}.
  \end{itemize}
  En nuestro caso, el algoritmo converge antes de llegar a 4 iteraciones, lo que indica que la partición es estable.
\end{itemize}

La función devuelve un objeto de clase \texttt{"kmeans"}, que en realidad es una \textbf{lista} con varios componentes. Los más relevantes en este ejercicio son:

\begin{itemize}
  \item \textbf{\texttt{\$cluster}}  
  Vector de longitud igual al número de observaciones (\(n = 8\)).  
  Para cada fila de \texttt{m}, indica el número de clúster al que ha sido asignada (1, 2, ..., K).  
  En nuestro ejercicio, el vector:
  \[
    (2,\ 2,\ 1,\ 2,\ 1,\ 1,\ 2,\ 1)
  \]
  codifica la pertenencia de cada estudiante a uno de los dos clústeres.

  \item \textbf{\texttt{\$centers}}  
  Matriz de dimensión \(K \times p\), donde:
  \begin{itemize}
    \item \(K\) es el número de clústeres (aquí 2).
    \item \(p\) es el número de variables (aquí 2: Teoría y Laboratorio).
  \end{itemize}
  Cada fila es el \textbf{centroide final} de un clúster, es decir, el vector de medias de las observaciones asignadas a ese clúster.  
  En nuestro caso:
  \[
  C_1 = (1.25,\ 1.50), \qquad C_2 = (4.00,\ 4.75)
  \]
  resumen el rendimiento “medio” de los estudiantes de cada grupo.

  \item \textbf{\texttt{\$size}}  
  Vector de longitud \(K\).  
  Cada componente indica cuántas observaciones han quedado en cada clúster.  
  Aquí obtenemos tamaños \((4, 4)\): ambos clústeres contienen exactamente 4 estudiantes.

  \item \textbf{\texttt{\$withinss}}  
  Vector de longitud \(K\) con la \textbf{suma de cuadrados intra-clúster} de cada clúster.  
  Para cada grupo, se calculan las distancias al cuadrado entre cada punto y su centroide, y se suman. Valores bajos indican clústeres \textbf{compactos}.  
  En el resultado:
  \[
    WSS_1 = 3.75,\quad WSS_2 = 2.75
  \]
  lo que refleja que, en ambos grupos, los puntos están bastante próximos a sus respectivos centroides.

  \item \textbf{\texttt{\$totss}}  
  Suma de cuadrados \textbf{total} del conjunto de datos respecto al centroide global (media de todas las observaciones sin agrupar).  
  Representa la variabilidad total inicial de los datos antes de hacer clustering.

  \item \textbf{\texttt{\$tot.withinss}}  
  Suma de las \texttt{\$withinss} de todos los clústeres:
  \[
    \texttt{tot.withinss} = \sum_{k=1}^{K} \texttt{withinss[k]}
  \]
  Es la variabilidad que todavía queda \textbf{dentro} de los clústeres tras la partición.

  \item \textbf{\texttt{\$betweenss}}  
  Suma de cuadrados \textbf{entre clústeres}. Se cumple:
  \[
    \texttt{totss} = \texttt{tot.withinss} + \texttt{betweenss}
  \]
  Cuanto mayor es \texttt{\$betweenss} (en relación con \texttt{\$totss}), mejor separadas están las medias de los clústeres.  
  En nuestro resultado, la razón:
  \[
    \frac{\texttt{betweenss}}{\texttt{totss}} = 84.8\%
  \]
  indica que casi el 85\% de la variabilidad total se explica por la separación entre los dos clústeres, lo que confirma que la partición encontrada es muy buena.
\end{itemize}

\item \textbf{\texttt{cbind(a, b, ...)}}  
Une objetos por columnas.  
Permite añadir la asignación de clúster como primera columna de la matriz:

\[
m = \begin{bmatrix}
\text{cluster} & \text{Teoría} & \text{Laboratorio}
\end{bmatrix}
\]

\item \textbf{\texttt{subset(x, condition)}}  
Filtra filas que cumplen una condición.  
Aquí se usa para extraer las observaciones de cada clúster:

\begin{verbatim}
subset(m, m[,1] == 1)
\end{verbatim}

\item \textbf{\texttt{[, -1]}}  
Indexación para eliminar la primera columna (la etiqueta de clúster), dejando solo las dos coordenadas originales.

\end{itemize}

\paragraph{Salida obtenida en R.}

El algoritmo produce la siguiente partición: dos clústeres de tamaño 4 cada uno, con centroides:

\[
C_1 = (1.25,\ 1.50), \qquad C_2 = (4.00,\ 4.75)
\]

El resultado incluye el vector de asignación:

\[
(2,\ 2,\ 1,\ 2,\ 1,\ 1,\ 2,\ 1)
\]

y las observaciones agrupadas quedan:

\[
\text{Clúster 1: } \{(1,2), (0,1), (2,2), (2,1)\}
\]
\[
\text{Clúster 2: } \{(4,4), (3,5), (5,5), (4,5)\}
\]

\subsection{Análisis e interpretación de los resultados}

Tras ejecutar el algoritmo K-Means con los centros iniciales especificados, se obtiene una partición final formada por \textbf{2 clústeres de igual tamaño} (4 observaciones cada uno). El algoritmo converge antes de alcanzar el límite de \texttt{iter.max = 4}, señal de que la separación entre grupos está claramente definida.

\paragraph{Centroides finales.}

Los centros obtenidos al finalizar el proceso son:

\[
C_1 = (1.25,\ 1.50), \qquad
C_2 = (4.00,\ 4.75)
\]

Estos centroides resumen el “punto medio” de cada agrupación. Puede observarse que:

\begin{itemize}
    \item \(C_1\) se sitúa en la región de valores bajos de ambas variables.
    \item \(C_2\) aparece claramente desplazado hacia valores altos.
\end{itemize}

Por tanto, el algoritmo ha identificado \textbf{dos grupos muy separados} en el espacio bidimensional: uno correspondiente a observaciones ``pequeñas'' y otro a observaciones ``grandes''.

\paragraph{Asignación de observaciones a los clústeres.}

El vector de pertenencia devuelto por \texttt{kmeans()} es:

\[
(2,\ 2,\ 1,\ 2,\ 1,\ 1,\ 2,\ 1)
\]

lo que produce la siguiente partición:

\[
\textbf{Clúster 1: } \{(1,2), (0,1), (2,2), (2,1)\}
\]
\[
\textbf{Clúster 2: } \{(4,4), (3,5), (5,5), (4,5)\}
\]

La separación es intuitiva: los puntos con coordenadas pequeñas quedan agrupados juntos, mientras que los de coordenadas altas forman un segundo grupo muy compacto.

\paragraph{Variabilidad interna y entre clústeres.}

El resultado muestra:

\begin{itemize}
    \item \textbf{Within-cluster sum of squares (WSS):}  
    \[
    WSS_1 = 3.75,\quad WSS_2 = 2.75
    \]
    Ambos valores son bajos, lo que significa que los puntos están cerca de sus centroides.
    
    \item \textbf{Between-cluster sum of squares (BSS):}  
    El algoritmo informa de que:
    \[
    \text{BSS / Total SS} = 84.8\%
    \]
    Es decir, casi el 85\% de la variabilidad total del conjunto se explica por la separación entre los dos clústeres.
\end{itemize}

Un valor tan alto de BSS indica que los clústeres encontrados están \textbf{bien separados} y que la estructura agrupada es muy clara.

\paragraph{Formas de los clústeres obtenidos.}

Visualizando los puntos:

\[
\text{Clúster 1: cercano al origen}
\qquad
\text{Clúster 2: puntos altos y concentrados}
\]

En ambos casos:

\begin{itemize}
    \item Los puntos se agrupan alrededor de centros muy representativos.
    \item La dispersión es pequeña, lo que confirma la \textbf{coherencia interna} de cada clúster.
\end{itemize}

\paragraph{Conclusión.}

El algoritmo K-Means ha identificado correctamente una estructura de dos clústeres claramente diferenciados:

\begin{itemize}
    \item Un clúster formado por observaciones con valores bajos en ambas dimensiones.
    \item Un segundo clúster formado por observaciones con valores notablemente más altos.
\end{itemize}

La varianza explicada del 84.8\% confirma que la partición es de alta calidad y que las observaciones se agrupan de manera natural en dos grupos bien definidos.

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ejercicio 1.3: Clasificación Supervisada (Árboles de Decisión)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

En este ejercicio guiado se realiza un análisis de clasificación supervisada utilizando árboles de decisión. El objetivo es construir un modelo capaz de predecir la calificación global (\textit{C.G}) de un estudiante (Aprobado/Suspenso) basándose en sus notas parciales.

\subsection{1. Descripción del conjunto de datos}
El conjunto de datos de entrenamiento consta de 9 observaciones. Las variables son cualitativas ordinales:
\begin{itemize}
    \item \textbf{Variables Predictoras:} \textit{Teoria}, \textit{Laboratorio}, \textit{Practicas} (Valores: A, B, C, D).
    \item \textbf{Variable Clase:} \textit{C.G} (Valores: Ap, Ss).
\end{itemize}

\subsection{2. Fundamento Teórico y Funciones de R}
Para la construcción del árbol se utiliza la librería \texttt{rpart}, que implementa una variante del algoritmo CART (\textit{Classification and Regression Trees}).
\begin{itemize}
    \item \textbf{\texttt{rpart(formula, data, method, control)}}: Función principal.
    \begin{itemize}
        \item \texttt{formula = C.G ~ .}: Indica que \textit{C.G} es la variable a predecir usando todas las demás.
        \item \texttt{method = "class"}: Especifica un árbol de clasificación.
        \item \texttt{control = rpart.control(minsplit = 1)}: Parámetro crítico en muestras pequeñas ($N=9$). Fuerza al algoritmo a intentar dividir nodos incluso si solo contienen 1 observación, permitiendo obtener un árbol puro que clasifique correctamente todos los ejemplos de entrenamiento.
    \end{itemize}
\end{itemize}

\subsection{3. Implementación en R y Resultados}

A continuación se muestra el código para generar el modelo y la salida textual del árbol generado.

<<ejercicio1_3_arboles, echo=TRUE, results=verbatim>>=
# Carga de la librería
library(rpart)

# 1. Creación del conjunto de datos
calificaciones <- data.frame(
  Teoria = c("A", "A", "D", "D", "B", "C", "B", "C", "B"),
  Laboratorio = c("A", "B", "D", "D", "C", "B", "B", "D", "A"),
  Practicas = c("B", "D", "C", "A", "B", "B", "A", "C", "C"),
  C.G = c("Ap", "Ss", "Ss", "Ss", "Ss", "Ap", "Ap", "Ss", "Ss")
)

print("Datos de entrenamiento:")
print(calificaciones)

# 2. Generación del Modelo
# Usamos minsplit=1 para asegurar que el árbol crezca completamente
arbol <- rpart(C.G ~ ., data = calificaciones,
               method = "class",
               control = rpart.control(minsplit = 1))

# 3. Visualización de la estructura del árbol
print("Estructura del Árbol generado (rpart):")
print(arbol)
@

\subsection{4. Análisis de los Resultados}

La salida textual del objeto \texttt{rpart} describe la estructura jerárquica del árbol de decisión generado. A continuación se interpreta cada nodo del resultado obtenido:

\begin{itemize}
    \item \textbf{Nodo 1 (Raíz):} Contiene las 9 observaciones completas.
    \begin{itemize}
        \item La clase mayoritaria inicial es \textbf{Ss} (Suspenso), con una probabilidad de 0.67 (6 suspensos frente a 3 aprobados).
        \item El algoritmo selecciona la variable \textbf{Laboratorio} como la que mejor discrimina en este primer nivel.
    \end{itemize}

    \item \textbf{División por Laboratorio:}
    \begin{itemize}
        \item \textbf{Nodo 3 (Derecha):} Si \texttt{Laboratorio} es \textbf{C} o \textbf{D}.
        \begin{itemize}
            \item Contiene 4 observaciones. Todas son \textbf{Ss}.
            \item El error (loss) es 0 y las probabilidades son (0.00, 1.00).
            \item Es un \textbf{nodo terminal (*)}, lo que significa que el árbol decide aquí: \emph{Si tienes C o D en Laboratorio, la predicción es Suspenso}.
        \end{itemize}

        \item \textbf{Nodo 2 (Izquierda):} Si \texttt{Laboratorio} es \textbf{A} o \textbf{B}.
        \begin{itemize}
            \item Contiene 5 observaciones. La clase mayoritaria ahora es \textbf{Ap} (probabilidad 0.60).
            \item El algoritmo necesita seguir dividiendo para mejorar la pureza, usando ahora la variable \textbf{Practicas}.
        \end{itemize}
    \end{itemize}

    \item \textbf{División por Prácticas (desde el Nodo 2):}
    \begin{itemize}
        \item \textbf{Nodo 4:} Si \texttt{Practicas} es \textbf{A} o \textbf{B}.
        \begin{itemize}
            \item Contiene 3 observaciones. Todas son \textbf{Ap} (probabilidad 1.00).
            \item Es un \textbf{nodo terminal (*)}. Decisión: \emph{Aprobado}.
        \end{itemize}

        \item \textbf{Nodo 5:} Si \texttt{Practicas} es \textbf{C} o \textbf{D}.
        \begin{itemize}
            \item Contiene 2 observaciones. Todas son \textbf{Ss} (probabilidad 1.00).
            \item Es un \textbf{nodo terminal (*)}. Decisión: \emph{Suspenso}.
        \end{itemize}
    \end{itemize}
\end{itemize}

\paragraph{Conclusión del modelo.}
El árbol generado ha logrado clasificar correctamente el 100\% de los ejemplos de entrenamiento (error 0 en todos los nodos terminales). Curiosamente, la variable \textbf{Teoría} no ha sido necesaria para la clasificación; el modelo determina que con saber las notas de Laboratorio y Prácticas es suficiente para determinar la Calificación Global en este conjunto de datos.

\subsection*{Prompt de IA Utilizado}
\begin{quote}
I need to perform a supervised classification analysis in R for Exercise 1.3. The dataset consists of 9 student records with qualitative ordinal variables: 'Teoria', 'Laboratorio', 'Practicas' (grades A, B, C, D) and a target class 'C.G' (Global Grade: Ap, Ss). Please write an R script using the \texttt{rpart} library to build a decision tree. The script must define the dataframe manually to ensure reproducibility. It is crucial to set the \texttt{minsplit} parameter to 1 in \texttt{rpart.control} to force the tree to split even with small sample sizes, allowing for a complete classification of the training set. Finally, print the tree object to display the text-based structure of the nodes and splits.
\end{quote}

\section{Ejercicio 2.2: Clustering jerárquico MIN, MAX, AVG y efecto del jitter}

\subsection{Descripción del conjunto de datos}
En este ejercicio se trabaja con un conjunto sintético de 15 observaciones bidimensionales, donde cada fila representa un punto en el plano \((x, y)\). Las coordenadas se almacenan en una matriz \texttt{datos} de dimensión \(15 \times 2\). 

\subsection{Funciones de R utilizadas}
Para realizar el análisis de clustering jerárquico se emplean funciones base de R junto con la función \texttt{agglomerative\_clustering()} del paquete \texttt{UAHDataScienceUC}.

\begin{itemize}
  \item \textbf{\texttt{matrix(data, ncol, byrow)}}: Construye la matriz bidimensional \texttt{datos} a partir de un vector que contiene las coordenadas de las observaciones ordenadas por filas.
  \item \textbf{\texttt{rownames(x)}}: Asigna nombres a las filas de la matriz, facilitando la identificación de cada observación en el dendrograma.
  \item \textbf{\texttt{set.seed()} y \texttt{runif()}}: Fijan la semilla para garantizar la reproducibilidad y generan valores aleatorios uniformes que actúan como jitter sobre las coordenadas originales.
  \item \textbf{\texttt{agglomerative\_clustering(data, proximity, distance\_method, ...)}}: Ejecuta el clustering jerárquico aglomerativo usando distintas reglas de enlace (\texttt{'single'}, \texttt{'complete'}, \texttt{'average'}) y distancia euclídea, devolviendo un objeto compatible con \texttt{hclust}.
  \item \textbf{\texttt{as.hclust()}} y \textbf{\texttt{cutree()}}: Transforman el resultado a un objeto \texttt{hclust} estándar y permiten obtener particiones con un número fijado de clústeres, en este caso \(k = 3\).
  \item \textbf{\texttt{plot()}}: Representa los dendrogramas de los distintos métodos de enlace.
\end{itemize}


\subsection{Implementación del análisis en R}

<<ej2_2_calculo, echo=TRUE, results=verbatim, fig=FALSE>>=
# ======================================================================
# CLUSTERING JERÁRQUICO: MIN, MAX, AVG Y EFECTO DEL JITTER
# ======================================================================

# Carga de la librería con agglomerative_clustering
# install.packages("UAHDataScienceUC")  # descomentar si es necesario
library(UAHDataScienceUC)

# Matriz con 15 puntos (x,y)
datos <- matrix(c(
  3.5,4.5, 0.75,3.25, 0,3,
  1.75,0.75, 3,3.75, 3.75,4.5,
  1.25,0.75, 0.25,3, 3.5,4.25,
  1.5,0.5, 1,1, 3,4,
  0.5,3, 2,0.25, 0,2.5
), ncol = 2, byrow = TRUE)

# Nombres de fila
rownames(datos) <- as.character(1:nrow(datos))

cat("Datos originales (sin jitter):\n")
print(datos)

# Jitter: ruido pequeño
# Se fija la semilla para que la generación de ruido sea reproducible
# (cada ejecución con esta semilla produce el mismo ruido).
set.seed(123)

# Se crea una matriz de ruido del mismo tamaño que 'datos',
# con valores uniformes entre -0.03 y 0.03 en cada coordenada.
ruido <- matrix(runif(length(datos), -0.03, 0.03), ncol = 2)

# Se suman los pequeños ruidos a los datos originales, obteniendo
# 'datos_jitter'. Esto simula jitter: ligeras perturbaciones de la 
# posición de cada punto.
datos_jitter <- datos + ruido

cat("\nDatos con jitter (perturbados):\n")
print(round(datos_jitter, 3))

# MIN (single-link) sin y con jitter
cl_min_original <- agglomerative_clustering(
  data = datos,
  proximity = "single",
  distance_method = "euclidean",
  learn = TRUE,
  waiting = FALSE
)

cl_min_jitter <- agglomerative_clustering(
  data = datos_jitter,
  proximity = "single",
  distance_method = "euclidean",
  learn = TRUE,
  waiting = FALSE
)

cat("\n=== Partición en 3 clústeres (single-link, sin jitter) ===\n")
print(cutree(as.hclust(cl_min_original), k = 3))

cat("\n=== Partición en 3 clústeres (single-link, con jitter) ===\n")
print(cutree(as.hclust(cl_min_jitter), k = 3))

# De esta forma se puede visualizar si el ruido añadido a los datos ha 
# perjudicado el resultado esperado. Para que todo esté correcto, los 
# datos deben continuar estando en el cluster que se obtuvo para los 
# datos originales.



# MAX (complete-link)
cl_max_original <- agglomerative_clustering(
  data = datos,
  proximity = "complete",
  distance_method = "euclidean",
  learn = TRUE,
  waiting = FALSE
)

# AVG (average-link)
cl_avg_original <- agglomerative_clustering(
  data = datos,
  proximity = "average",
  distance_method = "euclidean",
  learn = TRUE,
  waiting = FALSE
)
@

\subsection{Visualización de los dendrogramas}

% MAX (complete-link)
<<ej2_2_fig_max, echo=FALSE, results=hide, fig=TRUE, fig.align="center", fig.width=6, fig.height=5>>=
plot(as.hclust(cl_max_original), main = "MAX (complete-link)")
@

% AVG (average-link)
<<ej2_2_fig_avg, echo=FALSE, results=hide, fig=TRUE, fig.align="center", fig.width=6, fig.height=5>>=
plot(as.hclust(cl_avg_original), main = "AVG (average-link)")
@

% MIN (single-link) con jitter
<<ej2_2_fig_min_jitter, echo=FALSE, results=hide, fig=TRUE, fig.align="center", fig.width=6, fig.height=5>>=
plot(as.hclust(cl_min_jitter), main = "MIN (single-link) con jitter")
@

% MIN (single-link) sin jitter
<<ej2_2_fig_min, echo=FALSE, results=hide, fig=TRUE, fig.align="center", fig.width=6, fig.height=5>>=
plot(as.hclust(cl_min_original), main = "MIN (single-link) sin jitter")
@

\subsection{Correlación cophenética y comparación de métodos}

En esta subsección se cuantifica qué criterio de enlace (MIN, MAX o AVG) reproduce mejor las distancias originales entre las observaciones, utilizando la \emph{correlación cophenética}. Esta correlación compara las distancias euclídeas originales con las distancias inducidas por cada dendrograma (distancias cophenéticas).

<<ej2_2_coph, echo=TRUE, results=verbatim>>=
# ============================================
# CORRELACIÓN COPHENÉTICA MIN, MAX, AVG
# ============================================

# 1 Distancias originales entre los puntos ------------------------------
# Matriz de distancias euclídeas entre todas las parejas de observaciones.
# Será la referencia real que queremos que el dendrograma reproduzca.
dist_original <- dist(datos, method = "euclidean")
d_vec <- as.vector(dist_original)  # Se pasa a vector numérico

# 2 Conversión de los resultados a objetos hclust -----------------------
# Se convierten los resultados del clustering aglomerativo al formato 'hclust',
# necesario para usar cophenetic() y cutree().
hc_min <- as.hclust(cl_min_original)  # Árbol jerárquico para MIN
hc_max <- as.hclust(cl_max_original)  # Árbol jerárquico para MAX
hc_avg <- as.hclust(cl_avg_original)  # Árbol jerárquico para AVG

# 3 Distancias cophenéticas de cada dendrograma -------------------------
# Para cada par de puntos, cophenetic() devuelve la altura del nodo en el que
# se fusionan en el dendrograma: la "distancia" inducida por el árbol.
coph_min <- cophenetic(hc_min);  dh_min <- as.vector(coph_min)
coph_max <- cophenetic(hc_max);  dh_max <- as.vector(coph_max)
coph_avg <- cophenetic(hc_avg);  dh_avg <- as.vector(coph_avg)

# 4 Definición de varianza, covarianza y desviación típicas poblacionales ----
# Se usan versiones que dividen entre N (número de elementos), en lugar de N-1.

# Varianza poblacional (divide entre N)
var_pop <- function(x) {
  x <- as.vector(x)
  m <- mean(x)
  sum((x - m)^2) / length(x)
}

# Covarianza poblacional (divide entre N)
cov_pop <- function(x, y) {
  x <- as.vector(x); y <- as.vector(y)
  mx <- mean(x); my <- mean(y)
  sum((x - mx) * (y - my)) / length(x)
}

# Desviación típica poblacional
sd_pop <- function(x) sqrt(var_pop(x))

# 5 Función para la correlación cophenética (definición poblacional) ----
#   ρ = Cov_N(D, D̂) / (sd_N(D) * sd_N(D̂))
# donde D son las distancias originales y D̂ las cophenéticas.
coph_corr <- function(d, dh) {
  d  <- as.vector(d)   # Distancias originales
  dh <- as.vector(dh)  # Distancias cophenéticas

  num  <- cov_pop(d, dh)    # Covarianza poblacional Cov_N(D, D̂)
  sd_d  <- sd_pop(d)        # Desviación típica poblacional de D
  sd_dh <- sd_pop(dh)       # Desviación típica poblacional de D̂
  den  <- sd_d * sd_dh      # Producto de desviaciones típicas poblacionales

  rho <- num / den          # Coeficiente de correlación cophenética (poblacional)

  return(rho)
}

# 6 Cálculo de la correlación cophenética para cada método --------------
# Se obtiene ρ para MIN, MAX y AVG comparando distancias originales
# con las distancias inducidas por cada dendrograma.
cor_min <- coph_corr(d_vec, dh_min)
cor_max <- coph_corr(d_vec, dh_max)
cor_avg <- coph_corr(d_vec, dh_avg)

# 7 Mostrar resultados en consola ---------------------------------------
# Se imprimen las correlaciones cophenéticas para cada criterio de enlace,
# para identificar qué método reproduce mejor la estructura de distancias.
cat("Cophenetic MIN :", cor_min, "\n")
cat("Cophenetic MAX :", cor_max, "\n")
cat("Cophenetic AVG :", cor_avg, "\n")
@

\subsection{Análisis e interpretación de resultados}
Primero destacar que la razón de uso de un ruido sobre los datos iniciales se debe a que ante la ausencia de los mismos no sería posible visualizar correctamente los clusters que se van formando, y de esta forma se logra una visualización más clara sin perjudicar al resultado (ver cutree). Segundo destacar que a partir de los valores obtenidos de la correlacion cophenetica podemos concluir que la mejor técnica de clusterización sería el Group Average en este caso.

\subsection*{Prompt de IA utilizado}
\begin{quote}
Actúa como un tutor experto en R, estadística y clustering jerárquico, con experiencia específica en los paquetes \texttt{LearnClust} y \texttt{UAHDataScienceUC} de R.\

Tus tareas serán:\
Leer atentamente el código R que te pase y explicar qué hace, especialmente si usa funciones como \texttt{agglomerativeHC} de \texttt{LearnClust} y sus variantes \texttt{.details}.\
Detectar y explicar por qué, en algunos dendrogramas, distintas fusiones aparecen a la misma altura (por ejemplo, cuando varias proximidades tienen exactamente el mismo valor y visualmente no se distinguen bien los niveles).\
Proponer una solución usando el paquete \texttt{UAHDataScienceUC}, manteniendo la misma lógica de clusterización (distancia euclídea, tipos de enlace MIN/single, MAX/complete, AVG/average), pero:
\begin{itemize}
\item Añadiendo un pequeño \emph{jitter} (ruido aleatorio controlado) solo para el método MIN (single-link), con el objetivo de romper empates en las proximidades y hacer más visibles los distintos niveles del dendrograma.
\item Manteniendo los datos sin \emph{jitter} para los métodos MAX (complete-link) y AVG (average-link).
\end{itemize}
Generar código R completo y listo para ejecutar, que incluya:
\begin{itemize}
\item Creación de los datos (matrices o \texttt{data.frame}) a partir de los valores que te indique.
\item Generación de una versión \emph{jittered} de los datos únicamente para el clustering MIN.
\item Llamadas a \texttt{agglomerative\_clustering} con los parámetros adecuados (\texttt{proximity}, \texttt{distance\_method}, \texttt{learn}, \texttt{waiting}).
\item Conversión a \texttt{hclust} con \texttt{as.hclust} si eso ayuda a representar mejor los niveles del dendrograma.
\item Apertura de ventanas gráficas independientes (\texttt{windows()} en Windows, u otra función equivalente si se necesita) para dibujar un dendrograma por cada tipo de enlace:
\begin{itemize}
\item MIN (single-link) usando los datos con \emph{jitter}.
\item MAX (complete-link) usando los datos originales.
\item AVG (average-link) usando los datos originales.
\end{itemize}
\item Inclusión de títulos claros en \texttt{main} para que se identifique visualmente cada método y si usa \emph{jitter} o no.
\end{itemize}
Ajustar y explicar los dendrogramas para que se distingan bien los distintos niveles de fusión entre clusters, en particular:
\begin{itemize}
\item Mostrar cómo el \emph{jitter} en MIN consigue separar visualmente fusiones que antes aparecían a la misma altura.
\item Comentar que la estructura de clustering (qué puntos se agrupan con cuáles) se mantiene prácticamente igual, pero la representación gráfica gana claridad.
\end{itemize}
Mantener, siempre que sea posible, los mensajes explicativos paso a paso que ofrece \texttt{agglomerative\_clustering} (\texttt{learn = TRUE}), y usar \texttt{dist}, \texttt{hclust}, \texttt{as.hclust}, etc.\ solo como apoyo para la visualización cuando haga falta.

Explicarme brevemente, en español y de forma clara:
\begin{itemize}
\item Qué hace cada bloque de código.
\item Por qué el dendrograma original no distinguía bien los niveles (empates en las distancias/proximidades).
\item Cómo el \emph{jitter} aplicado solo al MIN soluciona el problema sin ``destrozar'' el clustering.
\item Cómo interpretar el dendrograma resultante: qué puntos forman cada cluster y en qué orden se fusionan.
\end{itemize}

Requisitos de estilo:
\begin{itemize}
\item Responde siempre en español.
\item Sé muy explícito con el código R (sin pseudocódigo) y comenta las líneas clave.
\item Si detectas errores de sintaxis o parámetros incoherentes, corrígelos y explica el motivo.
\item Si es relevante, indica cómo cambiar el código para otros sistemas operativos (por ejemplo, reemplazar \texttt{windows()} por \texttt{X11()} o \texttt{quartz()}).
\item Cuando te pase nuevos datos o matrices, genera automáticamente las tres variantes de clustering jerárquico (MIN/single con \emph{jitter}, MAX/complete sin \emph{jitter} y AVG/average sin \emph{jitter}) con sus correspondientes \emph{plots} en ventanas separadas, reutilizando la misma estructura de código y explicaciones.
\end{itemize}
\end{quote}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ejercicio 2.3: Clasificación Supervisada Manual (Árboles de Decisión)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

En este ejercicio autónomo se construye un árbol de decisión de forma manual para clasificar tipos de vehículos. Se implementa la lógica del algoritmo de particionamiento recursivo (siguiendo la filosofía de algoritmos como Hunt o ID3/CART) pero programando explícitamente el cálculo de la impureza y la ganancia de información.

\subsubsection{1. Fundamento Teórico}
Los árboles de decisión clasifican instancias ordenándolas desde el nodo raíz hasta algún nodo hoja, el cual proporciona la clasificación de la instancia. Cada nodo del árbol especifica una prueba sobre algún atributo de la instancia, y cada rama que desciende de ese nodo corresponde a uno de los posibles valores de dicho atributo.

Para seleccionar qué atributo es el mejor para dividir los datos en cada paso, utilizamos medidas de impureza. En este ejercicio, empleamos el \textbf{Índice de Gini}, que mide la frecuencia con la que un elemento aleatorio del conjunto sería identificado incorrectamente si se etiquetara aleatoriamente según la distribución de etiquetas en el subconjunto.

\[ Gini(S) = 1 - \sum_{i=1}^{c} p_i^2 \]
Donde $p_i$ es la probabilidad de que una tupla en $S$ pertenezca a la clase $i$.

La \textbf{Ganancia de Información} (o reducción de impureza) al dividir un nodo $S$ usando un atributo $A$ se calcula como:
\[ Ganancia(S, A) = Gini(S) - \sum_{v \in Values(A)} \frac{|S_v|}{|S|} Gini(S_v) \]
El algoritmo selecciona en cada paso el atributo que maximiza esta ganancia (o minimiza el Gini ponderado resultante).

\subsubsection{2. Definición del Conjunto de Datos}
El conjunto de datos consta de 10 observaciones con las siguientes características:
\begin{itemize}
    \item \textbf{Atributos Predictoras:} \textit{TipoCarnet} (A, B, N), \textit{Ruedas} (2, 4, 6), \textit{Pasajeros} (1, 2, 4, 5, 6).
    \item \textbf{Clase Objetivo:} \textit{Vehiculo} (Bicicleta, Camion, Coche, Moto).
\end{itemize}

<<ejercicio2_3_datos, echo=TRUE, results=verbatim>>=
# Definición del Dataset
datos_vehiculos <- data.frame(
  Carnet = c("B", "A", "N", "B", "B", "B", "N", "B", "B", "N"),
  Ruedas = c(4, 2, 2, 6, 4, 4, 2, 2, 6, 2),
  Pasajeros = c(5, 2, 1, 4, 6, 4, 2, 1, 2, 1),
  Vehiculo = c("Coche", "Moto", "Bicicleta", "Camion", "Coche", "Coche",
               "Bicicleta", "Moto", "Camion", "Bicicleta"),
  stringsAsFactors = FALSE
)
print(datos_vehiculos)
@

\subsubsection{3. Funciones Implementadas}
Para realizar el análisis manual, se han definido dos funciones clave que encapsulan la lógica matemática de la teoría:

\begin{enumerate}
    \item \textbf{\texttt{calcular\_gini(clases)}}:
    \begin{itemize}
        \item \textit{Objetivo:} Calcular la impureza de un nodo específico.
        \item \textit{Argumentos:} Un vector con las etiquetas de clase de las observaciones en ese nodo.
        \item \textit{Funcionamiento:} Genera una tabla de frecuencias relativas ($p_i$) para cada clase y aplica la fórmula $1 - \sum p_i^2$. Si el nodo es puro (solo una clase), devuelve 0.
    \end{itemize}

    \item \textbf{\texttt{evaluar\_division(datos, atributo, clase\_objetivo)}}:
    \begin{itemize}
        \item \textit{Objetivo:} Simular la división del nodo por un atributo específico y calcular la ganancia.
        \item \textit{Argumentos:} El dataframe actual, el nombre del atributo candidato y el nombre de la variable clase.
        \item \textit{Funcionamiento:}
        1. Calcula el Gini inicial del nodo padre.
        2. Itera sobre cada valor único del atributo, creando subconjuntos (ramas).
        3. Calcula el Gini de cada rama y realiza una suma ponderada según el número de elementos en cada rama.
        4. Devuelve la ganancia neta (Gini Inicial - Gini Ponderado).
    \end{itemize}
\end{enumerate}

<<ejercicio2_3_funciones, echo=TRUE, results=hide>>=
# Función auxiliar: Cálculo del Índice de Gini
calcular_gini <- function(clases) {
  n <- length(clases)
  if (n == 0) return(0)

  # Probabilidades de cada clase
  conteos <- table(clases)
  proporciones <- conteos / n

  # Fórmula: 1 - suma(pi^2)
  gini <- 1 - sum(proporciones^2)
  return(gini)
}

# Función principal: Evaluar la calidad de dividir por un atributo
evaluar_division <- function(datos, atributo, clase_objetivo) {
  valores <- unique(datos[[atributo]])
  n_total <- nrow(datos)

  # Impureza antes de dividir
  gini_inicial <- calcular_gini(datos[[clase_objetivo]])
  gini_ponderado <- 0

  cat(sprintf("\n[Analizando atributo: %s]\n", atributo))

  # Iteramos por cada posible rama (valor del atributo)
  for (val in valores) {
    subconjunto <- datos[datos[[atributo]] == val, ]
    n_sub <- nrow(subconjunto)
    gini_sub <- calcular_gini(subconjunto[[clase_objetivo]])

    # Acumulamos el Gini ponderado
    gini_ponderado <- gini_ponderado + (n_sub / n_total) * gini_sub

    clases_rama <- paste(unique(subconjunto[[clase_objetivo]]), collapse=",")
    cat(sprintf("   -> Rama '%s' (n=%d): Clases={%s} | Gini=%.3f\n",
                val, n_sub, clases_rama, gini_sub))
  }

  ganancia <- gini_inicial - gini_ponderado
  cat(sprintf("   => Ganancia Total: %.3f (Gini Ponderado: %.3f)\n",
              ganancia, gini_ponderado))

  return(ganancia)
}
@

\subsubsection{4. Construcción del Árbol y Análisis de Resultados}

El proceso de construcción se realiza de manera iterativa. En cada paso (nodo), evaluamos todos los atributos disponibles y seleccionamos el que ofrece la mayor ganancia de información.

\paragraph{Paso 1: Selección del Nodo Raíz.}
Evaluamos los tres atributos disponibles con el conjunto de datos completo (10 observaciones).

<<ejercicio2_3_paso1, echo=TRUE, results=verbatim>>=
cat("--- PASO 1: Evaluación del Nodo Raíz ---\n")
g_carnet <- evaluar_division(datos_vehiculos, "Carnet", "Vehiculo")
g_ruedas <- evaluar_division(datos_vehiculos, "Ruedas", "Vehiculo")
g_pasajeros <- evaluar_division(datos_vehiculos, "Pasajeros", "Vehiculo")
@

\textbf{Análisis del Paso 1:}
\begin{itemize}
    \item \textbf{Carnet:} Ganancia 0.373. Separa bien, pero mezcla clases en la rama 'B'.
    \item \textbf{Ruedas: Ganancia 0.500. Es el valor máximo.}
    \item \textbf{Pasajeros:} Ganancia 0.307.
\end{itemize}

\textbf{Decisión:} Seleccionamos \textbf{Ruedas} como nodo raíz. Analizando sus ramas:
\begin{itemize}
    \item Si Ruedas=4: La impureza es 0 (Solo hay Coche). $\to$ \textbf{Nodo Hoja: Coche}.
    \item Si Ruedas=6: La impureza es 0 (Solo hay Camion). $\to$ \textbf{Nodo Hoja: Camion}.
    \item Si Ruedas=2: La impureza es 0.480 (Mezcla de Moto y Bicicleta). $\to$ \textbf{Necesita subdivisión}.
\end{itemize}

\paragraph{Paso 2: Subdivisión de la rama 'Ruedas = 2'.}
Filtramos el dataset para quedarnos solo con las observaciones donde Ruedas es 2 y evaluamos los atributos restantes (Carnet y Pasajeros) para desempatar entre Moto y Bicicleta.

<<ejercicio2_3_paso2, echo=TRUE, results=verbatim>>=
cat("\n--- PASO 2: Evaluación de la rama 'Ruedas = 2' ---\n")
# Filtramos el subconjunto
datos_rama_ruedas2 <- datos_vehiculos[datos_vehiculos$Ruedas == 2, ]
print(datos_rama_ruedas2)

# Evaluamos atributos restantes
g2_carnet <- evaluar_division(datos_rama_ruedas2, "Carnet", "Vehiculo")
g2_pasajeros <- evaluar_division(datos_rama_ruedas2, "Pasajeros", "Vehiculo")
@

\textbf{Análisis del Paso 2:}
\begin{itemize}
    \item \textbf{Carnet: Ganancia 0.480.} El Gini ponderado baja a 0.000. Esto significa que este atributo separa perfectamente las clases restantes.
    \begin{itemize}
        \item Carnet A $\to$ Moto.
        \item Carnet B $\to$ Moto.
        \item Carnet N $\to$ Bicicleta.
    \end{itemize}
    \item \textbf{Pasajeros:} Ganancia 0.013. No logra separar bien las clases (hay bicicletas y motos con 1 pasajero).
\end{itemize}

\textbf{Decisión:} Seleccionamos \textbf{Carnet} para dividir este nodo.

\subsubsection{5. Estructura Final del Árbol}
Tras el análisis manual paso a paso, el árbol de decisión resultante es:

\begin{verbatim}
1. ¿Ruedas?
   |-- 4: Coche (Hoja Pura)
   |-- 6: Camion (Hoja Pura)
   |-- 2: ¿Carnet?
       |-- A: Moto (Hoja Pura)
       |-- B: Moto (Hoja Pura)
       |-- N: Bicicleta (Hoja Pura)
\end{verbatim}

Este modelo clasifica correctamente el 100\% de las instancias de entrenamiento utilizando un máximo de dos niveles de profundidad.

\subsubsection*{Prompt de IA Utilizado}
\begin{quote}
For Exercise 2.3 (Autonomous Supervised Classification), I need to construct a decision tree manually in R for a vehicle dataset with attributes 'Carnet', 'Ruedas', 'Pasajeros' and class 'Vehiculo'. Please provide a script that defines the dataset manually and implements two helper functions: \texttt{calcular\_gini} (to compute impurity) and \texttt{evaluar\_division} (to iterate through attribute values and calculate Information Gain). The script should not use \texttt{rpart}, but rather perform the calculations step-by-step: first evaluating all attributes for the root node, analyzing the results to pick the best splitter, filtering the data for any impure branch, and repeating the evaluation process for the next level. The output must show the Gini and Gain values to justify the decisions.
\end{quote}

\end{document}
