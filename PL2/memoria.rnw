%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PREÁMBULO DE LATEX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper]{article}

% --- Paquetes Esenciales ---
\usepackage[utf8]{inputenc} % Codificación de entrada
\usepackage[T1]{fontenc}    % Codificación de fuentes
\usepackage[spanish]{babel} % Idioma español
\usepackage{graphicx}       % Para insertar imágenes
\usepackage[a4paper,margin=2.5cm]{geometry} % Márgenes
\usepackage{Sweave}         % Estilos para Sweave

% --- Paquetes Adicionales Útiles ---
\usepackage{amsmath}        % Mejoras para matemáticas
\usepackage{amsfonts}       % Fuentes matemáticas
\usepackage{hyperref}       % Links clicables
\usepackage{float}          % Controlar posición de flotantes (figuras, tablas)
\usepackage{csquotes}       % Para citas textuales
\usepackage[backend=biber,style=ieee]{biblatex} % Bibliografía

\setlength{\parskip}{\baselineskip} % Un espacio entre párrafos

% \addbibresource{bibliografia.bib} % Descomenta esto y crea tu archivo .bib

% --- Configuración del Documento ---
\title{Memoria: Memoria PL2 - Fundamentos de la Ciencia de Datos}
\author{Oscar Morán, Asier Álamo, Edgar Alexis Conforme, Lucía Díaz}
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INICIO DEL DOCUMENTO
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}


% --- Portada ---
\begin{titlepage}
    \centering
    \vspace*{\fill}
    {\LARGE Universidad de Alcalá \\[0.5cm]}
    {\Large Escuela Politécnica Superior \\[1cm]}
    {\Huge \textbf{PL2 - Fundamentos de la Ciencia de Datos} \\[1.5cm]}
    \Large\textbf{Autor:} \\[0.3cm]
    {\Large Oscar Morán, Asier Álamo, Edgar Alexis Conforme, Lucía Díaz \\[2cm]}
    \textbf{Fecha:} \\[0.3cm]
    {\Large \today} \\[1.5cm]
    \vspace*{\fill}
\end{titlepage}


\newpage
\tableofcontents % Tabla de contenido automática
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ejercicio 1.1: Clasificación no supervisada (K-Means)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Descripción del conjunto de datos}

El conjunto de datos está formado por \textbf{8 observaciones}, cada una de ellas correspondiente a un estudiante. Para cada estudiante se registran dos calificaciones numéricas:

\begin{itemize}
  \item \textbf{Teoría}: nota obtenida en la parte teórica de la asignatura.
  \item \textbf{Laboratorio}: nota obtenida en las prácticas de laboratorio.
\end{itemize}

Ambas variables son cuantitativas (en una escala discreta de 0 a 5) y se tratan como coordenadas de un punto en el plano bidimensional \((\text{Teoría}, \text{Laboratorio})\). De este modo, cada estudiante queda representado por un par ordenado que permite aplicar directamente técnicas de clustering en \(\mathbb{R}^2\).

Las 8 observaciones utilizadas en el ejercicio son las siguientes:

\[
\{(4,4), (3,5), (1,2), (5,5), (0,1), (2,2), (4,5), (2,1)\}
\]

En la implementación en R, estos datos se almacenan inicialmente en una matriz de dimensión \(2 \times 8\), donde cada columna representa un estudiante y cada fila una de las dos variables (\textit{Teoría} y \textit{Laboratorio}). Posteriormente, la matriz se transpone para obtener una estructura de \(8 \times 2\), en la que cada fila corresponde a un estudiante y cada columna a una variable, formato estándar para aplicar el algoritmo \texttt{kmeans()}.

El propósito del ejercicio es agrupar automáticamente a los estudiantes en clústeres de rendimiento similares, basándose únicamente en sus calificaciones de Teoría y Laboratorio, sin utilizar información de etiquetas previas (clasificación no supervisada).


\subsection{Fundamento teórico}

El objetivo del ejercicio es aplicar un método de \textbf{clasificación no supervisada} para agrupar observaciones similares sin emplear una variable clase conocida.

\paragraph{Concepto de clustering por particionado.}
Los métodos de particionado buscan dividir un conjunto de $n$ observaciones en $K$ grupos disjuntos (clústeres), de forma que:

\begin{itemize}
  \item Las observaciones dentro de un mismo clúster sean lo más \textbf{similares} posible.
  \item Las observaciones pertenecientes a clústeres distintos sean lo más \textbf{diferentes} posible.
\end{itemize}

En el caso de K-Means, la similitud se mide mediante la \textbf{distancia euclídea} y cada clúster se representa por su \textbf{centroide}, que es el vector medio de todas las observaciones asignadas a ese clúster.

\paragraph{Algoritmo K-Means.}
El algoritmo opera iterativamente siguiendo una serie de pasos:

\begin{enumerate}
  \item \textbf{Inicialización}: Se seleccionan $K$ centros iniciales. En este ejercicio, los centros se proporcionan explícitamente en forma de matriz.
  \item \textbf{Asignación}: Cada observación se asigna al clúster cuyo centroide esté más próximo en distancia euclídea.
  \item \textbf{Actualización}: Para cada clúster, se recalcula su centroide como la media de todas las observaciones asignadas a él.
  \item \textbf{Iteración}: Se repiten los pasos de asignación y actualización hasta que los centroides dejan de cambiar de manera significativa o se alcanza el número máximo de iteraciones.
\end{enumerate}

\subsection{Implementación en R y resultados}

A continuación se presenta el código utilizado para aplicar el algoritmo K-Means al conjunto de datos. Todas las funciones empleadas pertenecen al paquete base \texttt{stats}, cargado automáticamente por R.

<<ejercicio1_2_kmeans, echo=TRUE, results=verbatim>>=
# Se crea una matriz de 2 filas y 8 columnas con los datos proporcionados
m <- matrix(c(4,4,3,5,
              1,2,5,5,
              0,1,2,2,
              4,5,2,1), 
            2, 8)

# Se transpone la matriz para que cada fila sea una observación
(m <- t(m))

# Se crea la matriz de centros iniciales (2 centros, 2 dimensiones)
c <- matrix(c(0,1,
              2,2), 
            2, 2)

# Se transponen los centros (cada fila será un centro)
(c <- t(c))

# Ejecución de k-means usando:
# - m como datos
# - c como centros iniciales (2 clusters)
# - iter.max = 4 (el límite de iteraciones)
(clasificacionss <- kmeans(m, c, 4))

# Se añade la columna con el número de cluster asignado a cada observación
m <- cbind(clasificacionss$cluster, m)

# Se filtran las observaciones del cluster 1
mc1 <- subset(m, m[,1] == 1)
mc1

# Se filtran las observaciones del cluster 2
mc2 <- subset(m, m[,1] == 2)
mc2

# Se elimina la columna del cluster (dejamos solo los valores originales)
(mc1 <- mc1[, -1])
(mc2 <- mc2[, -1])
@

\paragraph{Explicación detallada de las funciones y parámetros utilizados.}

\begin{itemize}

\item \textbf{\texttt{matrix(data, nrow, ncol)}}  
Construye una matriz a partir de un vector. En este ejercicio:

\begin{itemize}
  \item \textbf{\texttt{data}} contiene las calificaciones de 8 estudiantes.
  \item \textbf{\texttt{nrow = 2}} indica que hay dos variables (Teoría, Laboratorio).
  \item \textbf{\texttt{ncol = 8}} indica que hay 8 observaciones.
\end{itemize}

R rellena la matriz por columnas, por lo que posteriormente es necesario transponerla.

\item \textbf{\texttt{t(x)}}  
Devuelve la transpuesta de la matriz.  
Se utiliza para que cada fila represente una observación, formato requerido por \texttt{kmeans()}.

\item \textbf{\texttt{kmeans(x, centers, iter.max)}} — Paquete \texttt{stats}  

Aplica el algoritmo K-Means de particionado sobre un conjunto de datos numéricos.

\begin{itemize}
  \item \textbf{\texttt{x = m}}  
  Es el objeto que contiene los datos. En nuestro caso:
  \begin{itemize}
    \item \texttt{m} es una \textbf{matriz numérica} de dimensión \(8 \times 2\).
    \item Cada \textbf{fila} representa una observación (un estudiante).
    \item Cada \textbf{columna} representa una variable (\textit{Teoría} y \textit{Laboratorio}).
  \end{itemize}
  La función calcula distancias euclídeas entre filas de \texttt{m} para decidir a qué clúster pertenece cada observación.

  \item \textbf{\texttt{centers = c}}  
  Controla cómo se inicializan los centroides. Puede ser:
  \begin{itemize}
    \item Un \textbf{número entero} \(K\): R elige aleatoriamente \(K\) filas de \texttt{x} como centros iniciales.
    \item Una \textbf{matriz numérica} con \(K\) filas: cada fila se toma como centro inicial de un clúster.
  \end{itemize}
  En este ejercicio usamos la segunda opción:
  \begin{itemize}
    \item \texttt{c} es una matriz de dimensión \(2 \times 2\).
    \item Cada fila de \texttt{c} es un centro inicial en el plano \((\text{Teoría}, \text{Laboratorio})\).
  \end{itemize}
  Esto hace que el algoritmo sea \textbf{determinista}: siempre empieza en los mismos puntos y, por tanto, siempre produce la misma partición.

  \item \textbf{\texttt{iter.max = 4}}  
  Es el \textbf{número máximo de iteraciones} del algoritmo.  
  Cada iteración realiza dos pasos:
  \begin{enumerate}
    \item Reasignar cada observación al centro más cercano (en distancia euclídea).
    \item Recalcular los centroides como la media de las observaciones de cada clúster.
  \end{enumerate}
  El algoritmo se detiene cuando:
  \begin{itemize}
    \item los centros dejan de cambiar, o
    \item se ha alcanzado el límite de \texttt{iter.max}.
  \end{itemize}
  En nuestro caso, el algoritmo converge antes de llegar a 4 iteraciones.
\end{itemize}

La función devuelve un objeto de clase \texttt{"kmeans"}, que en realidad es una \textbf{lista} con varios componentes. Los más relevantes en este ejercicio son:

\begin{itemize}
  \item \textbf{\texttt{\$cluster}}  
  Vector de longitud igual al número de observaciones (\(n = 8\)).  
  Para cada fila de \texttt{m}, indica el número de clúster al que ha sido asignada (1, 2, ..., K).  
  En nuestro ejercicio, el vector:
  \[
    (2,\ 2,\ 1,\ 2,\ 1,\ 1,\ 2,\ 1)
  \]
  codifica la pertenencia de cada estudiante a uno de los dos clústeres.

  \item \textbf{\texttt{\$centers}}  
  Matriz de dimensión \(K \times p\), donde:
  \begin{itemize}
    \item \(K\) es el número de clústeres (aquí 2).
    \item \(p\) es el número de variables (aquí 2: Teoría y Laboratorio).
  \end{itemize}
  Cada fila es el \textbf{centroide final} de un clúster, es decir, el vector de medias de las observaciones asignadas a ese clúster.  
  En nuestro caso:
  \[
  C_1 = (1.25,\ 1.50), \qquad C_2 = (4.00,\ 4.75)
  \]
  resumen el rendimiento “medio” de los estudiantes de cada grupo.

  \item \textbf{\texttt{\$size}}  
  Vector de longitud \(K\).  
  Cada componente indica cuántas observaciones han quedado en cada clúster.  
  Aquí obtenemos tamaños \((4, 4)\): ambos clústeres contienen exactamente 4 estudiantes.

  \item \textbf{\texttt{\$withinss}}  
  Vector de longitud \(K\) con la \textbf{suma de cuadrados intra-clúster} de cada clúster.  
  Para cada grupo, se calculan las distancias al cuadrado entre cada punto y su centroide, y se suman. Valores bajos indican clústeres \textbf{compactos}.  
  En el resultado:
  \[
    WSS_1 = 3.75,\quad WSS_2 = 2.75
  \]
  lo que refleja que, en ambos grupos, los puntos están bastante próximos a sus respectivos centroides.

  \item \textbf{\texttt{\$totss}}  
  Suma de cuadrados \textbf{total} del conjunto de datos respecto al centroide global (media de todas las observaciones sin agrupar).  
  Representa la variabilidad total inicial de los datos antes de hacer clustering.

  \item \textbf{\texttt{\$tot.withinss}}  
  Suma de las \texttt{\$withinss} de todos los clústeres:
  \[
    \texttt{tot.withinss} = \sum_{k=1}^{K} \texttt{withinss[k]}
  \]
  Es la variabilidad que todavía queda \textbf{dentro} de los clústeres tras la partición.

  \item \textbf{\texttt{\$betweenss}}  
  Suma de cuadrados \textbf{entre clústeres}. Se cumple:
  \[
    \texttt{totss} = \texttt{tot.withinss} + \texttt{betweenss}
  \]
  Cuanto mayor es \texttt{\$betweenss} (en relación con \texttt{\$totss}), mejor separadas están las medias de los clústeres.  
  En nuestro resultado, la razón:
  \[
    \frac{\texttt{betweenss}}{\texttt{totss}} = 84.8\%
  \]
  indica que casi el 85\% de la variabilidad total se explica por la separación entre los dos clústeres.
\end{itemize}

\item \textbf{\texttt{cbind(a, b, ...)}}  
Une objetos por columnas.  
Permite añadir la asignación de clúster como primera columna de la matriz:

\[
m = \begin{bmatrix}
\text{cluster} & \text{Teoría} & \text{Laboratorio}
\end{bmatrix}
\]

\item \textbf{\texttt{subset(x, condition)}}  
Filtra filas que cumplen una condición.  
Aquí se usa para extraer las observaciones de cada clúster:

\begin{verbatim}
subset(m, m[,1] == 1)
\end{verbatim}

\item \textbf{\texttt{[, -1]}}  
Indexación para eliminar la primera columna (la etiqueta de clúster), dejando solo las dos coordenadas originales.

\end{itemize}

\paragraph{Salida obtenida en R.}

El algoritmo produce la siguiente partición: dos clústeres de tamaño 4 cada uno, con centroides:

\[
C_1 = (1.25,\ 1.50), \qquad C_2 = (4.00,\ 4.75)
\]

El resultado incluye el vector de asignación:

\[
(2,\ 2,\ 1,\ 2,\ 1,\ 1,\ 2,\ 1)
\]

y las observaciones agrupadas quedan:

\[
\text{Clúster 1: } \{(1,2), (0,1), (2,2), (2,1)\}
\]
\[
\text{Clúster 2: } \{(4,4), (3,5), (5,5), (4,5)\}
\]

\subsection{Análisis e interpretación de los resultados}

Tras ejecutar el algoritmo K-Means con los centros iniciales especificados, se obtiene una partición final formada por \textbf{2 clústeres de igual tamaño} (4 observaciones cada uno). El algoritmo converge antes de alcanzar el límite de \texttt{iter.max = 4}, señal de que la separación entre grupos está claramente definida.

\paragraph{Centroides finales.}

Los centros obtenidos al finalizar el proceso son:

\[
C_1 = (1.25,\ 1.50), \qquad
C_2 = (4.00,\ 4.75)
\]

Estos centroides resumen el “punto medio” de cada agrupación. Puede observarse que:

\begin{itemize}
    \item \(C_1\) se sitúa en la región de valores bajos de ambas variables.
    \item \(C_2\) aparece claramente desplazado hacia valores altos.
\end{itemize}

Por tanto, el algoritmo ha identificado \textbf{dos grupos muy separados} en el espacio bidimensional: uno correspondiente a observaciones ``pequeñas'' y otro a observaciones ``grandes''.

\paragraph{Asignación de observaciones a los clústeres.}

El vector de pertenencia devuelto por \texttt{kmeans()} es:

\[
(2,\ 2,\ 1,\ 2,\ 1,\ 1,\ 2,\ 1)
\]

lo que produce la siguiente partición:

\[
\textbf{Clúster 1: } \{(1,2), (0,1), (2,2), (2,1)\}
\]
\[
\textbf{Clúster 2: } \{(4,4), (3,5), (5,5), (4,5)\}
\]

La separación es intuitiva: los puntos con coordenadas pequeñas quedan agrupados juntos, mientras que los de coordenadas altas forman un segundo grupo muy compacto.

\paragraph{Variabilidad interna y entre clústeres.}

El resultado muestra:

\begin{itemize}
    \item \textbf{Within-cluster sum of squares (WSS):}  
    \[
    WSS_1 = 3.75,\quad WSS_2 = 2.75
    \]
    Ambos valores son bajos, lo que significa que los puntos están cerca de sus centroides.
    
    \item \textbf{Between-cluster sum of squares (BSS):}  
    El algoritmo informa de que:
    \[
    \text{BSS / Total SS} = 84.8\%
    \]
    Es decir, casi el 85\% de la variabilidad total del conjunto se explica por la separación entre los dos clústeres.
\end{itemize}

Un valor tan alto de BSS indica que los clústeres encontrados están \textbf{bien separados} y que la estructura agrupada es muy clara.

\paragraph{Conclusión.}

El algoritmo K-Means ha identificado correctamente una estructura de dos clústeres claramente diferenciados:

\begin{itemize}
    \item Un clúster formado por observaciones con valores bajos en ambas dimensiones.
    \item Un segundo clúster formado por observaciones con valores notablemente más altos.
\end{itemize}

La varianza explicada del 84.8\% confirma que la partición es de alta calidad y que las observaciones se agrupan de manera natural en dos grupos bien definidos.

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ejercicio 1.2: Clasificación no supervisada (Clusterización jerárquica aglomerativa)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Descripción del conjunto de datos}

El conjunto de datos del Ejercicio 1.2 está formado por \textbf{6 observaciones}, cada una de ellas correspondiente de nuevo a un estudiante de la asignatura. Para cada estudiante se registran dos calificaciones numéricas:

\begin{itemize}
  \item \textbf{Teoría}: rendimiento del estudiante en la parte teórica.
  \item \textbf{Laboratorio}: rendimiento en las actividades de laboratorio.
\end{itemize}

Al igual que en el Ejercicio 1.1, ambas variables se interpretan como coordenadas de un punto en el plano bidimensional \((\text{Teoría}, \text{Laboratorio})\). En este caso, las calificaciones no son enteras. Cada estudiante queda, por tanto, representado como un par ordenado en \(\mathbb{R}^2\), sobre el que se aplicará posteriormente un método de \textbf{clusterización jerárquica aglomerativa}. 

Las 6 observaciones utilizadas en el ejercicio son las siguientes:

\[
\{(0.89, 2.94),\ (4.36, 5.21),\ (3.75, 1.12),\ (6.25, 3.14),\ (4.10, 1.80),\ (3.90, 4.27)\}
\]

Desde el punto de vista de R, estos datos se almacenan inicialmente en una matriz de dimensión \(2 \times 6\), donde cada columna representa a un estudiante y cada fila a una de las dos variables. Posteriormente, la matriz se transpone para obtener una estructura de \(6 \times 2\).

\subsection{Fundamento teórico}

\paragraph{Idea general del algoritmo.}

El algoritmo jerárquico aglomerativo comienza considerando cada observación como un
clúster independiente y, en cada iteración, fusiona los dos clústeres más próximos hasta que
finalmente solo queda uno. El procedimiento puede describirse en dos pasos que se repiten:

\begin{enumerate}
  \item \textbf{Paso A: Cálculo de la matriz de distancias entre clústeres.}\\
        En la primera iteración cada punto es un clúster, por lo que este paso equivale a
        calcular todas las distancias entre parejas de observaciones. En las siguientes
        iteraciones, la matriz recoge las distancias entre los clústeres formados hasta el
        momento.
        
        En este ejercicio se emplea la \textbf{distancia euclídea} en \(\mathbb{R}^2\). Dadas dos
        observaciones \(x_i = (x_{i1}, x_{i2})\) y \(x_j = (x_{j1}, x_{j2})\), su distancia viene dada por:
        \[
           d(x_i, x_j) = \sqrt{(x_{i1} - x_{j1})^2 + (x_{i2} - x_{j2})^2}.
        \]

  \item \textbf{Paso B: Fusión de los dos clústeres más próximos.}\\
        Una vez calculada la matriz de distancias, se ordenan las proximidades y se selecciona
        el par de clústeres \((C_r, C_s)\) con menor distancia. Se fusionan en un nuevo
        clúster \(C_{rs}\) y se vuelve al Paso A, recalculando las distancias entre el nuevo
        clúster y el resto, hasta que todos los puntos quedan agrupados en un único clúster.
\end{enumerate}

\paragraph{Proximidad entre clústeres: criterios MIN y MAX.}

La definición de \emph{distancia entre clústeres} no es única. Dependiendo de cómo se
mida la proximidad entre dos clústeres \(C_r\) y \(C_s\), se obtiene un algoritmo de HAC
distinto. En esta práctica se utilizan dos criterios:

\begin{itemize}
  \item \textbf{Enlace MIN (Single Link).}\\
        La distancia entre dos clústeres se define como la distancia mínima entre cualquier
        pareja de puntos, uno en cada clúster:
        \[
          d_{\text{MIN}}(C_r, C_s) = \min_{x \in C_r,\; y \in C_s} d(x,y).
        \]

  \item \textbf{Enlace MAX (Complete Link).}\\
        La distancia entre dos clústeres se define como la distancia máxima entre pares de
        puntos de ambos clústeres:
        \[
          d_{\text{MAX}}(C_r, C_s) = \max_{x \in C_r,\; y \in C_s} d(x,y).
        \]
\end{itemize}

\subsection{Implementación en R y resultados}

A continuación se muestra el código utilizado para aplicar el algoritmo de \textbf{clusterización jerárquica aglomerativa} sobre el conjunto de datos del Ejercicio 1.2. En este caso se emplean las funciones \texttt{agglomerativeHC()} y \texttt{agglomerativeHC.details()} del paquete \texttt{LearnClust}, configuradas con distancia euclídea y dos criterios de enlace: \textbf{MIN} (single link) y \textbf{MAX} (complete link).

<<ejercicio1_2_hac, echo=TRUE, results=verbatim>>=
# ======================================================================
# CLUSTERING JERÁRQUICO AGLOMERATIVO (EJERCICIO 1.2)
# Distancia: EUC (euclídea) | Enlaces: MIN y MAX
# ======================================================================

# Instalación del paquete 
# install.packages("LearnClust")

# Carga de la librería que contiene agglomerativeHC()
library(LearnClust)

# Matriz con las 6 observaciones bidimensionales
m <- matrix(
  c(0.89, 2.94,
    4.36, 5.21,
    3.75, 1.12,
    6.25, 3.14,
    4.10, 1.80,
    3.90, 4.27),
  2, 6
)

# Transposición para que cada fila sea una observación (6 × 2)
(m <- t(m))

# Clustering jerárquico usando distancia euclídea y criterio de enlace MIN
agglomerativeHC(m, "EUC", "MIN")

# Segunda llamada al clustering MIN (se mantiene para reproducir la práctica)
agglomerativeHC(m, "EUC", "MIN")

# Detalles del proceso de fusión paso a paso (enlace MIN)
agglomerativeHC.details(m, "EUC", "MIN")

# Detalles del proceso de fusión paso a paso (enlace MAX)
agglomerativeHC.details(m, "EUC", "MAX")
@

\paragraph{Descripción del código y funciones utilizadas.}

\begin{itemize}
  \item \textbf{\texttt{library(LearnClust)}}\\
  Carga el paquete \texttt{LearnClust}, que proporciona las funciones específicas para clustering jerárquico utilizadas en este ejercicio:
  \texttt{agglomerativeHC()} y \texttt{agglomerativeHC.details()}.

  \item \textbf{\texttt{matrix()} y \texttt{t()}}\\
  Se construye primero una matriz de dimensión \(2 \times 6\), donde cada columna contiene las coordenadas \((\text{Teoría}, \text{Laboratorio})\) de un estudiante.  
  Posteriormente, se aplica la transpuesta:
  \begin{verbatim}
(m <- t(m))
  \end{verbatim}
  de forma que la estructura final es \(6 \times 2\): cada \textbf{fila} es un estudiante y cada \textbf{columna} una variable numérica.

  \item \textbf{\texttt{agglomerativeHC(m, "EUC", "MIN")}}\\
  Ejecuta el algoritmo de clustering jerárquico aglomerativo sobre la matriz \texttt{m} usando:
  \begin{itemize}
    \item \texttt{"EUC"}: distancia euclídea entre observaciones.
    \item \texttt{"MIN"}: criterio de enlace mínimo (single link), donde la distancia entre dos clústeres se define como la mínima distancia entre dos puntos, uno de cada clúster.
  \end{itemize}
  La función devuelve una lista con:
  \begin{itemize}
    \item \texttt{\$dendrogram}: información necesaria para construir el dendrograma.
    \item \texttt{\$clusters}: los clústeres formados en cada paso del proceso aglomerativo.
    \item \texttt{\$groupedClusters}: tabla que indica qué clústeres se van fusionando en cada iteración.
  \end{itemize}

  \item \textbf{\texttt{agglomerativeHC.details(m, "EUC", "MIN")}}\\
  Muestra por consola el funcionamiento interno del algoritmo con enlace MIN, paso a paso. Para cada iteración:
  \begin{itemize}
    \item Imprime la \textbf{matriz de distancias} entre los clústeres activos.
    \item Indica la \textbf{distancia mínima} encontrada.
    \item Señala qué \textbf{clústeres} se fusionan.
    \item Muestra el \textbf{nuevo clúster} resultante de la fusión.
  \end{itemize}
  Este proceso se repite hasta que todos los datos quedan agrupados en un único clúster.

  \item \textbf{\texttt{agglomerativeHC.details(m, "EUC", "MAX")}}\\
  Ejecuta exactamente el mismo procedimiento, pero usando el criterio de enlace MAX (complete link):
  \[
    d_{\text{MAX}}(C_r, C_s) = \max_{x \in C_r,\; y \in C_s} d(x,y),
  \]
  por lo que las matrices de distancia y las fusiones pueden diferir de las obtenidas con enlace MIN, especialmente en los pasos posteriores, cuando los clústeres ya contienen varias observaciones.
\end{itemize}

\subsection{Análisis e interpretación de los resultados}

La salida de las funciones \texttt{agglomerativeHC()} y \texttt{agglomerativeHC.details()} permite seguir paso a paso cómo se van formando los clústeres a partir de las 6 observaciones iniciales.

\paragraph{Estructura jerárquica y primeras fusiones.}

En ambos criterios de enlace (MIN y MAX), el primer paso del algoritmo es el mismo:

\begin{itemize}
  \item En el \textbf{STEP 1} se detecta que la distancia mínima entre observaciones corresponde al par
  \((3.75, 1.12)\) y \((4.10, 1.80)\).  
  \item Estas dos observaciones se fusionan formando el primer clúster de tamaño 2.
\end{itemize}

En el \textbf{STEP 2}, tanto con enlace MIN como con enlace MAX, el algoritmo agrupa ahora las observaciones
\((4.36, 5.21)\) y \((3.90, 4.27)\), que forman otro clúster de tamaño 2 situado en la zona de “buen rendimiento” en ambas componentes (Teoría y Laboratorio).

\paragraph{Crecimiento de los clústeres con enlace MIN (single link).}

Con el criterio MIN, la proximidad entre clústeres se mide por la \textbf{distancia mínima} entre sus puntos. Esto da lugar a un efecto de ``encadenamiento'' (\emph{chaining}):

\begin{itemize}
  \item En el \textbf{STEP 3}, el clúster de puntos bajos \((3.75, 1.12)\)--\((4.10, 1.80)\) se une con el clúster de puntos altos \((4.36, 5.21)\)--\((3.90, 4.27)\), formando un único clúster de 4 elementos que cubre un rango amplio del plano.
  \item En el \textbf{STEP 4}, se incorpora la observación \((6.25, 3.14)\) a este clúster de 4 puntos, de nuevo buscando el enlace que minimiza la distancia con alguno de los elementos del clúster ya formado.
  \item Finalmente, en el \textbf{STEP 5}, se añade la observación restante \((0.89, 2.94)\), completando el clúster único de las 6 observaciones.
\end{itemize}

El resultado es un gran clúster que se va extendiendo mediante la unión de puntos o grupos que están relativamente cerca de \emph{alguno} de los elementos ya agrupados, incluso si el resto del clúster está lejos.

\paragraph{Crecimiento de los clústeres con enlace MAX (complete link).}

Con el criterio MAX, la distancia entre clústeres se define como la \textbf{distancia máxima} entre pares de puntos de ambos clústeres. Esto favorece clústeres más compactos:

\begin{itemize}
  \item Los \textbf{STEP 1} y \textbf{STEP 2} coinciden con el caso MIN (se forman los mismos clústeres de tamaño 2).
  \item En el \textbf{STEP 3}, el criterio MAX decide fusionar primero la observación \((6.25, 3.14)\) con el clúster de rendimiento alto \((4.36, 5.21)\)--\((3.90, 4.27)\). Geométricamente, estos tres puntos forman un grupo bastante compacto en la parte superior derecha del plano.
  \item En el \textbf{STEP 4}, la observación \((0.89, 2.94)\) se une al clúster de puntos bajos \((3.75, 1.12)\)--\((4.10, 1.80)\), construyendo un segundo clúster con notas medias/bajas.
  \item Finalmente, en el \textbf{STEP 5}, se fusionan estos dos grandes clústeres, obteniendo el clúster final con las 6 observaciones.
\end{itemize}

\paragraph{Posible elección de número de clústeres.}

Se pueden identificar de forma natural \textbf{dos grandes grupos}:

\begin{itemize}
  \item Un clúster de \textbf{rendimiento bajo/medio}: formado por los puntos cercanos a la parte baja del plano \((3.75, 1.12)\), \((4.10, 1.80)\) y, según el criterio, también \((0.89, 2.94)\).
  \item Un clúster de \textbf{rendimiento alto}: formado por \((4.36, 5.21)\), \((3.90, 4.27)\) y \((6.25, 3.14)\), situados en la zona de mejores calificaciones.
\end{itemize}

Ambos criterios de enlace convergen a una estructura muy similar: un grupo de estudiantes con mejores calificaciones y otro con calificaciones más discretas. La diferencia entre MIN y MAX se observa principalmente en el orden de las fusiones intermedias.

\paragraph{Conclusión.}

El análisis jerárquico aglomerativo confirma que las 6 observaciones se organizan de manera natural en dos grupos principales, coherentes con el rendimiento global de los estudiantes. El uso de distancia euclídea en \(\mathbb{R}^2\) permite que el algoritmo aproveche directamente la estructura geométrica de los datos, mientras que la comparación entre los criterios de enlace MIN y MAX ilustra cómo distintas definiciones de “distancia entre clústeres” pueden modificar el proceso de fusión sin alterar la interpretación del conjunto de datos.


\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ejercicio 1.3: Clasificación Supervisada (Árboles de Decisión)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

En este ejercicio guiado se realiza un análisis de clasificación supervisada utilizando árboles de decisión. El objetivo es construir un modelo capaz de predecir la calificación global (\textit{C.G}) de un estudiante (Aprobado/Suspenso) basándose en sus notas parciales.

\subsection{1. Descripción del conjunto de datos}
El conjunto de datos de entrenamiento consta de 9 observaciones. Las variables son cualitativas ordinales:
\begin{itemize}
    \item \textbf{Variables Predictoras:} \textit{Teoria}, \textit{Laboratorio}, \textit{Practicas} (Valores: A, B, C, D).
    \item \textbf{Variable Clase:} \textit{C.G} (Valores: Ap, Ss).
\end{itemize}

\subsection{2. Fundamento Teórico y Funciones de R}
Para la construcción del árbol se utiliza la librería \texttt{rpart}, que implementa una variante del algoritmo CART (\textit{Classification and Regression Trees}).
\begin{itemize}
    \item \textbf{\texttt{rpart(formula, data, method, control)}}: Función principal.
    \begin{itemize}
        \item \texttt{formula = C.G ~ .}: Indica que \textit{C.G} es la variable a predecir usando todas las demás.
        \item \texttt{method = "class"}: Especifica un árbol de clasificación.
        \item \texttt{control = rpart.control(minsplit = 1)}: Parámetro crítico en muestras pequeñas ($N=9$). Fuerza al algoritmo a intentar dividir nodos incluso si solo contienen 1 observación, permitiendo obtener un árbol puro que clasifique correctamente todos los ejemplos de entrenamiento.
    \end{itemize}
\end{itemize}

\subsection{3. Implementación en R y Resultados}

A continuación se muestra el código para generar el modelo y la salida textual del árbol generado.

<<ejercicio1_3_arboles, echo=TRUE, results=verbatim>>=
# Carga de la librería
library(rpart)

# 1. Creación del conjunto de datos
calificaciones <- data.frame(
  Teoria = c("A", "A", "D", "D", "B", "C", "B", "C", "B"),
  Laboratorio = c("A", "B", "D", "D", "C", "B", "B", "D", "A"),
  Practicas = c("B", "D", "C", "A", "B", "B", "A", "C", "C"),
  C.G = c("Ap", "Ss", "Ss", "Ss", "Ss", "Ap", "Ap", "Ss", "Ss")
)

print("Datos de entrenamiento:")
print(calificaciones)

# 2. Generación del Modelo
# Usamos minsplit=1 para asegurar que el árbol crezca completamente
arbol <- rpart(C.G ~ ., data = calificaciones,
               method = "class",
               control = rpart.control(minsplit = 1))

# 3. Visualización de la estructura del árbol
print("Estructura del Árbol generado (rpart):")
print(arbol)
@

\subsection{4. Análisis de los Resultados}

La salida textual del objeto \texttt{rpart} describe la estructura jerárquica del árbol de decisión generado. A continuación se interpreta cada nodo del resultado obtenido:

\begin{itemize}
    \item \textbf{Nodo 1 (Raíz):} Contiene las 9 observaciones completas.
    \begin{itemize}
        \item La clase mayoritaria inicial es \textbf{Ss} (Suspenso), con una probabilidad de 0.67 (6 suspensos frente a 3 aprobados).
        \item El algoritmo selecciona la variable \textbf{Laboratorio} como la que mejor discrimina en este primer nivel.
    \end{itemize}

    \item \textbf{División por Laboratorio:}
    \begin{itemize}
        \item \textbf{Nodo 3 (Derecha):} Si \texttt{Laboratorio} es \textbf{C} o \textbf{D}.
        \begin{itemize}
            \item Contiene 4 observaciones. Todas son \textbf{Ss}.
            \item El error (loss) es 0 y las probabilidades son (0.00, 1.00).
            \item Es un \textbf{nodo terminal (*)}, lo que significa que el árbol decide aquí: \emph{Si tienes C o D en Laboratorio, la predicción es Suspenso}.
        \end{itemize}

        \item \textbf{Nodo 2 (Izquierda):} Si \texttt{Laboratorio} es \textbf{A} o \textbf{B}.
        \begin{itemize}
            \item Contiene 5 observaciones. La clase mayoritaria ahora es \textbf{Ap} (probabilidad 0.60).
            \item El algoritmo necesita seguir dividiendo para mejorar la pureza, usando ahora la variable \textbf{Practicas}.
        \end{itemize}
    \end{itemize}

    \item \textbf{División por Prácticas (desde el Nodo 2):}
    \begin{itemize}
        \item \textbf{Nodo 4:} Si \texttt{Practicas} es \textbf{A} o \textbf{B}.
        \begin{itemize}
            \item Contiene 3 observaciones. Todas son \textbf{Ap} (probabilidad 1.00).
            \item Es un \textbf{nodo terminal (*)}. Decisión: \emph{Aprobado}.
        \end{itemize}

        \item \textbf{Nodo 5:} Si \texttt{Practicas} es \textbf{C} o \textbf{D}.
        \begin{itemize}
            \item Contiene 2 observaciones. Todas son \textbf{Ss} (probabilidad 1.00).
            \item Es un \textbf{nodo terminal (*)}. Decisión: \emph{Suspenso}.
        \end{itemize}
    \end{itemize}
\end{itemize}

\paragraph{Conclusión del modelo.}
El árbol generado ha logrado clasificar correctamente el 100\% de los ejemplos de entrenamiento (error 0 en todos los nodos terminales). Curiosamente, la variable \textbf{Teoría} no ha sido necesaria para la clasificación; el modelo determina que con saber las notas de Laboratorio y Prácticas es suficiente para determinar la Calificación Global en este conjunto de datos.

\subsection*{Prompt de IA Utilizado}
\begin{quote}
I need to perform a supervised classification analysis in R for Exercise 1.3. The dataset consists of 9 student records with qualitative ordinal variables: 'Teoria', 'Laboratorio', 'Practicas' (grades A, B, C, D) and a target class 'C.G' (Global Grade: Ap, Ss). Please write an R script using the \texttt{rpart} library to build a decision tree. The script must define the dataframe manually to ensure reproducibility. It is crucial to set the \texttt{minsplit} parameter to 1 in \texttt{rpart.control} to force the tree to split even with small sample sizes, allowing for a complete classification of the training set. Finally, print the tree object to display the text-based structure of the nodes and splits.
\end{quote}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ Ejercicio 2.1 : Implementación del Algoritmo K-means}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Fundamento Teórico}
Entre las técnicas existentes de clasificación no supervisada, el algoritmo \textbf{K-means} es uno de los métodos más utilizados dada su simplicidad y su capacidad para particionar datos numéricos en K grupos. 
Este algoritmo sigue un procedimiento iterativo compuesto por dos grandes fases (A y B) :
\begin{itemize}
    \item \textbf{Paso A (primera iteración):}
    \begin{itemize}
        \item Selección de K y elección de los centroides iniciales.
        \item Cálculo de la distancia euclidiana entre cada punto y cada centroide.
        \item Asignación de cada punto al centroide más cercano.
    \end{itemize}
    \item \textbf{Paso B (iteraciones siguientes):}
    \begin{itemize}
        \item Recalcular los centroides como la media de los puntos asignados.
        \item Recalcular distancias.
        \item Reasignar puntos.
    \end{itemize}
\end{itemize}
El proceso continúa hasta que la asignación deja de cambiar entre iteraciones consecutivas, criterio considerado como señal de convergencia del algoritmo (criterio de parada).
El algoritmo utiliza como medida de similitud la distancia euclidiana, definida en las diapositivas como:
$$d(p,c)=\sqrt{(p_1-c_1)^2+(p_2-c_2)^2}$$
La lógica del algoritmo consiste, por tanto, en minimizar la distancia de cada punto respecto a su centroide y actualizar estos centros hasta alcanzar estabilidad.

\subsection{Definición del Conjunto de Datos}
El conjunto de datos utilizado está compuesto por 15 observaciones, cada una formada por un par numérico \texttt{\{Velocidad, Temperatura\}}. Estos valores representan medidas normalizadas de rendimiento de un microprocesador.
Este tipo de datos bidimensionales resulta adecuado para ser analizado mediante K-means, puesto que ambas características (velocidad de respuesta y temperatura) permiten separar visualmente grupos diferentes de comportamiento térmico y rendimiento.
Se considera, según el enunciado, que lo más probable es que el conjunto de datos se organice en tres clusters claramente diferenciados.

Definición y visualización de la matriz de datos:
<<data_definition, echo=TRUE>>=
# Matriz con los 15 pares {Velocidad, Temperatura}
# Se cargan por filas (byrow=TRUE)
datos <- matrix(c(
  3.5,4.5,
  0.75,3.25,
  0,3,
  1.75,0.75,
  3,3.75,
  3.75,4.5,
  1.25,0.75,
  0.25,3,
  3.5,4.25,
  1.5,0.5,
  1,1,
  3,4,
  0.5,3,
  2,0.25,
  0,2.5),
  ncol=2, byrow=TRUE)

colnames(datos) <- c("Velocidad","Temp")

K <- 3        # Número de clusters
n <- nrow(datos)

cat("Matriz de datos (Velocidad, Temp):\n")
print(datos)
cat("\nNúmero de clusters (K):", K, "\n")
@

\subsection{Funciones implementadas.}
A continuación, se describen las funciones que se utilizan para realizar este ejercicio.

\subsubsection{Función \texttt{suma}}
La primera función desarrollada es \texttt{suma()}, cuya finalidad es calcular la suma de los elementos de un vector numérico.
Para ello, la función recorre secuencialmente cada elemento del vector acumulándolo en una variable inicializada a cero. Este proceso reproduce exactamente la definición elemental de suma como acumulación iterativa. 
Funciona mientras el vector tenga al menos un elemento.
<<func_suma, echo=TRUE>>=
suma <- function(v){
  total <- 0
  for(i in 1:length(v)){
    total <- total + v[i]
  }
  return(total)
}
@

\subsubsection{Función \texttt{which\_min}}
Otra operación fundamental para el algoritmo K-means es la identificación del índice correspondiente al valor mínimo dentro de un vector. Recibe un vector \texttt{v} y devuelve el índice del valor mínimo.
Compara sucesivamente cada elemento del vector con un valor mínimo provisional. En caso de encontrar un valor inferior, la función actualiza tanto el mínimo actual como su posición. Esta función es esencial en el paso de asignación de puntos, ya que permite seleccionar el centroide más cercano a cada observación 
según sus distancias calculadas.
<<func_which_min, echo=TRUE>>=
which_min <- function(v){
  indice <- 1
  valor  <- v[1]

  for(i in 2:length(v)){
    if(v[i] < valor){
      valor <- v[i]
      indice <- i
    }
  }
  return(indice)
}
@

\subsubsection{Función \texttt{media}}
El algoritmo K-means recalcula centroides como la media de los puntos asignados a cada cluster. Por ello, también se implementa manualmente la función \texttt{media()}, 
basada en la función anterior \texttt{suma()}. Esta función divide la suma total de los elementos entre el número de componentes del vector. Esta función es importante en el paso B del algoritmo, 
donde se obtienen los nuevos centroides.
<<func_media, echo=TRUE>>=
media <- function(v){
  return(suma(v) / length(v))
}
@

\subsubsection{Función \texttt{dist\_euclidea}}
La distancia euclidiana es el criterio utilizado por K-means para medir la similitud entre un punto y un centroide. La función \texttt{dist\_euclidea()} implementa exactamente la definición matemática de esta distancia: la raíz cuadrada de la suma de los cuadrados de las diferencias entre coordenadas correspondientes. Para ello, la función calcula el vector de diferencias entre \texttt{p} y \texttt{c}, 
lo eleva al cuadrado y finalmente aplica raíz cuadrada a la suma de los elementos.
<<func_dist_euclidea, echo=TRUE>>=
dist_euclidea <- function(p, c){
  v <- (p - c)^2
  return(sqrt(suma(v)))
}
@

\subsubsection{Función \texttt{inicializar\_centroides}}
Se ha creado una función flexible para inicializar centroides. Esta función permite dos modos de uso:
\begin{itemize}
    \item Inicialización manual, en la que el usuario proporciona directamente los centroides iniciales.
    \item Inicialización aleatoria dentro del rango de los datos, donde cada centroide se genera mediante números aleatorios uniformes comprendidos entre el mínimo y el máximo de cada variable. Esta modalidad está diseñada para simular cómo se inicializa K-means en la práctica, donde no siempre se parte de centroides conocidos.
\end{itemize}
Con esta flexibilidad, el algoritmo se vuelve más general y capaz de adaptarse a distintos escenarios sin modificar su estructura interna.
En este ejercicio se utiliza la \textbf{inicialización manual} tomando las observaciones 1, 4 y 8 como centroides iniciales.
<<func_inicializar_centroides, echo=TRUE>>=
inicializar_centroides <- function(datos, K, manual = NULL){

  # --- OPCIÓN 1: pasas los centroides manualmente ---
  if(!is.null(manual)){
    return(manual)
  }

  # --- OPCIÓN 2: centroides aleatorios dentro del rango de los datos ---

  # Para cada columna (velocidad y temperatura), obtenemos rango
  mins <- apply(datos, 2, min)
  maxs <- apply(datos, 2, max)

  centroides <- matrix(0, nrow=K, ncol=ncol(datos))

  for(k in 1:K){
    for(j in 1:ncol(datos)){
      # número aleatorio uniforme dentro del rango de la columna
      centroides[k, j] <- runif(1, mins[j], maxs[j])
    }
  }

  return(centroides)
}

### → O bien pones tus centroides
centroides <- inicializar_centroides(datos, K, manual = datos[c(1,4,8), ])
cat("Centroides iniciales (observaciones 1, 4, 8):\n")
print(centroides)

### → O bien los generas aleatoriamente (más genérico)
#centroides <- inicializar_centroides(datos, K)
@

\subsubsection{Función \texttt{asignar\_clusters}}
Una de las piezas centrales del algoritmo es la función \texttt{asignar\_clusters()}, que implementa la fase de asignación de puntos. 
Para cada observación del conjunto de datos, la función calcula su distancia a cada uno de los centroides utilizando \texttt{dist\_euclidea()}. Una vez obtenidas las distancias, se determina cuál de ellas es la mínima mediante 
\texttt{which\_min()}. El resultado final es un vector que indica, para cada punto, el índice del cluster al que pertenece.
<<func_asignar_clusters, echo=TRUE>>=
asignar_clusters <- function(datos, centroides){
  n <- nrow(datos)
  K <- nrow(centroides)
  asignacion <- rep(0, n)

  for(i in 1:n){
    distancias <- rep(0, K)

    # Calculamos distancia desde el punto i a cada centroide
    for(k in 1:K){
      distancias[k] <- dist_euclidea(datos[i,], centroides[k,])
    }

    # Elegimos el centroide más cercano
    asignacion[i] <- which_min(distancias)
  }

  return(asignacion)
}
@

\subsubsection{Función \texttt{recalcular\_centroides}}
La segunda fase del algoritmo consiste en recalcular los centroides en función de los puntos asignados en la fase anterior. Para ello se implementa la función \texttt{recalcular\_centroides()}, que agrupa todas las observaciones pertenecientes a cada cluster y 
calcula de manera independiente la media de Velocidad y la media de Temperatura usando la función \texttt{media()}. De esta forma se obtiene una matriz de centroides actualizada que se utilizará en la siguiente iteración del algoritmo. Esta implementación manual 
permite visualizar claramente el mecanismo mediante el cual los centroides se desplazan hacia el centro geométrico de cada grupo de puntos. Devuelve una matriz con los nuevos centroides.
<<func_recalcular_centroides, echo=TRUE>>=
recalcular_centroides <- function(datos, asignacion, K){
  nuevos_centroides <- matrix(0, nrow=K, ncol=2)

  for(k in 1:K){
    # Extraemos todos los puntos asignados al cluster k
    puntos <- datos[asignacion == k, , drop=FALSE]

    # Calculamos manualmente el centroide:
    # media de la columna 1 (Velocidad)
    # media de la columna 2 (Temp)
    if(nrow(puntos) > 0){ # Evitar dividir por cero si un cluster queda vacío
        nuevos_centroides[k,1] <- media(puntos[,1])
        nuevos_centroides[k,2] <- media(puntos[,2])
    } else {
        # Si un cluster queda vacío, se mantiene el centroide anterior o se redefine
        # Aquí se dejará en cero por simplicidad en esta implementación manual.
        nuevos_centroides[k,1] <- 0 
        nuevos_centroides[k,2] <- 0
    }
  }

  return(nuevos_centroides)
}
@

\subsubsection{Bucle principal del algoritmo K-means}
Finalmente, todas las funciones anteriores se integran dentro de un bucle \texttt{repeat} que representa la ejecución completa del algoritmo. 
En cada iteración se asignan los puntos a los centroides y posteriormente se recalculan estos centroides. El proceso continúa hasta que la asignación deja de cambiar entre una iteración y la siguiente, 
lo que indica que se ha alcanzado la convergencia. Esta condición de parada reproduce fielmente el comportamiento teórico del algoritmo: una vez que los puntos permanecen estables, el algoritmo ha encontrado una solución.

\medskip
\noindent El resultado de la ejecución iterativa es el siguiente:
<<k_means_iterations, echo=TRUE>>=
### =============================
### ITERACIONES DEL ALGORITMO K-MEANS
### =============================

iter <- 1
repeat{
  cat("\n--- Iteración", iter, "---\n")

  # Paso A.2 / B.2: asignación de clusters
  asignacion_nueva <- asignar_clusters(datos, centroides)
  print(asignacion_nueva)

  # Criterio de parada: si no cambia la clasificación, ha convergido
  if(iter > 1 && all(asignacion_nueva == asignacion_ant)){
    cat("\n>>> La clasificación ha convergido.\n")
    break
  }

  # Paso B.1: recalcular centroides
  centroides <- recalcular_centroides(datos, asignacion_nueva, K)
  cat("Nuevos Centroides:\n")
  print(centroides)

  asignacion_ant <- asignacion_nueva
  iter <- iter + 1
}
@

\subsection{Desarrollo del Algoritmo y Análisis de Resultados}
Tras inicializar los centroides con las observaciones 1, 4 y 8, el algoritmo comienza a iterar, alternando fases de asignación y recálculo.
En cada iteración se muestran los clusters asignados y los centroides recalculados.
Al ejecutarse el código, la clasificación final estabilizada es:
<<final_results, echo=TRUE>>=
### =============================
### RESULTADOS FINALES
### =============================

cat("\n\n=========================\n")
cat("   CLUSTER FINAL\n")
cat("=========================\n")
print(asignacion_nueva)

cat("\nCentroides finales:\n")
print(centroides)
@

Esto significa:
\begin{itemize}
    \item Cluster 1: puntos 1, 5, 6, 9, 12
    \item Cluster 2: puntos 4, 7, 10, 11, 14
    \item Cluster 3: puntos 2, 3, 8, 13, 15
\end{itemize}

Los centroides obtenidos al finalizar el proceso fueron:
\texttt{
\Sexpr{format(centroides[1,], digits=3)}\\
\Sexpr{format(centroides[2,], digits=3)}\\
\Sexpr{format(centroides[3,], digits=3)}
}

\subsubsection{Interpretación:}
\begin{itemize}
    \item Cluster 1 agrupa microprocesadores de alto rendimiento y alta temperatura (velocidad $\sim3.3-3.7$ y temperatura $\sim4-4.5$).
    \item Cluster 2 reúne valores de baja temperatura y velocidad media-baja, característicos de microprocesadores que operan en frío.
    \item Cluster 3 concentra valores de baja velocidad y temperaturas medias, representando un comportamiento intermedio.
\end{itemize}
Los centroides finales muestran tres regiones bien diferenciadas en el plano Velocidad-Temperatura, coherentes con la interpretación visual inicial del ejercicio.



\subsubsection*{Prompt de IA Utilizado}
\begin{quote}
I need you to generate a manual code for the K-means algorithm in R, without using kmeans() or automatic functions, 
so I can understand each step of the algorithm just like we did in class.
To do it like we did in class, I was thinking of:

Basic functions such as: a sum function, a whichMin function to get the index of the minimum, a mean function, 
and a dist\_euclidea function explicitly calculated as $\sqrt{((x_1-x_2)^2+(y_1-y_2)^2)}$, without using external functions.

An initialize\_centroids function, which allows both manually entering centroids and generating them randomly within the data range.

An assign\_clusters function, which uses two for loops: one to iterate through each point and another to calculate the distance 
to each centroid, choosing the closest centroid.

A recalculate\_centroids function, which reconstructs each centroid by manually calculating the mean of the columns of assigned points.
If you need more functions, include them, but follow this idea.
\end{quote}



\newpage

\section{Ejercicio 2.2: Clustering jerárquico MIN, MAX, AVG y efecto del jitter}

\subsection{Descripción del conjunto de datos}
En este ejercicio se trabaja con un conjunto sintético de 15 observaciones bidimensionales, donde cada fila representa un punto en el plano \((x, y)\). Las coordenadas se almacenan en una matriz \texttt{datos} de dimensión \(15 \times 2\). 

\subsection{Funciones de R utilizadas}
Para realizar el análisis de clustering jerárquico se emplean funciones base de R junto con la función \texttt{agglomerative\_clustering()} del paquete \texttt{UAHDataScienceUC}.

\begin{itemize}
  \item \textbf{\texttt{matrix(data, ncol, byrow)}}: Construye la matriz bidimensional \texttt{datos} a partir de un vector que contiene las coordenadas de las observaciones ordenadas por filas.
  \item \textbf{\texttt{rownames(x)}}: Asigna nombres a las filas de la matriz, facilitando la identificación de cada observación en el dendrograma.
  \item \textbf{\texttt{set.seed()} y \texttt{runif()}}: Fijan la semilla para garantizar la reproducibilidad y generan valores aleatorios uniformes que actúan como jitter sobre las coordenadas originales.
  \item \textbf{\texttt{agglomerative\_clustering(data, proximity, distance\_method, ...)}}: Ejecuta el clustering jerárquico aglomerativo usando distintas reglas de enlace (\texttt{'single'}, \texttt{'complete'}, \texttt{'average'}) y distancia euclídea, devolviendo un objeto compatible con \texttt{hclust}.
  \item \textbf{\texttt{as.hclust()}} y \textbf{\texttt{cutree()}}: Transforman el resultado a un objeto \texttt{hclust} estándar y permiten obtener particiones con un número fijado de clústeres, en este caso \(k = 3\).
  \item \textbf{\texttt{plot()}}: Representa los dendrogramas de los distintos métodos de enlace.
\end{itemize}

\subsection{Fundamento teórico}
La clusterización jerárquica aglomertiva es un método de clusterización no supervisado donde se pueden emplear tres técnicas diferentes (Single Link; Complete Link y Average Link).

Cada una de ellas tienen un procedimiento diferente y se pueden comparar al final del todo con lo que se conoce como coeficiente cophenetico, que determina cual de las tres técnicas ha resultado mejor a la hora de formar los clusters.

El procedimiento de clusterización siempre comienza con la obtención de las distancias euclideas entre cada uno de los datos, una vez se tienen se eligen diferentes valores para las distancias entre los clusters dependiendo de la técnica empleado, o lo que se conoce como proximidad.

Para MIN o Single Link: La proximidad se entiende como la distancia que hay entre los dos puntos más cercanos de los dos clusters.

Para MAX o Complete Link: La proximidad se entiende como la distancia que hay entre los dos puntos más lejanos de los dos clusters.

Para AVG o Average Link: La proximidad se entiende como la media de distancias entre todos las parejas que se pueden formar con los puntos de los dos clusters.

Las fórmulas a emplear para el cálculo del coeficiente cophenetico son las siguientes:

\[
\mathrm{Cov}(X,Y) = \frac{1}{N}\sum_{k=1}^{N}\bigl(x_k - \bar{x}\bigr)\bigl(y_k - \bar{y}\bigr)
\]

\[
S_X = \sqrt{\frac{1}{N}\sum_{k=1}^{N}\bigl(x_k - \bar{x}\bigr)^2}
\qquad
S_Y = \sqrt{\frac{1}{N}\sum_{k=1}^{N}\bigl(y_k - \bar{y}\bigr)^2}
\]

\[
\rho = \frac{S_{XY}}{S_X\,S_Y}
\]




\subsection{Implementación del análisis en R}

<<ej2_2_calculo, echo=TRUE, results=verbatim, fig=FALSE>>=
# ======================================================================
# CLUSTERING JERÁRQUICO: MIN, MAX, AVG Y EFECTO DEL JITTER
# ======================================================================

# Carga de la librería con agglomerative_clustering
# install.packages("UAHDataScienceUC")  # descomentar si es necesario
library(UAHDataScienceUC)

# Matriz con 15 puntos (x,y)
datos <- matrix(c(
  3.5,4.5, 0.75,3.25, 0,3,
  1.75,0.75, 3,3.75, 3.75,4.5,
  1.25,0.75, 0.25,3, 3.5,4.25,
  1.5,0.5, 1,1, 3,4,
  0.5,3, 2,0.25, 0,2.5
), ncol = 2, byrow = TRUE)

# Nombres de fila
rownames(datos) <- as.character(1:nrow(datos))

cat("Datos originales (sin jitter):\n")
print(datos)

# Jitter: ruido pequeño
# Se fija la semilla para que la generación de ruido sea reproducible
# (cada ejecución con esta semilla produce el mismo ruido).
set.seed(123)

# Se crea una matriz de ruido del mismo tamaño que 'datos',
# con valores uniformes entre -0.03 y 0.03 en cada coordenada.
ruido <- matrix(runif(length(datos), -0.03, 0.03), ncol = 2)

# Se suman los pequeños ruidos a los datos originales, obteniendo
# 'datos_jitter'. Esto simula jitter: ligeras perturbaciones de la 
# posición de cada punto.
datos_jitter <- datos + ruido

cat("\nDatos con jitter (perturbados):\n")
print(round(datos_jitter, 3))

# MIN (single-link) sin y con jitter
cl_min_original <- agglomerative_clustering(
  data = datos,
  proximity = "single",
  distance_method = "euclidean",
  learn = FALSE,
  waiting = FALSE
)

cl_min_jitter <- agglomerative_clustering(
  data = datos_jitter,
  proximity = "single",
  distance_method = "euclidean",
  learn = FALSE,
  waiting = FALSE
)

cat("\n=== Partición en 3 clústeres (single-link, sin jitter) ===\n")
print(cutree(as.hclust(cl_min_original), k = 3))

cat("\n=== Partición en 3 clústeres (single-link, con jitter) ===\n")
print(cutree(as.hclust(cl_min_jitter), k = 3))

# De esta forma se puede visualizar si el ruido añadido a los datos ha 
# perjudicado el resultado esperado. Para que todo esté correcto, los 
# datos deben continuar estando en el cluster que se obtuvo para los 
# datos originales.



# MAX (complete-link)
cl_max_original <- agglomerative_clustering(
  data = datos,
  proximity = "complete",
  distance_method = "euclidean",
  learn = FALSE,
  waiting = FALSE
)

# AVG (average-link)
cl_avg_original <- agglomerative_clustering(
  data = datos,
  proximity = "average",
  distance_method = "euclidean",
  learn = FALSE,
  waiting = FALSE
)
@

\subsection{Formación de los clusters}

<<ej2_2_formacion, echo=TRUE, results=verbatim, fig=FALSE>>=
# Función auxiliar para imprimir el proceso de fusiones
imprimir_fusiones <- function(hc, titulo, datos) {
  cat("\n=== Proceso de fusiones (", titulo, ") ===\n", sep = "")
  merge_mat <- hc$merge
  heights   <- hc$height
  
  idx_to_label <- function(idx) {
    if (idx < 0) {
      # hoja: índice negativo -> observación original
      return(rownames(datos)[-idx])
    } else {
      # clúster intermedio
      return(paste0("C", idx))
    }
  }
  
  for (i in seq_len(nrow(merge_mat))) {
    left  <- merge_mat[i, 1]
    right <- merge_mat[i, 2]
    cat(
      sprintf(
        "Paso %2d: se fusionan %-4s y %-4s a distancia %.3f\n",
        i,
        idx_to_label(left),
        idx_to_label(right),
        heights[i]
      )
    )
  }
}

# MIN (sin jitter)
hc_min_original <- as.hclust(cl_min_original)
imprimir_fusiones(hc_min_original, "single-link, sin jitter", datos)

# MIN (con jitter)
hc_min_jitter_hc <- as.hclust(cl_min_jitter)
imprimir_fusiones(hc_min_jitter_hc, "single-link, con jitter", datos)

# MAX
hc_max <- as.hclust(cl_max_original)
imprimir_fusiones(hc_max, "complete-link", datos)

# AVG
hc_avg <- as.hclust(cl_avg_original)
imprimir_fusiones(hc_avg, "average-link", datos)
@


\subsection{Visualización de los dendrogramas}

% MAX (complete-link)
<<ej2_2_fig_max, echo=FALSE, results=hide, fig=TRUE, fig.align="center", fig.width=6, fig.height=5>>=
plot(as.hclust(cl_max_original), main = "MAX (complete-link)")
@

% AVG (average-link)
<<ej2_2_fig_avg, echo=FALSE, results=hide, fig=TRUE, fig.align="center", fig.width=6, fig.height=5>>=
plot(as.hclust(cl_avg_original), main = "AVG (average-link)")
@

% MIN (single-link) con jitter
<<ej2_2_fig_min_jitter, echo=FALSE, results=hide, fig=TRUE, fig.align="center", fig.width=6, fig.height=5>>=
plot(as.hclust(cl_min_jitter), main = "MIN (single-link) con jitter")
@

% MIN (single-link) sin jitter
<<ej2_2_fig_min, echo=FALSE, results=hide, fig=TRUE, fig.align="center", fig.width=6, fig.height=5>>=
plot(as.hclust(cl_min_original), main = "MIN (single-link) sin jitter")
@

\subsection{Correlación cophenética y comparación de métodos}

En esta subsección se cuantifica qué criterio de enlace (MIN, MAX o AVG) reproduce mejor las distancias originales entre las observaciones, utilizando la \emph{correlación cophenética}. Esta correlación compara las distancias euclídeas originales con las distancias inducidas por cada dendrograma (distancias cophenéticas).

<<ej2_2_coph, echo=TRUE, results=verbatim>>=
# ============================================
# CORRELACIÓN COPHENÉTICA MIN, MAX, AVG
# ============================================

# 1 Distancias originales entre los puntos ------------------------------
# Matriz de distancias euclídeas entre todas las parejas de observaciones.
# Será la referencia real que queremos que el dendrograma reproduzca.
dist_original <- dist(datos, method = "euclidean")
d_vec <- as.vector(dist_original)  # Se pasa a vector numérico

# 2 Conversión de los resultados a objetos hclust -----------------------
# Se convierten los resultados del clustering aglomerativo al formato 'hclust',
# necesario para usar cophenetic() y cutree().
hc_min <- as.hclust(cl_min_original)  # Árbol jerárquico para MIN
hc_max <- as.hclust(cl_max_original)  # Árbol jerárquico para MAX
hc_avg <- as.hclust(cl_avg_original)  # Árbol jerárquico para AVG

# 3 Distancias cophenéticas de cada dendrograma -------------------------
# Para cada par de puntos, cophenetic() devuelve la altura del nodo en el que
# se fusionan en el dendrograma: la "distancia" inducida por el árbol.
coph_min <- cophenetic(hc_min);  dh_min <- as.vector(coph_min)
coph_max <- cophenetic(hc_max);  dh_max <- as.vector(coph_max)
coph_avg <- cophenetic(hc_avg);  dh_avg <- as.vector(coph_avg)

# 4 Definición de varianza, covarianza y desviación típicas poblacionales ----
# Se usan versiones que dividen entre N (número de elementos), en lugar de N-1.

# Varianza poblacional (divide entre N)
var_pop <- function(x) {
  x <- as.vector(x)
  m <- mean(x)
  sum((x - m)^2) / length(x)
}

# Covarianza poblacional (divide entre N)
cov_pop <- function(x, y) {
  x <- as.vector(x); y <- as.vector(y)
  mx <- mean(x); my <- mean(y)
  sum((x - mx) * (y - my)) / length(x)
}

# Desviación típica poblacional
sd_pop <- function(x) sqrt(var_pop(x))

# 5 Función para la correlación cophenética (definición poblacional) ----
#   ρ = Cov_N(D, D̂) / (sd_N(D) * sd_N(D̂))
# donde D son las distancias originales y D̂ las cophenéticas.
coph_corr <- function(d, dh) {
  d  <- as.vector(d)   # Distancias originales
  dh <- as.vector(dh)  # Distancias cophenéticas

  num  <- cov_pop(d, dh)    # Covarianza poblacional Cov_N(D, D̂)
  sd_d  <- sd_pop(d)        # Desviación típica poblacional de D
  sd_dh <- sd_pop(dh)       # Desviación típica poblacional de D̂
  den  <- sd_d * sd_dh      # Producto de desviaciones típicas poblacionales

  rho <- num / den          # Coeficiente de correlación cophenética (poblacional)

  return(rho)
}

# 6 Cálculo de la correlación cophenética para cada método --------------
# Se obtiene ρ para MIN, MAX y AVG comparando distancias originales
# con las distancias inducidas por cada dendrograma.
cor_min <- coph_corr(d_vec, dh_min)
cor_max <- coph_corr(d_vec, dh_max)
cor_avg <- coph_corr(d_vec, dh_avg)

# 7 Mostrar resultados en consola ---------------------------------------
# Se imprimen las correlaciones cophenéticas para cada criterio de enlace,
# para identificar qué método reproduce mejor la estructura de distancias.
cat("Cophenetic MIN :", cor_min, "\n")
cat("Cophenetic MAX :", cor_max, "\n")
cat("Cophenetic AVG :", cor_avg, "\n")
@

\subsection{Análisis e interpretación de resultados}
Primero destacar que la razón de uso de un ruido sobre los datos iniciales se debe a que ante la ausencia de los mismos no sería posible visualizar correctamente los clusters que se van formando, y de esta forma se logra una visualización más clara sin perjudicar al resultado (ver cutree). Segundo destacar que a partir de los valores obtenidos de la correlacion cophenetica podemos concluir que la mejor técnica de clusterización sería el Group Average en este caso al tener el mayor valor del coeficiente cophenetico.

\subsection*{Prompt de IA utilizado}
\begin{quote}
Actúa como un tutor experto en R, estadística y clustering jerárquico, con experiencia en los paquetes \texttt{LearnClust} y \texttt{UAHDataScienceUC}.

Tus tareas son:
\begin{itemize}
  \item Leer y explicar el código R que te proporcione, especialmente las funciones de clustering jerárquico.
  \item Analizar por qué en algunos dendrogramas varias fusiones aparecen a la misma altura.
  \item Proponer una solución usando \texttt{UAHDataScienceUC}, manteniendo distancia euclídea y los enlaces MIN (single), MAX (complete) y AVG (average).
  \item Introducir un pequeño \emph{jitter} solo en el método MIN para romper empates y mejorar la visualización, manteniendo MAX y AVG sin \emph{jitter}.
  \item Generar código R completo y ejecutable que:
  \begin{itemize}
    \item Genere una versión con \emph{jitter} solo para el método MIN.
    \item Aplique los tres métodos de clustering (MIN con \emph{jitter}, MAX y AVG sin \emph{jitter}).
    \item Represente los cuatro dendrogramas en ventanas separadas con títulos claros indicando el método y si usa \emph{jitter} o no.
  \end{itemize}
  \item Explicar brevemente qué hace cada bloque de código, por qué el dendrograma original no distinguía bien los niveles, cómo ayuda el \emph{jitter} en MIN y cómo interpretar los dendrogramas resultantes.
\end{itemize}
\end{quote}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ejercicio 2.3: Clasificación Supervisada Manual (Árboles de Decisión)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

En este ejercicio autónomo se construye un árbol de decisión de forma manual para clasificar tipos de vehículos. Se implementa la lógica del algoritmo de particionamiento recursivo (siguiendo la filosofía de algoritmos como Hunt o ID3/CART) pero programando explícitamente el cálculo detallado de la impureza y la ganancia de información para mostrar el proceso matemático subyacente.

\subsubsection{1. Fundamento Teórico}
Los árboles de decisión clasifican instancias ordenándolas desde el nodo raíz hasta algún nodo hoja. Para seleccionar el atributo óptimo de división en cada nodo, utilizamos medidas de impureza. En este ejercicio, empleamos el \textbf{Índice de Gini}, que mide la probabilidad de clasificar incorrectamente un elemento aleatorio.

\[ Gini(S) = 1 - \sum_{i=1}^{c} p_i^2 \]
Donde $p_i$ es la probabilidad de que una tupla en $S$ pertenezca a la clase $i$.

La \textbf{Ganancia de Información} al dividir un nodo $S$ usando un atributo $A$ se calcula como la reducción de impureza ponderada:
\[ Ganancia(S, A) = Gini(S) - \sum_{v \in Values(A)} \frac{|S_v|}{|S|} Gini(S_v) \]
El algoritmo selecciona en cada paso el atributo que maximiza esta ganancia.

\subsubsection{2. Definición del Conjunto de Datos}
El conjunto de datos consta de 10 observaciones con las siguientes características:
\begin{itemize}
    \item \textbf{Atributos Predictoras:} \textit{TipoCarnet} (A, B, N), \textit{Ruedas} (2, 4, 6), \textit{Pasajeros} (1, 2, 4, 5, 6).
    \item \textbf{Clase Objetivo:} \textit{Vehiculo} (Bicicleta, Camion, Coche, Moto).
\end{itemize}

<<ejercicio2_3_datos, echo=TRUE, results=verbatim>>=
# Definición del Dataset
datos_vehiculos <- data.frame(
  Carnet = c("B", "A", "N", "B", "B", "B", "N", "B", "B", "N"),
  Ruedas = c(4, 2, 2, 6, 4, 4, 2, 2, 6, 2),
  Pasajeros = c(5, 2, 1, 4, 6, 4, 2, 1, 2, 1),
  Vehiculo = c("Coche", "Moto", "Bicicleta", "Camion", "Coche", "Coche", 
               "Bicicleta", "Moto", "Camion", "Bicicleta"),
  stringsAsFactors = FALSE
)
print(datos_vehiculos)
@

\subsubsection{3. Funciones Implementadas para el Cálculo Detallado}
Para ilustrar el proceso matemático, se han implementado funciones que no solo devuelven el resultado, sino que imprimen la traza del cálculo:

\begin{enumerate}
    \item \textbf{\texttt{calcular\_gini}}: Aplica la fórmula $1 - \sum p_i^2$ sobre un vector de clases.
    \item \textbf{\texttt{evaluar\_division\_detallada}}: Realiza el proceso completo de evaluación de un atributo, imprimiendo paso a paso:
    \begin{itemize}
        \item El Gini del nodo padre.
        \item El análisis de cada rama (valor del atributo), mostrando su Gini individual y la fórmula de ponderación utilizada ($\frac{n_{sub}}{n_{total}} \times Gini_{sub}$).
        \item El cálculo final de la Ganancia restando el Gini ponderado total al Gini del padre.
    \end{itemize}
\end{enumerate}

<<ejercicio2_3_funciones_didacticas, echo=TRUE, results=hide>>=
# Función auxiliar: Cálculo del Índice de Gini
calcular_gini <- function(clases) {
  n <- length(clases)
  if (n == 0) return(0)
  conteos <- table(clases)
  proporciones <- conteos / n
  gini <- 1 - sum(proporciones^2)
  return(gini)
}

# Función didáctica: Evaluar atributo mostrando fórmulas
evaluar_division_detallada <- function(datos, atributo, clase_objetivo) {
  # 1. Gini del Padre
  gini_padre <- calcular_gini(datos[[clase_objetivo]])
  cat(sprintf("\n[ANALIZANDO ATRIBUTO: %s]\n", atributo))
  cat(sprintf("   1. Gini del Nodo Padre: %.4f\n", gini_padre))
  
  # 2. Análisis de Hijos
  cat("   2. Análisis de ramas (valores):\n")
  valores <- unique(datos[[atributo]])
  n_total <- nrow(datos)
  gini_ponderado_total <- 0
  formula_ponderada_txt <- c()
  
  for (val in valores) {
    # Subconjunto
    sub <- datos[datos[[atributo]] == val, ]
    n_sub <- nrow(sub)
    gini_hijo <- calcular_gini(sub[[clase_objetivo]])
    
    # Acumular ponderación
    peso <- n_sub / n_total
    gini_ponderado_total <- gini_ponderado_total + (peso * gini_hijo)
    
    # Texto para visualización
    clases_str <- paste(unique(sub[[clase_objetivo]]), collapse=",")
    cat(sprintf("      - Valor '%s' (n=%d): Clases={%s} -> Gini=%.4f\n", 
                val, n_sub, clases_str, gini_hijo))
    
    # Guardar fórmula para imprimir al final
    formula_ponderada_txt <- c(formula_ponderada_txt, 
                               sprintf("(%d/%d * %.4f)", n_sub, n_total, gini_hijo))
  }
  
  # 3. Calcular Ganancia
  ganancia <- gini_padre - gini_ponderado_total
  
  cat("   3. CÁLCULO FINAL:\n")
  cat(sprintf("      Gini Ponderado = Suma [ %s ]\n", 
              paste(formula_ponderada_txt, collapse=" + ")))
  cat(sprintf("      Gini Ponderado = %.4f\n", gini_ponderado_total))
  cat(sprintf("      GANANCIA = %.4f (Padre) - %.4f (Pond.) = >> %.4f <<\n", 
              gini_padre, gini_ponderado_total, ganancia))
  
  return(ganancia)
}
@

\subsubsection{4. Ejecución del Algoritmo Paso a Paso}

\paragraph{Paso 1: Selección del Nodo Raíz.}
Se evalúan los tres atributos disponibles con el conjunto completo ($N=10$). El objetivo es encontrar el que proporcione la mayor ganancia.

<<ejercicio2_3_paso1, echo=TRUE, results=verbatim>>=
cat("\n=== PASO 1: SELECCIÓN DEL PRIMER NODO (RAÍZ) ===\n")

g_carnet <- evaluar_division_detallada(datos_vehiculos, "Carnet", "Vehiculo")
g_ruedas <- evaluar_division_detallada(datos_vehiculos, "Ruedas", "Vehiculo")
g_pasajeros <- evaluar_division_detallada(datos_vehiculos, "Pasajeros", "Vehiculo")
@

\textbf{Análisis de Resultados (Paso 1):}
Observando la salida detallada:
\begin{itemize}
    \item \textbf{Carnet:} Ganancia 0.3733.
    \item \textbf{Ruedas: Ganancia 0.5000.} Es la máxima ganancia posible.
    \item \textbf{Pasajeros:} Ganancia 0.3067.
\end{itemize}
\textbf{Decisión:} Se selecciona \textbf{Ruedas} como raíz. Analizamos sus ramas:
\begin{itemize}
    \item Ruedas=4: Gini 0.0000 (Solo Coches) $\to$ Hoja.
    \item Ruedas=6: Gini 0.0000 (Solo Camiones) $\to$ Hoja.
    \item Ruedas=2: Gini 0.4800 (Mezcla Moto/Bicicleta) $\to$ Nodo Intermedio (Requiere subdivisión).
\end{itemize}

\paragraph{Paso 2: Subdivisión de la rama 'Ruedas = 2'.}
Filtramos los datos para trabajar solo con el subconjunto impuro (vehículos de 2 ruedas) y repetimos el proceso.

<<ejercicio2_3_paso2, echo=TRUE, results=verbatim>>=
cat("\n=== PASO 2: SUBDIVIDIR LA RAMA 'RUEDAS = 2' ===\n")

# Filtramos los datos
datos_ruedas2 <- datos_vehiculos[datos_vehiculos$Ruedas == 2, ]
print("Datos restantes en este nodo:")
print(datos_ruedas2)

# Evaluamos atributos restantes
g2_carnet <- evaluar_division_detallada(datos_ruedas2, "Carnet", "Vehiculo")
g2_pasajeros <- evaluar_division_detallada(datos_ruedas2, "Pasajeros", "Vehiculo")
@

\textbf{Análisis de Resultados (Paso 2):}
\begin{itemize}
    \item \textbf{Carnet: Ganancia 0.4800.} El Gini ponderado es 0.0000, lo que indica una clasificación perfecta en todas sus ramas.
    \item \textbf{Pasajeros:} Ganancia 0.0133.
\end{itemize}
\textbf{Decisión:} Se selecciona \textbf{Carnet} para dividir este nodo.

\subsubsection{5. Estructura Final del Árbol}
El algoritmo finaliza ya que todas las ramas han llegado a nodos con Gini 0 (hojas puras).

\begin{verbatim}
1. ¿Ruedas?
   |-- Si tiene 4: Coche (Hoja)
   |-- Si tiene 6: Camion (Hoja)
   |-- Si tiene 2: ¿Tipo de Carnet?
       |-- A: Moto (Hoja)
       |-- N: Bicicleta (Hoja)
       |-- B: Moto (Hoja)
\end{verbatim}

\subsubsection*{Prompt de IA Utilizado}
\begin{quote}
For Exercise 2.3, I need to generate a "Master Script" in R that manually implements a Decision Tree using the Gini Index, but with a highly didactic output. The script should define a specific function, \texttt{evaluar\_division\_detallada}, which not only returns the Information Gain but also prints the step-by-step math to the console: showing the Gini of the parent node, iterating through each attribute value to show the Gini of the child nodes, displaying the weighted sum formula (e.g., "(2/10 * 0.48) + ..."), and finally subtracting this from the parent's Gini to show the final Gain. The script must apply this function first to the full vehicle dataset to select the root, and then to the filtered subset for the impure branch, concluding with a text-based summary of the final tree structure.
\end{quote}

\end{document}

