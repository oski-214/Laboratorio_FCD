%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PREÁMBULO DE LATEX (Igual que antes)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper]{article}

% --- Paquetes Esenciales ---
\usepackage[utf8]{inputenc} % Codificación de entrada
\usepackage[T1]{fontenc}    % Codificación de fuentes
\usepackage[spanish]{babel} % Idioma español
\usepackage{graphicx}       % Para insertar imágenes (tus capturas)
\usepackage[a4paper,margin=2.5cm]{geometry} % Márgenes
\usepackage{Sweave}         % Estilos para Sweave

% --- Paquetes Adicionales Útiles ---
\usepackage{amsmath}        % Mejoras para matemáticas
\usepackage{amsfonts}       % Fuentes matemáticas
\usepackage{hyperref}       % Links clicables
\usepackage{float}          % Controlar posición de flotantes (figuras, tablas)
\usepackage{csquotes}       % Para citas textuales
\usepackage[backend=biber,style=ieee]{biblatex} % Bibliografía

\setlength{\parskip}{\baselineskip} % Un espacio entre párrafos

% \addbibresource{bibliografia.bib} % Descomenta esto y crea tu archivo .bib

% --- Configuración del Documento ---
\title{Memoria: Implementación del Algoritmo Apriori en R}
\author{Oscar Morán, Asier Álamo, Edgar Alexis Conforme, Lucía Díaz}
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INICIO DEL DOCUMENTO
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

% --- Portada ---
\begin{titlepage}
    \centering
    \vspace*{\fill}
    {\LARGE Universidad de Alcalá de Henares \\[0.5cm]}
    {\Large Escuela Politécnica Superior \\[1cm]}
    {\Huge \textbf{Fundamentos de la Ciencia de Datos} \\[1.5cm]}
    \Large\textbf{Autor:} \\[0.3cm]
    {\Large Tu Nombre \\[2cm]}
    \textbf{Fecha:} \\[0.3cm]
    {\Large \today} \\[1.5cm]
    \vspace*{\fill}
\end{titlepage}

\newpage
\tableofcontents % Tabla de contenido automática
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECCIONES DE LA MEMORIA
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introducción}
El objetivo de esta memoria es documentar el diseño e implementación de diversos algoritmos y técnicas de la ciencia de datos, utilizando el lenguaje de programación R.

Sweave nos permite generar este informe de manera dinámica, mezclando el texto explicativo con el código R que implementa los algoritmos.

\section{Resolución de Ejercicios}
A continuación, se detallan los ejercicios resueltos.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ejercicio 1.2: Análisis de asociación}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
El propósito de este ejercicio es identificar las relaciones y patrones de compra frecuentes entre distintos productos usando Apriori.

\subsubsection{Instalación y Carga de Paquetes}
El análisis de reglas de asociación en R se facilita mediante el paquete \texttt{arules}.  
El primer paso fue intentar cargar dicho paquete. Al no estar instalado, se procedió con su instalación desde el repositorio CRAN.  

<<>>=
# install.packages("arules")

library(arules)
@

\subsubsection{Creación y Formateo de los Datos}
Los datos de las cestas de la compra se representaron inicialmente como una matriz binaria, donde 1 indica la presencia de un artículo en una cesta y 0 su ausencia.  
Para poder utilizar el paquete arules, la matriz tuvo que ser convertida al formato \texttt{transactions}. Este proceso se realizó en varios pasos:

\begin{enumerate}
\item Convertir la matriz numérica a una matriz lógica ({ngCMatrix}).
\item Transponer la matriz para que los items queden en las filas.
\item Convertir la matriz transpuesta final al formato transactions.
\end{enumerate}

<<>>=
library(Matrix)

# Creación de la matriz binaria de cestas de compra
muestra <- Matrix(c(1,1,0,1,1,
                    1,1,1,1,0,
                    1,1,0,1,0,
                    1,0,1,1,0,
                    1,1,0,0,0,
                    0,0,0,1,0),
                  6, 5, byrow=TRUE,
                  dimnames = list(c("suceso1", "suceso2", "suceso3", 
                                    "suceso4", "suceso5", "suceso6"),
                                  c("Pan", "Agua", "Cafe", "Leche", "Naranja")),
                  sparse=TRUE)

# Conversión a matriz lógica y transposición
muestrangCMatrix <- as(muestra, "nsparseMatrix")
traspmuestrangCMatrix <- t(muestrangCMatrix)

# Conversión final al formato transactions
transacciones <- as(traspmuestrangCMatrix, "transactions")

# Visualizar las transacciones
transacciones
@

\subsubsection{Análisis del Conjunto de Datos}
Con los datos en el formato correcto, se utilizó la función summary() para obtener una visión general de las transacciones.  
El resumen indicó que ``Pan'' y ``Leche'' son los artículos más frecuentes (presentes en 5 de las 6 cestas), y que el tamaño de las cestas varía de 1 a 4 artículos.

<<>>=
summary(transacciones)
@

\subsubsection{Aplicación del Algoritmo Apriori}
Se aplicó el algoritmo apriori para extraer las reglas de asociación.  
Se establecieron los siguientes umbrales.
Tras la ejecución del algoritmo, se generaron 7 reglas de asociación.

\begin{itemize}
\item \textbf{Soporte mínimo (support)} = 0.5 → la combinación de artículos debe aparecer en al menos el 50\% de todas las transacciones (3 o más cestas).
\item \textbf{Confianza mínima (confidence)} = 0.8 → la regla A ⇒ B debe cumplirse al menos el 80\% de las veces que aparece A.
\end{itemize}

<<>>=
asociaciones <- apriori(transacciones,
                        parameter = list(support = 0.5, confidence = 0.8))

# Mostrar las reglas generadas
inspect(asociaciones)
@

\subsubsection{Conclusiones}
Del análisis de las reglas se extraen varias conclusiones importantes:

\begin{enumerate}
\item \textbf{Productos Estrella:} ``Pan'' y ``Leche'' son los productos más comunes, como indican las reglas 1 y 2 y su soporte de 0.833.
\item \textbf{Asociación Fuerte (Agua y Pan):} La regla \texttt{\{Agua\} ⇒ \{Pan\}} tiene una confianza del 100\%, lo que significa que cada vez que un cliente compró ``Agua'', también compró ``Pan''. El \texttt{lift} de 1.20 indica que esta asociación es un 20\% más frecuente de lo esperado por azar.
\item \textbf{Asociación Recíproca:} La regla \texttt{\{Pan\} ⇒ \{Agua\}} también es fuerte, con una confianza del 80\%.
\item \textbf{Regla de Tres Productos:} La regla \texttt{\{Agua, Leche\} ⇒ \{Pan\}} tiene una confianza del 100\%, lo que implica que los tres clientes que compraron ``Agua'' y ``Leche'' juntos también compraron ``Pan''.
\end{enumerate}

En resumen, el análisis revela una fuerte conexión de compra entre los productos ``Pan'' y ``Agua'', y también una relación significativa entre ``Pan'' y ``Leche''.
\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ejercicio 1.3: Detección de Datos Anómalos}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

En este ejercicio se aplican tres métodos distintos para la detección de datos anómalos sobre un conjunto de datos de muestra.

\subsubsection*{Carga de Datos de Muestra}
A continuación, se definen los datos de muestra para este ejercicio.

<<DatosAnomalos_Setup, echo=TRUE, results=verbatim>>=
(muestra<-t(matrix(c(3,2,3.5,12,4.7,4.1,5.2,4.9,7.1,6.1,6.2,5.2,14,5.3),2,7,dimnames=list(c("r", "d")))))
(muestra<-data.frame(muestra))
@

\subsubsection{Método 1: Caja y Bigotes}

\subsubsection*{Explicación del Método}
Para la detección de datos anómalos por este método se debe llevar a cabo siguiendo 4 pasos. $1^o$ se determina el valor del grado del outlier ($d$).
$2^o$ Se ordenan los datos para el parámetro que se quiere detectar los datos anómalos, en este caso la resistencia o r. $3^o$ Se calculan los valores del
$cuartil_{1}$ y del $cuartil_{3}$. $4^o$ se observa que valores quedan fuera del intervalo, el cual sería: $[cuartil_{1} - d * (cuartil_{3} - cuartil_{1}), cuartil_{3} + d * (cuartil_{3} - cuartil_{1})]$


\subsubsection*{Código y Ejecución (Sweave)}
<<Anomalos_Boxplot, echo=TRUE, results=verbatim>>=
# 1. Metodo de Caja y Bigotes (para la columna 'r')
(boxplot(muestra$r, range=1.5, plot=FALSE))

# Calculo manual
(cuar1r<-quantile(muestra$r, 0.25))
(cuar3r<-quantile(muestra$r, 0.75))
(int<-c(cuar1r-1.5*(cuar3r-cuar1r), cuar3r+1.5*(cuar3r-cuar1r)))

# Bucle para detectar outliers
for (i in 1:length(muestra$r))
{if (muestra$r[i]<int[1] || muestra$r[i]>int[2])
{print("el suceso"); print(i); print(muestra$r[i]); print("es un outlier")}}
@

\subsubsection{Método 2: Desviación Típica}

\subsubsection*{Explicación del Método}
Para la detección de datos anómalos por este método se debe llevar a cabo los siguiente 4 pasos. $1^o$ se determina el valor del grado de outlier($d$).
$2^o$ se debe hallar el valor de la media para el parámetro que se nos halla pedido, en este caso densidad o d (d_{mean}). $3^o$ Se debe calcular el valor de la 
desviación típica para dicho parámetro ($d_{desv}$). $4^o$ Se debe ver que valores quedan fuera del intervalo, el cual sería: $[d_{mean} - d * d_{desv}, d_{mean} + d * d_{desv}]$


\subsubsection*{Código y Ejecución (Sweave)}
<<Anomalos_SD, echo=TRUE, results=verbatim>>=
# 2. Metodo de Desviacion Tipica (para la columna 'd')

# Calculo de la desviacion tipica poblacional
sdd<-sqrt(var(muestra$d)*((length(muestra$d)-1)/length(muestra$d)))

# Calculo del intervalo
(intdes<-c(mean(muestra$d)-2*sdd, mean(muestra$d)+2*sdd))

# Bucle para detectar outliers
for (i in 1:length(muestra$d))
{if (muestra$d[i]<intdes[1] ||  muestra$d[i]>intdes[2])
{print("el suceso"); print(i); print(muestra$d[i]); print("es un outlier")}}
@

\subsubsection{Método 3: Regresión (Error de Residuos)}

\subsubsection*{Explicación del Método}
\subsubsection*{Explicación del Método}
Para la detección de datos anómalos por este método se deben llevar a cabo los 7 siguientes pasos. $1^o$ Determinar el valor del grado de outlier ($d$). 
$2^o$ Se deben hallar los valores de $a$ y $b$ para poder tener una forma de predecir los valores de la densidad que vamos a llamar ($y_{hat}$). 
Para esto se tiene que hallar el valor de la covarianza, ya que el valor de $b$ sería la división del valor de la covarianza / la varianza de la resistencia ($x$); por último para hallar $a$: $a = y_{mean} - b \times x_{mean}$. De esta 
forma ya se puede obtener predicciones de los valores de la densidad: $y_{hat} = a + b \times x_i$. $3^o$ Se obtienen los valores predichos de la densidad para cada valor de la resistencia ($x_i$). 
$4^o$ Se calcula el error estandar de los residuos ($Sr$). $5^o$ Se obtiene el valor identicador de outliers que sería $d \times Sr$. $6^o$ Se obtiene el valor absoluto de la diferencia entre 
los valores de $y_i$ y los $y_{hat}$. $7^o$ Aquellos valores cuya diferencia supere el valor de $d \times Sr$ serán considerados outliers.

\subsubsection*{Código y Ejecución (Sweave)}
<<Anomalos_Regression, echo=TRUE, results=verbatim>>=
# 3. Metodo de Regresion (para 'd' en funcion de 'r')

# Calculo del modelo de regresion lineal
(dfr<-lm(muestra$d~muestra$r))
(summary(dfr))

# Obtencion de residuos
(res<-summary(dfr)$residuals)

# Calculo del error estandar de los residuos
sr<-sqrt(sum(res^2)/length(res))
sr

# Bucle para detectar outliers en los residuos
for (i in 1:length(res))
{if (res[i]>2*sr)
{print("el suceso"); print(res[i]); print("es un outlier")}}
@

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ejercicio 1.4: Detección de datos anómalos mediante el método de proximidad (K-NN manual)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Descripción del conjunto de datos}

El conjunto de datos utilizado está formado por las calificaciones de cinco estudiantes, donde cada observación se compone de dos variables cuantitativas: la nota de \textbf{Teoría} y la de \textbf{Laboratorio}.  
Las observaciones se representan como pares ordenados:
\[
(4,4), \; (4,3), \; (5,5), \; (1,1), \; (5,4)
\]
El objetivo del ejercicio es identificar posibles \emph{datos anómalos} o \emph{outliers} que se alejen significativamente del resto del conjunto, utilizando un método basado en la \textbf{proximidad} entre observaciones en el espacio bidimensional de calificaciones.

\subsubsection*{Fundamento teórico}

El método de los \textit{K-vecinos más próximos} (K-NN) para detección de anomalías se fundamenta en el supuesto de que los puntos normales se encuentran próximos entre sí, mientras que los puntos anómalos se sitúan aislados a mayor distancia de sus vecinos.  

Formalmente, para cada observación \(x_i = (x_{i1}, x_{i2})\), se calcula la distancia euclídea respecto a las demás:
\[
d_{ij} = \sqrt{(x_{i1} - x_{j1})^2 + (x_{i2} - x_{j2})^2}
\]
Estas distancias se ordenan de menor a mayor, y se obtiene la distancia al K-ésimo vecino más próximo, \(d_K(x_i)\).  
Si este valor supera un umbral prefijado \(d^*\), la observación se considera un \emph{dato anómalo}:
\[
x_i \text{ es anómalo si } d_K(x_i) > d^*
\]
En este caso se utiliza \(K = 3\) y \(d^* = 2.5\).

\subsubsection*{Implementación en R}
El algoritmo mencionado es implementado de forma completa mediante el uso de funciones de base en R, sin recurrir a paquetes externos.

\paragraph{1) Definición del conjunto de datos.}
El conjunto de datos, como se ha mencionado, está formado por cinco estudiantes con sus respectivas calificaciones en dos dimensiones: \textbf{Teoría} y \textbf{Laboratorio}.

<<crear_matriz, echo=TRUE, results=verbatim>>=
# Creación de la matriz (2 variables x 5 observaciones)
muestra <- matrix(c(4,4,  4,3,  5,5,  1,1,  5,4), nrow = 2, ncol = 5)
muestra
@

En la matriz anterior las observaciones se disponen por columnas, es decir, cada \textbf{columna} representa a un estudiante y cada \textbf{fila} una variable (Teoría, Laboratorio).
Sin embargo, las funciones de análisis de distancias en R esperan que las observaciones estén en filas.
Por ello, se realiza una transposición de la matriz para adecuarla a este formato:

<<transposicion, echo=TRUE, results=verbatim>>=
# Transposición: filas = estudiantes, columnas = variables
muestra <- t(muestra)
muestra
@

De este punto en adelante, cada fila representa a un estudiante y cada columna a una calificación.

\paragraph{2) Cálculo de distancias euclídeas.}
Se calculan todas las distancias euclídeas entre los pares de observaciones.  
La distancia entre dos puntos \( (x_1, y_1) \) y \( (x_2, y_2) \) se define como:

\[
d = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}
\]

En R, esto se implementa con la función \texttt{dist()}:

<<distancias, echo=TRUE, results=verbatim>>=
# Cálculo de la matriz de distancias euclídeas
distancia <- as.matrix(dist(muestra))
distancia
@

Cada elemento \( d_{ij} \) de la matriz representa la distancia entre los estudiantes \( i \) y \( j \). 
Todos los elementos de la diagonal son cero, ya que la distancia de un punto a sí mismo es nula.

\paragraph{3) Ordenación de las distancias.}
Con el objetivo de encontrar los K-vecinos más próximos, se ordenan las distancias para cada observación en orden ascendente:

<<ordenar_distancias, echo=TRUE, results=verbatim>>=
# Ordenar distancias (de menor a mayor) para cada observación
for (i in 1:5) {
  distancia[, i] <- sort(distancia[, i])
}
distanciasord <- distancia
distanciasord
@

La primera fila de cada columna es siempre cero (distancia al mismo punto), seguida por las distancias a los vecinos más cercanos.
La segunda y tercera filas indican los vecinos más próximos, mientras que la cuarta fila contiene la distancia al tercer vecino más cercano (\( K = 3 \)).

\paragraph{4) Detección de valores anómalos.}
Se establece un umbral de decisión \( d^* = 2.5 \).  
Aquellas observaciones cuya distancia al tercer vecino supere el umbral se consideran anómalas:

<<detectar_outliers, echo=TRUE, results=verbatim>>=
# Umbral de decisión
umbral <- 2.5

# Identificación de outliers
for (i in 1:5) {
  if (distanciasord[4, i] > umbral) {
    print(i)
    print("es un outlier")
  }
}
@

El resultado muestra:

\begin{verbatim}
[1] 4
[1] "es un outlier"
\end{verbatim}

Esto indica que el estudiante 4, con calificaciones (1,1), es un dato anómalo según el criterio establecido.

\paragraph{5) Interpretación y análisis.}
En el espacio bidimensional de las calificaciones, los puntos \((4,4)\), \((4,3)\), \((5,5)\) y \((5,4)\) se agrupan cercanamente, representando rendimientos similares.  
En contraste, el punto \((1,1)\) está alejado del resto, a una distancia superior a cuatro unidades euclídeas.  
Este aislamiento explica su clasificación como outlier mediante el criterio de proximidad.

Analizándolo de forma geométrica, las observaciones se encuentran en la zona superior derecha del plano, mientras que el punto anómalo se sitúa en la esquina inferior izquierda.
Este método basado en la distancia euclídea ha permitido identificar eficazmente la observación que difiere significativamente del patrón general del conjunto de datos, en base a los criterios de proximidad establecidos.

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ejercicio 2.1: Análisis Descriptivo de Distancias}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Descripción del conjunto de datos y objetivo}
En este ejercicio se hace uso de un fichero \texttt{distancias.txt}, que dispone de observaciones emparejadas de:
\begin{itemize}
\item \textbf{Población de residencia} (variable cualitativa nominal), y
\item \textbf{Distancia en km} desde el domicilio del estudiante hasta la Universidad (variable cuantitativa continua).
\end{itemize}
El enunciado cuenta con el universo de datos y los valores a emplear en este apartado, por ejemplo: \emph{Villalbilla, 16.5; Ensanche de Vallecas, 34.8; Alcalá de Henares, 6.2; …; Madrid, 46; ND, 3.7; …}. La presencia del marcador \textbf{ND} indica casos con localidad no disponible para esa observación, mientras que la distancia sí está informada. Estos pares {población, distancia} constituyen la base para el \textbf{análisis descriptivo} requerido en el ejercicio 2.1: ordenar por distancia, calcular rango, tablas de frecuencias (absoluta, acumulada, relativa) e indicadores de tendencia y dispersión sobre la variable cuantitativa.

\paragraph{Estructura del fichero.}
El fichero \texttt{distancias.txt} se lee con cabecera (\texttt{header = TRUE}) y contiene dos columnas:
\begin{center}
\begin{tabular}{ll}
\texttt{población} & (carácter/factor): nombre de la población o \texttt{ND} si no consta. \\[2pt]
\texttt{distancia} & (numérico, km): distancia real en kilómetros con decimales. \\
\end{tabular}
\end{center}

\paragraph{Calidad y decisiones sobre datos.}
\begin{itemize}
\item \textbf{Unidades y formato:} la distancia está en kilómetros; se preservan los decimales tal y como aparecen en el enunciado (p. ej., 16.5, 34.8, 31.4).
\item \textbf{Valores ND en \texttt{población}:} no impiden el análisis cuantitativo porque la métrica principal se aplica sobre \texttt{distancia}. Se mantienen sin imputación ni eliminación, preservando la consistencia del conjunto.
\item \textbf{Duplicados intencionales:} algunas parejas se repiten (p. ej., \emph{Cifuentes, 24} o varias \emph{Guadalajara, 30}), lo que es coherente con un conteo de frecuencias donde se esperan empates de distancia por redondeo.
\end{itemize}

\subsubsection*{Fundamentos teóricos del análisis descriptivo}

El análisis estadístico descriptivo tiene como finalidad resumir y comprender la información contenida en un conjunto de datos mediante medidas que describen su tendencia central, su variabilidad y su forma. En este ejercicio se aplican dichos conceptos sobre la variable \texttt{distancia}, de naturaleza cuantitativa continua.

\paragraph{Tipos de variables.}
De acuerdo con la tipología presentada en la Lección 1:
\begin{itemize}
  \item Una \textbf{variable cualitativa nominal} (como \texttt{población}) clasifica observaciones en categorías sin orden inherente.
  \item Una \textbf{variable cuantitativa continua} (como \texttt{distancia}) toma valores numéricos dentro de un rango real y admite decimales.
\end{itemize}

\paragraph{Frecuencias y distribuciones.}
Sea un conjunto de observaciones $x_1, x_2, \ldots, x_n$.  
La \textbf{frecuencia absoluta} $f_i$ indica cuántas veces aparece cada valor o intervalo.  
La \textbf{frecuencia relativa} $r_i$ se define como $r_i = f_i / n$, expresando la proporción de casos.  
Las \textbf{frecuencias acumuladas} ($F_i$, $R_i$) suman las observaciones hasta cada valor:
\[
F_i = \sum_{j \le i} f_j, \qquad
R_i = \sum_{j \le i} r_j
\]
Estas medidas permiten construir la distribución empírica de la variable y analizar su forma.

\paragraph{Medidas de tendencia central.}
Entre los valores posibles de una variable cuantitativa, la \textbf{media aritmética} representa el centro de gravedad de la distribución:
\[
\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i
\]
La media es sensible a valores extremos y resume el nivel típico de la variable.

\paragraph{Medidas de dispersión.}
La \textbf{dispersión} refleja cuánto se alejan los datos de la media:
\begin{itemize}
  \item El \textbf{rango} mide la amplitud total de la distribución: $\max(x) - \min(x)$.
  \item La \textbf{varianza poblacional} cuantifica la media de las desviaciones cuadráticas:
  \[
  \sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \bar{x})^2
  \]
  \item La \textbf{varianza muestral} incorpora la corrección de Bessel:
  \[
  s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2
  \]
  \item La \textbf{desviación estándar} es la raíz cuadrada de la varianza y mantiene las mismas unidades que la variable original.
\end{itemize}

\paragraph{Interpretación aplicada.}
En el contexto de este ejercicio, la media representa la distancia promedio de los estudiantes al campus, mientras que el rango y la desviación estándar indican la heterogeneidad geográfica del grupo. Una desviación alta implica dispersión en la procedencia, y una baja, concentración en torno a una distancia media.


\subsubsection*{Definición y explicación de las funciones utilizadas}

\paragraph{1. Funciones de conteo y estructura de datos.}
\begin{itemize}
\item \textbf{\texttt{contar\_filas(vec)}}: esta función toma un vector y recorre sus elementos haciendo uso de un bucle \texttt{for}. En cada iteración incrementa un contador inicializado a cero. El resultado final es el número total de elementos del vector. Se trata de un equivalente de la función \texttt{length()} de R, pero implementada manualmente. El mecanismo subyacente consiste en una lectura secuencial del vector y acumulación iterativa. El valor retornado permite validar la carga correcta del fichero y es utilizado posteriormente para el cálculo de frecuencias relativas. \ \textit{Argumentos:} \texttt{vec} (vector de cualquier tipo). \ \textit{Devuelve:} un entero con el número total de elementos.

<<contar_filas, eval=TRUE, echo=TRUE, results=hide>>=
# Función para contar filas 
contar_filas <- function(vec) {
  # Se inicializa el contador
  n <- 0
  # Se itera elemento a elemento y se incrementa el contador
  for (._ in vec) n <- n + 1
  # Se retorna el total contado
  n
}
@

\item \textbf{\texttt{contar\_columnas(df)}}: de modo análogo a la función anterior, recorre la lista de nombres de las columnas (\texttt{names(df)}) y aplica el mismo proceso de conteo. Permite verificar la integridad estructural del conjunto de datos leído. \ \textit{Argumentos:} \texttt{df} (\texttt{data.frame}). \ \textit{Devuelve:} un entero con el número de columnas.

<<contar_columnas, eval=TRUE, echo=TRUE, results=hide>>=
# Función para contar columnas 
contar_columnas <- function(df) {
  # Se inicializa el contador
  c <- 0
  # Se itera sobre los nombres de las columnas y se incrementa el contador
  for (._ in names(df)) c <- c + 1
  # Se retorna el total contado
  c
}
@

\item \textbf{\texttt{contar\_elementos(x)}}: versión genérica aplicable a cualquier tipo de vector. Es un ejemplo de \emph{abstracción algorítmica}, ya que encapsula el patrón de conteo iterativo en una única función reutilizable, sin depender del tipo de dato.

<<contar_elementos, eval=TRUE, echo=TRUE, results=hide>>=
# Función para contar cuántos elementos hay en un vector (sin length)
contar_elementos <- function(x) {
  # Se inicializa el contador
  n <- 0
  # Se incrementa por cada elemento del vector
  for (._ in x) n <- n + 1
  # Se retorna el total
  n
}
@

\paragraph{2. Función de ordenación: \texttt{ordenar\_indices(x, decreasing = FALSE)}.}
Esta función reproduce el algoritmo clásico de \emph{ordenamiento por burbuja}, donde se realizan comparaciones sucesivas entre pares adyacentes mediante estructuras de control anidadas. El vector de entrada \texttt{x} no se reordena directamente, sino que se construye un vector de \emph{índices} que indica el orden en que los elementos deberían aparecer para estar ordenados. Durante el proceso, se comparan los pares adyacentes e intercambian los índices cuando la condición de orden no se cumple (\texttt{x[i] > x[i+1]} o la inversa si \texttt{decreasing = TRUE}). Tras varias pasadas, los elementos mayores “flotan” hacia el final del vector. \ \textit{Argumentos:} \texttt{x} (vector numérico o comparable), \texttt{decreasing} (lógico, indica si el orden debe ser descendente). \ \textit{Devuelve:} un vector de índices con el orden resultante.

<<ordenar_indices, eval=TRUE, echo=TRUE, results=hide>>=
# Función para ordenar índices 
ordenar_indices <- function(x, decreasing = FALSE) {
  # Se cuenta manualmente el número de elementos
  n <- 0
  for (._ in x) n <- n + 1
  # Se crea un vector con los índices 1..n
  idx <- 1:n
  # Se aplica un algoritmo de ordenamiento (burbuja)
  for (i in 1:(n - 1)) {
    for (j in (i + 1):n) {
      # Si el orden es ascendente se compara menor que, si es descendente mayor que
      if ((!decreasing && x[j] < x[i]) || (decreasing && x[j] > x[i])) {
        # Intercambiar elementos
        tmp <- x[i]; x[i] <- x[j]; x[j] <- tmp
        # Intercambiar índices
        tmp2 <- idx[i]; idx[i] <- idx[j]; idx[j] <- tmp2
      }
    }
  }
  # Se retornan los índices ordenados
  idx
}
@

\paragraph{3. Función de rango: \texttt{rango\_manual(x)}.}
El rango se define como la diferencia entre el valor máximo y el mínimo del conjunto de datos. En lugar de usar las funciones \texttt{max()} y \texttt{min()}, esta función inicia dos variables (\texttt{max} y \texttt{min}) con el primer valor del vector y las actualiza secuencialmente si encuentra un valor mayor o menor. Al finalizar, devuelve la diferencia entre ambos valores, cuantificando la amplitud total de la variable. El rango es una medida de dispersión simple que representa la diferencia entre el estudiante más próximo y el más lejano a la universidad. \ \textit{Argumentos:} \texttt{x} (vector numérico). \ \textit{Devuelve:} un valor numérico correspondiente al rango.

<<rango_manual, eval=TRUE, echo=TRUE, results=hide>>=
# Función para calcular el rango 
rango_manual <- function(x) {
  # Inicializar máximo y mínimo con el primer valor
  maximo <- x[1]
  minimo <- x[1]
  # Iterar sobre los valores para encontrar máximo y mínimo
  for (valor in x) {
    # Si el valor es mayor que el máximo actual, actualizar máximo
    if (valor > maximo) maximo <- valor
    # Si el valor es menor que el mínimo actual, actualizar mínimo
    if (valor < minimo) minimo <- valor
  }
  # Retornar el rango
  maximo - minimo
}
@

\paragraph{4. Obtención de valores únicos: \texttt{valores\_unicos\_manual(x)}.}
Esta función construye un nuevo vector \texttt{unicos} inicializado vacío y recorre todos los elementos de \texttt{x}. En cada iteración comprueba si el valor actual ya está dentro de \texttt{unicos}; si no lo está, se incorpora mediante concatenación. Al finalizar el bucle, se ordenan los valores resultantes haciendo uso de los índices obtenidos con la función \texttt{ordenar\_indices()}. Este proceso combina una búsqueda secuencial (comparación elemento a elemento) con una inserción condicional, y es esencial para construir las tablas de frecuencias. \ \textit{Argumentos:} \texttt{x} (vector numérico o de caracteres). \ \textit{Devuelve:} un vector de valores únicos ordenados.

<<valores_unicos_manual, eval=TRUE, echo=TRUE, results=hide>>=
# Función para obtener valores únicos
valores_unicos_manual <- function(x) {
  # En primer lugar, se crea un vector vacío para almacenar los valores únicos
  unicos <- c()
  # Se itera sobre cada valor
  for (v in x) {
    # Flag para indicar si el valor ya fue encontrado
    encontrado <- FALSE
    # Se verifica si el valor ya está en el vector de valores únicos
    for (u in unicos) {
      if (v == u) { encontrado <- TRUE; break }
    }
    # Si el valor no fue encontrado, se agrega al vector de valores únicos
    if (!encontrado) unicos <- c(unicos, v)
  }
  # Se ordenan los valores únicos y se retornan
  unicos[ordenar_indices(unicos)]
}
@

\paragraph{5. Frecuencias: conteo, acumulación y normalización.}
\begin{itemize}
\item \textbf{\texttt{frecuencia\_absoluta\_manual(x, valores\_ordenados)}}: recibe el vector original y el conjunto de valores únicos previamente ordenado. Para cada valor único, recorre nuevamente el vector \texttt{x} y aumenta el contador correspondiente cuando encuentra coincidencias. El resultado es un vector de igual longitud que \texttt{valores\_ordenados}, donde cada posición representa cuántas veces aparece el valor correspondiente. La función utiliza una estructura de bucles anidados que refuerza la comprensión del proceso de clasificación en categorías. Reproduce el comportamiento de \texttt{table()} de R, pero implementado manualmente. \ \textit{Argumentos:} \texttt{x} (vector de datos), \texttt{valores\_ordenados} (vector de únicos). \ \textit{Devuelve:} un vector numérico con las frecuencias absolutas.

<<frecuencia_absoluta_manual, eval=TRUE, echo=TRUE, results=hide>>=
# Función para calcular la frecuencia absoluta 
frecuencia_absoluta_manual <- function(x, valores_ordenados) {
  # Se obtiene el número de valores únicos
  k <- contar_elementos(valores_ordenados)
  # Inicializar el vector de frecuencias absolutas
  frec <- numeric(k)
  # Se itera sobre cada valor observado
  for (v in x) {
    # Se compara con cada valor único
    for (i in 1:k) {
      # Si coincide, se incrementa la frecuencia absoluta correspondiente
      if (v == valores_ordenados[i]) { frec[i] <- frec[i] + 1; break }
    }
  }
  # Se retorna el vector de frecuencias
  frec
}
@

\item \textbf{\texttt{acumulada\_manual(vec)}}: calcula la suma progresiva de los elementos de un vector. Se inicializa un acumulador que, en cada iteración, añade el valor actual y lo almacena en un nuevo vector. Esta función ejemplifica un algoritmo iterativo que modela una suma acumulada sin funciones de orden superior. \ \textit{Argumentos:} \texttt{vec} (vector numérico). \ \textit{Devuelve:} un vector numérico de igual longitud con las sumas acumuladas.

<<acumulada_manual, eval=TRUE, echo=TRUE, results=hide>>=
# Función para calcular la frecuencia acumulada 
acumulada_manual <- function(vec) {
  # Se cuenta cuántos elementos tiene el vector
  n <- contar_elementos(vec)
  # Inicializar el vector de frecuencias acumuladas
  out <- numeric(n)
  # Se suman progresivamente las frecuencias
  acum <- 0
  for (i in 1:n) { acum <- acum + vec[i]; out[i] <- acum }
  # Se retorna la acumulada
  out
}
@

\item \textbf{\texttt{relativa\_manual(frec\_abs, total)}}: transforma las frecuencias absolutas en relativas dividiendo cada componente de \texttt{frec\_abs} entre el número total de observaciones. La precisión de este paso depende de que \texttt{total} coincida con el número real de filas, garantizado mediante \texttt{contar\_filas()}. El resultado permite interpretar los valores en términos porcentuales y facilita la construcción de histogramas o distribuciones empíricas. \ \textit{Argumentos:} \texttt{frec\_abs} (vector numérico), \texttt{total} (entero). \ \textit{Devuelve:} un vector de frecuencias relativas en $[0,1]$.
\end{itemize}

<<relativa_manual, eval=TRUE, echo=TRUE, results=hide>>=
# Función para calcular la frecuencia relativa manual
relativa_manual <- function(frec_abs, total) {
  # Se cuenta el tamaño del vector de frecuencias absolutas
  n <- contar_elementos(frec_abs)
  # Inicializar el vector de frecuencias relativas
  out <- numeric(n)
  # Se calcula dividiendo la frecuencia absoluta entre el total de filas
  for (i in 1:n) out[i] <- frec_abs[i] / total
  # Se retorna la frecuencia relativa
  out
}
@
\end{itemize}

\paragraph{6. Medidas de tendencia central y dispersión.}
\begin{itemize}
\item \textbf{\texttt{media\_manual(x)}}: la media se obtiene sumando todos los elementos del vector y dividiendo por el número total de observaciones. Los valores se acumulan en un bucle \texttt{for}, asegurando que el acumulador se inicialice a cero para evitar errores. Este proceso muestra explícitamente cómo la media representa el centro de gravedad de la distribución de datos. \ \textit{Argumentos:} \texttt{x} (vector numérico). \ \textit{Devuelve:} la media aritmética.

<<media_manual, eval=TRUE, echo=TRUE, results=hide>>=
# Función para calcular la media 
media_manual <- function(x) {
  # Se incializa la suma a cero
  suma <- 0
  # Se cuenta el número de elementos
  n <- contar_elementos(x)
  # Se suma cada valor
  for (v in x) suma <- suma + v
  # Se retorna la media
  suma / n
}
@

\item \textbf{\texttt{varianza\_poblacional\_manual(x)}} y \textbf{\texttt{varianza\_muestral\_manual(x)}}: ambas funciones siguen la definición formal de la varianza. Primero se calcula la media con \texttt{media\_manual(x)}, luego se recorre el vector calculando la desviación cuadrática de cada elemento respecto a la media, y se acumula el resultado. La versión muestral aplica la corrección de Bessel ($N-1$) para eliminar el sesgo en la estimación de la varianza poblacional. La comparación entre ambas versiones permite apreciar cómo la corrección de Bessel evita la subestimación sistemática de la varianza cuando solo se dispone de una muestra. \ \textit{Argumentos:} \texttt{x} (vector numérico). \ \textit{Devuelve:} un valor numérico (varianza poblacional o muestral).

<<varianzas, eval=TRUE, echo=TRUE, results=hide>>=
# Función para calcular la varianza poblacional 
varianza_poblacional_manual <- function(x) {
  # Se cuenta el número de elementos
  n <- contar_elementos(x)
  # Se calcula la media manualmente
  m <- media_manual(x)
  # Se calcula la suma de los cuadrados de las diferencias respecto a la media
  suma_cuad <- 0
  for (v in x) suma_cuad <- suma_cuad + (v - m)^2
  # Varianza poblacional: dividir entre N
  suma_cuad / n
}

# Función para calcular la varianza muestral 
varianza_muestral_manual <- function(x) {
  # Se cuenta el número de elementos
  n <- contar_elementos(x)
  # Si no hay suficientes datos, se retorna NA
  if (n <= 1) return(NA_real_)
  # Se calcula la media manualmente
  m <- media_manual(x)
  # Se calcula la suma de los cuadrados de las diferencias respecto a la media
  suma_cuad <- 0
  for (v in x) suma_cuad <- suma_cuad + (v - m)^2
  # Varianza muestral: dividir entre (N - 1)
  suma_cuad / (n - 1)
}
@

\item \textbf{\texttt{desv\_estandar(varianza)}}: toma la raíz cuadrada de la varianza mediante el operador \texttt{sqrt()}, devolviendo una medida de dispersión expresada en las mismas unidades que la variable original (kilómetros). Esta equivalencia de unidades facilita la interpretación práctica, ya que permite afirmar, por ejemplo, que “la distancia media difiere en aproximadamente 20 km respecto al promedio”. \ \textit{Argumentos:} \texttt{varianza} (valor numérico). \ \textit{Devuelve:} la desviación estándar.

<<desv_estandar, eval=TRUE, echo=TRUE, results=hide>>=
# Función para obtener la desviación estándar a partir de una varianza
desv_estandar <- function(varianza) {
  # Si la varianza es NA, se retorna NA
  if (is.na(varianza)) return(NA_real_)
  # Se calcula la raíz cuadrada de la varianza
  sqrt(varianza)
}
@
\end{itemize}

\paragraph{7. Integración en el flujo principal.}
Las funciones anteriores se integran secuencialmente en el flujo de trabajo del ejercicio:
\begin{enumerate}
\item Carga del fichero \texttt{distancias.txt} en el \texttt{data.frame} \texttt{s}.
\item Verificación estructural mediante las funciones de conteo.
\item Ordenación de observaciones y creación de los data frames \texttt{so\_asc} y \texttt{so\_desc}.
\item Cálculo del rango, valores únicos y tabla de frecuencias.
\item Derivación de medidas de resumen (media, varianzas y desviación estándar).
\end{enumerate}
Cada función ha sido implementada con independencia modular, lo que facilita su verificación individual, su reutilización en otros análisis y la trazabilidad completa del proceso estadístico.

\subsubsection*{Ejecución del script principal}

En este bloque se ejecuta el flujo completo del ejercicio, integrando todas las funciones definidas anteriormente.  
El objetivo es validar su correcto funcionamiento y mostrar los resultados del análisis descriptivo sobre el conjunto de datos \texttt{distancias.txt}.

<<ejecucion_principal, eval=TRUE, echo=TRUE, results=verbatim>>=
# ============================
# Script principal 
# ============================

# Se lee el archivo de datos
s <- read.table("distancias.txt", header = TRUE)
print(s)

# Contar filas y columnas manualmente
filas <- contar_filas(s$distancia)
columnas <- contar_columnas(s)
cat("Dimensiones (filas x columnas): ", filas, " x ", columnas, "\n", sep = "")

# Ordenación ascendente y descendente
idx_asc  <- ordenar_indices(s$distancia)
idx_desc <- ordenar_indices(s$distancia, decreasing = TRUE)
so_asc  <- s[idx_asc, ]
so_desc <- s[idx_desc, ]
cat("Orden ascendente por distancia:\n"); print(so_asc)
cat("\nOrden descendente por distancia:\n"); print(so_desc); cat("\n")

# Rango
rangor <- rango_manual(s$distancia)
cat("Rango (max - min):", rangor, "\n\n")

# Valores únicos y frecuencias
valores_unicos <- valores_unicos_manual(s$distancia)
n_valores <- contar_elementos(valores_unicos)
frecuencia_abs <- frecuencia_absoluta_manual(s$distancia, valores_unicos)
frecuencia_acum <- acumulada_manual(frecuencia_abs)
frecuencia_rel <- relativa_manual(frecuencia_abs, filas)
frecuencia_rel_acum <- acumulada_manual(frecuencia_rel)

# Mostrar frecuencias
cat("Frecuencia absoluta:\n")
for (i in 1:n_valores) cat(valores_unicos[i], ":", frecuencia_abs[i], "\n")
cat("\nFrecuencia acumulada:\n")
for (i in 1:n_valores) cat(valores_unicos[i], ":", frecuencia_acum[i], "\n")
cat("\nFrecuencia relativa:\n")
for (i in 1:n_valores) cat(valores_unicos[i], ":", frecuencia_rel[i], "\n")
cat("\nFrecuencia relativa acumulada:\n")
for (i in 1:n_valores) cat(valores_unicos[i], ":", frecuencia_rel_acum[i], "\n")

# Medidas descriptivas
media <- media_manual(s$distancia)
varianza_poblacional <- varianza_poblacional_manual(s$distancia)
varianza_muestral <- varianza_muestral_manual(s$distancia)
desv_est_poblacional <- desv_estandar(varianza_poblacional)
desv_est_muestral <- desv_estandar(varianza_muestral)

cat("\nMedia:", media, "\n")
cat("Varianza poblacional:", varianza_poblacional, "\n")
cat("Desviación estándar poblacional:", desv_est_poblacional, "\n")
cat("Varianza muestral:", varianza_muestral, "\n")
cat("Desviación estándar muestral:", desv_est_muestral, "\n")
@

\subsubsection*{AI-assisted development}

The following prompts were used during the development of this exercise to refine the logic of manual implementations, the documentation of each function, and the integration within the \texttt{Sweave} environment.  
All resulting code was subsequently reviewed, tested, and adapted.

\begin{itemize}
  \item \textbf{Prompt 1:}  
  \textit{"How can I implement a manual version of the \texttt{length()} function in R using a for loop?"}  
  Guided the creation of \texttt{contar\_filas()}, \texttt{contar\_columnas()}, and \texttt{contar\_elementos()}, each based on iterative counting.

  \item \textbf{Prompt 2:}  
  \textit{"Show me how to sort numeric data manually in R using nested loops (bubble sort) and return only the index order."}  
  Used to design \texttt{ordenar\_indices()}, which reproduces a bubble sort algorithm returning index positions.

  \item \textbf{Prompt 3:}  
  \textit{"Write an R function that calculates the range of a numeric vector without using \texttt{max()} or \texttt{min()}."}  
  Inspired the implementation of \texttt{rango\_manual()} through iterative comparison of values.

  \item \textbf{Prompt 4:}  
  \textit{"How can I manually find unique values in a numeric vector in R, without using \texttt{unique()}?"}  
  Helped define \texttt{valores\_unicos\_manual()}, combining sequential search and conditional appending.

  \item \textbf{Prompt 5:}  
  \textit{"Explain how to calculate absolute, cumulative, and relative frequencies manually in R using loops."}  
  Used to design \texttt{frecuencia\_absoluta\_manual()}, \texttt{acumulada\_manual()}, and \texttt{relativa\_manual()}, which reproduce frequency analysis logic step by step.

  \item \textbf{Prompt 6:}  
  \textit{"How can I organize multiple R functions with explanations inside a Sweave (.Rnw) document so that each function is displayed with comments but executed only once?"}  
  Clarified the correct use of chunk options (\texttt{eval=TRUE}, \texttt{echo=TRUE}, \texttt{results=hide}) for reproducible reports.

  \item \textbf{Prompt 7:}  
  \textit{"How can I integrate all the defined functions into a final executable section that prints descriptive statistics and frequency tables?"}  
  Used to design the final execution block combining all steps of the descriptive analysis.

  \item \textbf{Prompt 8:}  
  \textit{"How do I fix a Sweave error saying ‘results=markup not recognized’ when executing a chunk?"}  
  Guided the correction by replacing \texttt{results=markup} with \texttt{results=verbatim} for Sweave compatibility.
\end{itemize}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ejercicio 2.2: Fases del Algoritmo Apriori}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

El algoritmo se ha dividido en 7 fases lógicas, que se detallan a continuación.

\subsubsection{Fase 1: Carga de Datos y Sucesos Elementales Candidatos}

\subsubsection*{Explicación de la Fase}
[En esta primera fase el objetivo es ver que sucesos elementales de la muestra superan o igualan el umbral de soporte. 
Para ello se tiene que calcular el $n^o$ de veces que aparece dicho suceso elemental en la muestra, dividido por el núemro de 
elemento de la muestra. Los sucesos elementales que superen o igualen el umbral, son los que en la siguiente 
fase se utilizarán para crear los sucesos candidatos de las diferentes dimensiones. Antes de llevar a cabo estos pasos, se debe
cargar la muestra.]

\subsubsection*{Prompt de IA Utilizado}
\begin{quote}
[From the image i have just sent you i need to use a support threshold of 80% in order 
to decide which of the elements ("Faros de Xenon", "Alarma" and so on) go through and 
are going to be consider in the next step to start creating the candidates subsets. 
So in the image i have just sent you it is clear that i an identify the number of 
times each element appears, so how can i go through them all dividing the number 
of times they appear and dividing it by the amount of "sucesos"
that there are (there are 8) so that i get the percentages of the times they appear
for a subset of 8 elements and can discriminate which elements surpass the 0.5 threshold]
\end{quote}

\subsubsection*{Código y Ejecución (Sweave)}
<<Fase1_codigo, echo=TRUE, results=verbatim>>=
# 1ª FASE ----------------------------------------------------
library(Matrix)
library(arules)

muestra<-Matrix(c(1,1,1,1,0,0, 1,1,0,1,1,0, 1,1,1,0,0,0, 1,0,1,1,1,0, 1,1,0,1,0,0, 0,0,1,0,0,0, 1,1,0,1,0,0, 0,0,0,0,1,1), 8, 6, byrow=TRUE, dimnames = list(c("suceso1", "suceso2", "suceso3", "suceso4", "suceso5", "suceso6", "suceso7", "suceso8"), c("Faros de Xenon", "Control de Velocidad", "Navegador", "Bluetooth", "Techo Solar", "Alarma")),sparse=TRUE)

muestrangCMatrix<-as(muestra, "nsparseMatrix")
trapmuestrangCMatrix<-t(muestrangCMatrix)
transacciones<-as(trapmuestrangCMatrix, "transactions")

summary(transacciones)

frequency <- itemFrequency(transacciones, type ="absolute")
frequency

support_threshold <- 0.5
numero_sucesos <-length(transacciones)
support_sucesos <-frequency/numero_sucesos
support_sucesos

elementos_S_validos <- support_sucesos[support_sucesos >= support_threshold]
elementos_S_validos
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Fase 2: Generación de Sucesos Candidatos (L2, L3, L4)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Explicación de la Fase}
[En esta segunda fase, se van a construir los sucesos candidatos para cada una de las dimensiones necesarias en función del número
de sucesos candidatos elementales que superan el umbral de soporte en la primera fase. En este caso va a ser necesario
crear sucesos candidatos hasta la cuarta dimensión. Para crear estos sucesos candidatos de dimensión K, se deben utilizar sucesos
candidatos de la dimensión K-1 donde los últimos elementos de ambos sean diferentes y el resto de elemetos de ambos sean iguales
de forma que no haya conflicto a la hora de combinar ambos sucesos].

\subsubsection*{Prompt de IA Utilizado}
\begin{quote}
[Okey great lets continue on to the next step. In this next step i need you get
the candidate subsets that are going to form up to 4 dimmensions 
(because 4 elements passed the threshold). 

In case you do not know how this candidates subsets are formed, 
you need to start with the lowest dimmension possible other that K=1 and so 
for this dimmension and the follwing you need to have subsets in which the last 
element of the subset are different and the previous ones are exactly the same 
in order to combine them.

For example, for the dimmension k=2 we need to combine the subsets 
that passed the threshold in a way that the new subsets that are formed with 
to subsets of the k=1 dimmension have the last element different and the rest are the same.
In this dimmension because there are combinations of subsets of 1 elements you basically
need to have all possible combinations of the k=1 subsets .

But for the k=3 and so on you need to combine sets of k=2 o k=k-1 respectively
following the rules I told you so for example if we have {X,C} and {X,N} which
are from the k=2 dimmension then we can combine them because it follows the 
rules i mentioned obtaining anew subset of {X,C,N} for the dimmension k=3.

And for yout information the elements that passed the threshold were Faros de Xenon, 
Control de Velocidad, Navegador, Bluetooth.]
\end{quote}

\subsubsection*{Código y Ejecución (Sweave)}
<<Fase2_codigo, echo=TRUE, results=verbatim>>=
# 2ª FASE ----------------------------------------------------
L1 <- c("Faros de Xenon", "Control de Velocidad", "Navegador", "Bluetooth")
L1_lista <- lapply(L1, function(x) c(x))

generate_candidates <- function(L_prev) {
   n <- length(L_prev)
   if (n <= 1) return(list())     # nothing to join
   
   k <- length(L_prev[[1]]) + 1   # target candidate size
   
   # make sure items inside each subset are sorted
   L_prev <- lapply(L_prev, sort)
   candidates <- list()
   
   for (i in seq_len(n - 1)) {         # iterate all but last
     for (j in seq(i + 1, n)) {         # compare with following sets
       a <- L_prev[[i]]
       b <- L_prev[[j]]
       
       # join rule: all first (k-2) items identical
       if (k - 2 == 0 || all(a[1:(k - 2)] == b[1:(k - 2)])) {
         new_set <- sort(unique(c(a, b)))
         if (length(new_set) == k) {
           candidates <- append(candidates, list(new_set))
         }
       }
     }
   }
   
   # remove duplicates (if any)
   if (length(candidates) > 0) {
     candidates <- unique(lapply(candidates, function(x) paste(sort(x), collapse = ",")))
     candidates <- lapply(candidates, function(x) strsplit(x, ",")[[1]])
   }
   return(candidates)
}

L2 <- generate_candidates(L1_lista)
print("Candidatos L2:")
print(L2)

L3 <- generate_candidates(L2)
print("Candidatos L3:")
print(L3)

L4 <- generate_candidates(L3)
print("Candidatos L4:")
print(L4)
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Fase 3: Hash-Tree de sucesos candidatos para cada dimensión}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Explicación de la Fase}
[En esta fase se van a crear los hash-trees de los sucesos candidatos. Para llevar a cabo este paso se debe llevar a cabo una 
transformación numérica de los sucesos candidatos. Una vez se tenga hecho, se podrá crear un hash tree por 
cada dimensión (L2, L3, L4)]

\subsubsection*{Prompt de IA Utilizado}
\begin{quote}
[Okey great, thanks, lets move on to the next step, now we need to start working 
with tree structures, first you are going to focus on the candidates subsets, 
which first need to be transformed the words into numbers, using this transformation: 
\{ Faros de Xenon =1, Control de Velocidad = 2, Navegador =3, Bluetooth =4, Techo Solar =5,
Alarma =6). Once you transform the candidates to the numbers, this numbers must be sorted, 
for example if you have a subset that ends up being 243, it has to trasform to 234.]
\end{quote}

\subsubsection*{Código y Ejecución (Sweave)}
<<Fase3_codigo, echo=TRUE, results=verbatim>>=
# 3º FASE ----------------------------------------------------
item_map <- c(
   "Faros de Xenon" = 1,
   "Control de Velocidad" = 2,
   "Navegador" = 3,
   "Bluetooth" = 4,
   "Techo Solar" = 5,
   "Alarma" = 6
)

encode_candidates <- function(candidates, mapping) {
   lapply(candidates, function(subset) {
     nums <- unname(mapping[subset])     # map words → numbers
     sort(nums)
   })
}

L2_numerico <- encode_candidates(L2, item_map)
L3_numerico <- encode_candidates(L3, item_map)
L4_numerico <- encode_candidates(L4, item_map)

print("Candidatos L2 Numéricos:")
print(L2_numerico)
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Fase 4: Construcción de Hash Trees para cada suceso de la muestra para cada dimensión}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Explicación de la Fase}
[En esta fase se van a construir los hash trees de cada uno de los sucesos de la muestra para cada una de las dimensiones previas
(L2, L3, L4). Para llevar a cabo esta tarea se deben llevar a cabo los siguientes pasos. $1^o$ Se deben transformar los sucesos de la
muestra a los valores númericos de la misma forma que se hizo en la fase previa. $2^o$ Una vez se tengan los valores númericos se deben
crear todas las combinaciones de cada suceso de la muestra para cada una de las dimensiones (L2, L3, L4). $3^o$ Se deben crear
los hash-trees para cada dimensión a partir de todas las combinaciones posibles creadas.]

\subsubsection*{Prompt de IA Utilizado}
\begin{quote}
[okey, so the next step is to transform all the elementos of the sample to the numbers
following this mapping function, so that as we have done for the candidate
 subsets we can transform the sample into trees of k=2, 3 and 4 dimmensions .

The way of getting the subsets of those dimmensions if basically combining all possible
 ways the values of the sample subsets so that you get subsets of size =2, 3 or 4. 
Another critical thing is that you need to have the numbers sorted ascendingly so 
if for example one subset is {2,3,1} it has to be {1,2,3} before it gets transformed 
to a tree. So with that being said, this is the mapping:
\begin{verbatim}
item_map <- c( "Faros de Xenon" = 1, "Control de Velocidad" = 2, "Navegador" = 3,
"Bluetooth" = 4, "Techo Solar" = 5, "Alarma" = 6 )
\end{verbatim}
And this is sample that needs to be transformed first into numbers and then into subsets
of size = 2, 3 and 4 so that the trees of this samples are created:
\begin{verbatim}
muestra <- Matrix(c(1,1,1,1,0,0, 1,1,0,1,1,0, 1,1,1,0,0,0, 1,0,1,1,1,0, 1,1,0,1,0,0, 
0,0,1,0,0,0, 1,1,0,1,0,0, 0,0,0,0,1,1), nrow = 8, ncol = 6, byrow=TRUE, 
dimnames = list(c("suceso1", "suceso2", "suceso3", "suceso4", "suceso5", "suceso6", 
"suceso7", "suceso8"), c("Faros de Xenon", "Control de Velocidad", "Navegador", 
"Bluetooth", "Techo Solar", "Alarma")),sparse=TRUE).
\end{verbatim}
The trees need to be done for every level for every subset of the sample.]
\end{quote}

\subsubsection*{Código y Ejecución (Sweave)}
<<Fase4_codigo, echo=TRUE, results=verbatim>>=
# 4º FASE ----------------------------------------------------
all_candidates <- c(L2_numerico, L3_numerico, L4_numerico)

L2_candidatos <- Filter(function(x) length(x) == 2, all_candidates)
L3_candidatos <- Filter(function(x) length(x) == 3, all_candidates)
L4_candidatos <- Filter(function(x) length(x) == 4, all_candidates)



make_node <- function(item = NULL, is_end = FALSE) {
    # Create a new environment, not a list
    node <- new.env(parent = emptyenv()) 
    node$item <- item
    node$children <- list() # We can still use a list to hold the child envs
    node$is_end <- is_end
    node # Return the environment
}

insert_candidate <- function(root, candidate) {
    current <- root
    for (it in candidate) {
      key <- as.character(it)
      # create child if it doesn't exist
      if (!(key %in% names(current$children))) {
        current$children[[key]] <- make_node(item = it, is_end = FALSE)
      }
      # move to the child node
      current <- current$children[[key]]
    }
    # mark end of candidate
    current$is_end <- TRUE
    invisible(NULL)
}
 
 
build_tree <- function(candidates) {
    root <- make_node(item = NULL, is_end = FALSE)
    for (cand in candidates) {
      insert_candidate(root, cand)
    }
    root
}
 
 
 
 
print_tree <- function(node, depth = 0) {
    indent <- paste(rep("  ", depth), collapse = "")
    if (is.null(node$item)) {
      cat("(root)\n")
    } else {
      cat(indent, "- ", node$item, if (node$is_end) " [end]\n" else "\n", sep = "")
    }
    # iterate children in numeric order of keys
    if (length(node$children) > 0) {
      keys <- as.integer(names(node$children))
      keys <- sort(keys, na.last = TRUE)
      for (k in as.character(keys)) {
        print_tree(node$children[[k]], depth + 1)
      }
    }
}
 
 
arbol_L2_candidatos <- build_tree(L2_candidatos)
arbol_L3_candidatos <- build_tree(L3_candidatos)
arbol_L4_candidatos <- build_tree(L4_candidatos) 

print_tree(arbol_L2_candidatos)


# for the sample trees

transactions_num <- apply(muestra, 1, function(row) {
    items <- names(row[row == 1])
    nums <- unname(item_map[items])
    sort(nums)
})


# Function to generate subsets of a given size k
generate_subsets <- function(items, k) {
    if (length(items) < k) return(list())
    combn(items, k, simplify = FALSE)
}


transaction_subsets <- lapply(transactions_num, function(items) {
    list(
      C2 = generate_subsets(items, 2),
      C3 = generate_subsets(items, 3),
      C4 = generate_subsets(items, 4)
    )
})
 
all_trees <- lapply(transaction_subsets, function(subs) {
   list(
      tree_C2 = if (length(subs$C2) > 0) build_tree(subs$C2) else NULL,
      tree_C3 = if (length(subs$C3) > 0) build_tree(subs$C3) else NULL,
      tree_C4 = if (length(subs$C4) > 0) build_tree(subs$C4) else NULL
    )
  }) 
print_tree(all_trees$suceso1$tree_C2)
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Fase 5: Conteo sucesos candidatos para analizar la asociación de los que superen el umbral de soporte}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Explicación de la Fase}
[En esta fase se cuenta el número de veces que aparecen los sucesos candidatos en los árboles de los sucesos de la muestra. Este paso es importante ya 
que solo aquellos sucesos que superen el umbral de soporte serán considerados en la fase de análisis de asociación. De forma que se contabiliza si el 
$n^o$ de veces que aparecen los sucesos candidatos/$n^o$ de sucesos de la muestra es superior o igual al umbral de soporte]

\subsubsection*{Prompt de IA Utilizado}
\begin{quote}
[okey great, we can now move on to the next phase which consists on counting 
how many times do the leafs of the candidates tree in each level appear 
in each of the sample trees as this values need to be counted in order 
to see which of the candidates subsets move on to the next phase.

okey, great no that i have the supports for every tree subset, 
we need to filter by threshold that i have already defined previously 
support_threshold <- 0.5. The ones that pass the cut will pass on to 
the next phase.]
\end{quote}

\subsubsection*{Código y Ejecución (Sweave)}
<<Fase5_codigo, echo=TRUE, results=verbatim>>=
# 5º FASE ----------------------------------------------------
# (Incluimos las funciones de la Fase 4 que se necesitan aquí)


get_leaf_paths <- function(node, current_path = integer()) {
    paths <- list()
    if (!is.null(node$item)) {
      current_path <- c(current_path, node$item)
    }
    if (node$is_end) {
      paths <- append(paths, list(current_path))
    }
    if (length(node$children) > 0) {
      for (child in node$children) {
        paths <- append(paths, get_leaf_paths(child, current_path))
      }
    }
    paths
}

path_in_tree <- function(tree, path) {
    current <- tree
    for (item in path) {
      key <- as.character(item)
      if (!(key %in% names(current$children))) {
        return(FALSE)
      }
      current <- current$children[[key]]
    }
   return(TRUE)
}

count_supports <- function(candidate_tree, sample_trees_by_level) {
    candidate_paths <- get_leaf_paths(candidate_tree)
    supports <- numeric(length(candidate_paths))
    
    for (i in seq_along(candidate_paths)) {
      path <- candidate_paths[[i]]
      supports[i] <- sum(sapply(sample_trees_by_level, function(trees) {
        tree_level <- NULL
        k <- length(path)
        if (k == 2) tree_level <- trees$tree_C2
        else if (k == 3) tree_level <- trees$tree_C3
        else if (k == 4) tree_level <- trees$tree_C4
        if (is.null(tree_level)) return(FALSE)
        path_in_tree(tree_level, path)
      }))
    }
    data.frame(
      itemset = sapply(candidate_paths, function(p) paste(p, collapse = ",")),
      support = supports
    )
}

soporte_arbol_L2 <- count_supports(arbol_L2_candidatos, all_trees)
soporte_arbol_L3 <- count_supports(arbol_L3_candidatos, all_trees)
soporte_arbol_L4 <- count_supports(arbol_L4_candidatos, all_trees)

print("Soporte L2 (Antes de filtrar):")
print(soporte_arbol_L2)
print("Soporte L3 (Antes de filtrar):")
print(soporte_arbol_L3)
print("Soporte L4 (Antes de filtrar):")
print(soporte_arbol_L4)

filter_by_threshold <- function(support_df, threshold, total_tx) {
    support_df$support_ratio <- support_df$support / total_tx
    subset(support_df, support_ratio >= threshold)
}

L2_candidatos_post_arbol <- filter_by_threshold(soporte_arbol_L2, support_threshold, numero_sucesos)
L3_candidatos_post_arbol <- filter_by_threshold(soporte_arbol_L3, support_threshold, numero_sucesos)
L4_candidatos_post_arbol <- filter_by_threshold(soporte_arbol_L4, support_threshold, numero_sucesos)

print("Itemsets Frecuentes L2 (Post-Filtro):")
print(L2_candidatos_post_arbol)
print("Itemsets Frecuentes L3 (Post-Filtro):")
print(L3_candidatos_post_arbol)
print("Itemsets Frecuentes L4 (Post-Filtro):")
print(L4_candidatos_post_arbol)
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Fase 6: Generación de Reglas de Asociación}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Explicación de la Fase}
[En esta fase a partir de los sucesos candidatos que superaron el umbral de soporte se van a crear las diversas reglas de asociación
para cada uno de ellos. Para saber cuantas reglas de asociación se deben crear por cada suceso de cada dimensión, se debe considerar el cálculo de
$2^K-2$. De forma que si por ejemplo $K=3$, habrá 6 reglás de asociación que se deben crear y analizar. Para la creación de las reglas de asociación
se debe entender que se forman con la siguiente estructura $A \to B-A$, donde $B$ podría ser $\{Co, Li\}$ y $A$ sería $\{Co\}$ o $\{Li\}$.]

\subsubsection*{Prompt de IA Utilizado}
\begin{quote}
[Okay, we can move onto the next phase, no we have to create associations 
from every level (L2, L3, L4) . The amount of associations that can be 
created for each subset of each level follows the following formula: $2^k-2$. 
So for example for every subset of $k=2$, there are two possible associations. 

Before you start creating every association for each subset you have to reverse 
the numeric values for the words that we previously changed.]
\end{quote}

\subsubsection*{Código y Ejecución (Sweave)}
<<Fase6_codigo, echo=TRUE, results=verbatim>>=
# 6ª FASE ----------------------------------------------------
item_map_rev <- setNames(names(item_map), item_map)

generate_associations <- function(itemset) {
    k <- length(itemset)
    associations <- list()
    if (k < 2) return(associations)
 
   all_subsets <- unlist(lapply(1:(k - 1), function(i) combn(itemset, i, simplify = FALSE)), recursive = FALSE)
 
   for (lhs in all_subsets) {
     rhs <- setdiff(itemset, lhs)
     associations <- append(associations, list(list(lhs = lhs, rhs = rhs)))
   }
   associations
}

# Helper: convert "1,2,3" -> c(1,2,3)
split_itemset <- function(s) as.integer(unlist(strsplit(s, ",")))

create_associations_from_L <- function(L_df) {
    if (nrow(L_df) == 0) return(data.frame())
    
    all_associations <- list()
    for (i in seq_len(nrow(L_df))) {
      itemset <- split_itemset(L_df$itemset[i])
      associations <- generate_associations(itemset)
      all_associations <- append(all_associations, lapply(associations, function(r) {
        data.frame(
          lhs = paste(item_map_rev[as.character(sort(r$lhs))], collapse = ", "),
          rhs = paste(item_map_rev[as.character(sort(r$rhs))], collapse = ", "),
          k = length(itemset),
          support = L_df$support[i],
          support_ratio = L_df$support_ratio[i],
          stringsAsFactors = FALSE
        )
      }))
    }
    
   do.call(rbind, all_associations)
}

associaciones_L2 <- create_associations_from_L(L2_candidatos_post_arbol)
associaciones_L3 <- create_associations_from_L(L3_candidatos_post_arbol)
associaciones_L4 <- create_associations_from_L(L4_candidatos_post_arbol)

print("Reglas generadas desde L2:")
print(associaciones_L2)
print("Reglas generadas desde L3:")
print(associaciones_L3)
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Fase 7: Cálculo de Confianza y Filtrado Final}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Explicación de la Fase}
[En esta fase final se va a realizar el cálculo de confianza de cada una de las reglas de asociación para así poder sacar conclusiones de la muestra con
cierto grado de confianza. Para llevar a cabo este cálculo se deberá dividir el $n^o$ de veces que aparece B en la muestra/$n^o$ de veces que aparece A en la muestra.
Si por ejemplo se tiene que $B=\{Co,Li\}$ y se tiene la siguiente regla de asociación $Co \to Li$ en este caso $A=Co$. Y esos son los conjuntos que se deben
contabilizar de la muestra. Aquellas reglas de asociación que superen dicho umbral de confianza, se podrá afirmar con el umbral de confianza que si sucede
una, la otra también sucede con ese grado de confianza.]

\subsubsection*{Prompt de IA Utilizado}
\begin{quote}
[so now we enter the last phase, we need to evaluate the confidence threshold 
for every association we created. In my case the confidence threshold will be of 
0.8 can you help me analyze this last phase for all the associations in every 
level please.

And also, do not incorporate prunning to the code.]
\end{quote}

\subsubsection*{Código y Ejecución (Sweave)}
<<Fase7_codigo, echo=TRUE, results=verbatim>>=
# 7ª FASE ----------------------------------------------------
confidence_threshold <- 0.8

convert_numeric_str_to_text_str <- function(num_str, map_rev) {
    nums_numeric <- split_itemset(num_str) 
    nums_char <- as.character(nums_numeric)
    names <- unname(map_rev[nums_char])
    paste(names, collapse = ", ") 
}

soporte_L1_map_df <- data.frame(
    lhs_str = names(frequency),
    lhs_support = as.integer(frequency),
    stringsAsFactors = FALSE
)

soporte_L2_map_df <- data.frame(
    lhs_str = sapply(soporte_arbol_L2$itemset, convert_numeric_str_to_text_str, map_rev = item_map_rev),
    lhs_support = soporte_arbol_L2$support,
    stringsAsFactors = FALSE
)

soporte_L3_map_df <- data.frame(
    lhs_str = sapply(soporte_arbol_L3$itemset, convert_numeric_str_to_text_str, map_rev = item_map_rev),
    lhs_support = soporte_arbol_L3$support,
    stringsAsFactors = FALSE
)

master_support_lookup <- rbind(soporte_L1_map_df, soporte_L2_map_df, soporte_L3_map_df)
master_support_lookup <- na.omit(master_support_lookup)

calculate_and_filter_confidence <- function(associations_df, support_lookup, threshold) {
    if (nrow(associations_df) == 0) {
      return(data.frame(
        lhs = character(),
        rhs = character(),
        k = integer(),
        support_ratio = numeric(),
        confidence = numeric(),
        stringsAsFactors = FALSE
      ))
    }
    
    merged_df <- merge(associations_df, support_lookup, by.x = "lhs", by.y = "lhs_str", all.x = TRUE)
    
    merged_df$lhs_support[is.na(merged_df$lhs_support)] <- 0
    
    merged_df$confidence <- merged_df$support / merged_df$support
    
    merged_df$confidence[is.nan(merged_df$confidence)] <- 0
    
    strong_rules <- merged_df[, c("lhs", "rhs", "k", "support_ratio", "confidence")]
    
    # Modificado: No filtrar por threshold aquí, solo ordenar
    strong_rules[order(-strong_rules$confidence), ]
}

reglas_fuertes_L2 <- calculate_and_filter_confidence(associaciones_L2, master_support_lookup, confidence_threshold)
reglas_fuertes_L3 <- calculate_and_filter_confidence(associaciones_L3, master_support_lookup, confidence_threshold)
reglas_fuertes_L4 <- calculate_and_filter_confidence(associaciones_L4, master_support_lookup, confidence_threshold)

# Filtrado final
reglas_finales_L2 <- subset(reglas_fuertes_L2, confidence >= confidence_threshold)
reglas_finales_L3 <- subset(reglas_fuertes_L3, confidence >= confidence_threshold)
reglas_finales_L4 <- subset(reglas_fuertes_L4, confidence >= confidence_threshold)

print("--- Reglas que superan el umbral de confianza (0.8) ---")
print("Reglas Finales L2:")
print(reglas_finales_L2)
print("Reglas Finales L3:")
print(reglas_finales_L3)
print("Reglas Finales L4:")
print(reglas_finales_L4)
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FIN DE LAS FASES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ejercicio 2.3: Detección de Datos Anómalos}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

En este ejercicio se implementaron tres métodos distintos para la detección de datos anómalos, utilizando técnicas estadísticas básicas.  
Se trabajó con dos variables: \texttt{Velocidad} y \texttt{Temperatura}.  

<<setup, echo=TRUE, message=FALSE>>=
# Creación del conjunto de datos
datos <- data.frame(
  Velocidad = c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5),
  Temperatura = c(7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73)
)

# Funciones auxiliares del ejercicio 2.1
contar_elementos <- function(x) {
  n <- 0
  for (._ in x) n <- n + 1
  n
}

media_manual <- function(x) {
  suma <- 0
  n <- contar_elementos(x)
  for (v in x) suma <- suma + v
  suma / n
}
@

\subsubsection{Medidas de ordenación (Velocidad), Método Caja y Bigotes}

En esta parte del trabajo se buscó identificar posibles valores atípicos en la variable \texttt{Velocidad}.  
Se calcularon los cinco números resumen (mínimo, Q1, mediana, Q3 y máximo) mediante la función \texttt{fivenum()}.  
Posteriormente, se aplicó la regla del rango intercuartílico (IQR) para detectar outliers, considerando un dato como atípico si cumple:
\[
x < Q_1 - 1.5 \times IQR \quad \text{o} \quad x > Q_3 + 1.5 \times IQR
\]. Finalmente, se iteró manualmente sobre todos los valores de Velocidad, comparando cada uno con estos límites para identificar y reportar aquellos que quedaban fuera del intervalo.

\subsubsection*{Prompt de IA Utilizado}

\begin{quote}
[I have the following dataset of Speed values: {10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5}. I want you to apply the box-and-whisker outlier detection method. To do this, follow these steps: 
sort the data, calculate the first and third quartiles, compute the interquartile range (IQR) using the formula provided earlier, determine the detection limits, and finally indicate which
 of the original Speed values fall outside this interval. You are not allowed to use: matrix, boxplot, quantile, length, mean, or sd.]
\end{quote}



<<ordenacion_velocidad, echo=TRUE, results=verbatim>>=
# Resumen de cinco números para Velocidad
resumen_vel <- fivenum(datos$Velocidad)
names(resumen_vel) <- c("Mínimo", "Q1", "Mediana", "Q3", "Máximo")
print(resumen_vel)

# Cálculo del IQR y límites
IQRv <- resumen_vel["Q3"] - resumen_vel["Q1"]
lim_inf_v <- resumen_vel["Q1"] - 1.5 * IQRv
lim_sup_v <- resumen_vel["Q3"] + 1.5 * IQRv

cat("\n--- OUTLIERS en VELOCIDAD ---\n")
i <- 1
for (v in datos$Velocidad) {
  if (v < lim_inf_v || v > lim_sup_v) {
    cat("Índice:", i, "→ Velocidad =", v, "es un outlier\n")
  }
  i <- i + 1
}
if (all(datos$Velocidad >= lim_inf_v & datos$Velocidad <= lim_sup_v)) {
  cat("No se detectaron outliers en velocidad.\n")
}
@



\subsubsection{Medidas de dispersión (Temperatura), Método Desviación Típica}

Se aplicó el método de la desviación típica para detectar valores atípicos en la variable \texttt{Temperatura}.  
Asumiendo distribución normal, se calculó la media y varianza de forma manual, y se definieron los límites inferior y superior como:
\[
L_i = \bar{x} - 2\sigma \quad ; \quad L_s = \bar{x} + 2\sigma
\]
Los valores fuera de este intervalo se consideraron atípicos.

\subsubsection*{Prompt de IA Utilizado}
\begin{quote}
[Now you have to do the same as before but with different data and another method. Analyze the following Temperature dataset: 
{7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73}. I want you to identify outliers using the standard deviation method. 
It is important to follow this procedure: calculate the arithmetic mean of all values; compute the standard deviation; then apply the 
outlier detection rule using the formulas: lower limit = mean(x) − d × standard deviation, and upper limit = mean(x) + d × standard deviation. 
The code must check the data and indicate which values are outliers because they do not meet this condition. You are not allowed to use: lm, summary, 
or length.]
\end{quote}

<<dispersión_temperatura, echo=TRUE, results=verbatim>>=
n <- contar_elementos(datos$Temperatura)
media_temp <- media_manual(datos$Temperatura)

# Varianza y desviación típica manuales
var_temp <- 0
for (t in datos$Temperatura) {
  var_temp <- var_temp + (t - media_temp)^2
}
var_temp <- var_temp / n
desv_temp <- sqrt(var_temp)

# Límites de outliers
lim_inf_t <- media_temp - 2 * desv_temp
lim_sup_t <- media_temp + 2 * desv_temp

cat("\n--- OUTLIERS en TEMPERATURA ---\n")
i <- 1
for (t in datos$Temperatura) {
  if (t < lim_inf_t || t > lim_sup_t) {
    cat("Índice:", i, "→ Temperatura =", t, "es un outlier\n")
  }
  i <- i + 1
}
@

\subsubsection{Detección por regresión lineal}

Finalmente, se utilizó un enfoque basado en regresión lineal entre \texttt{Velocidad} (variable independiente) y \texttt{Temperatura} (variable dependiente).  
Se calcularon las medias de Velocidad y Temperatura con las funciones del ejercicio 2.1. Después, se calcularon los coeficientes de la recta de regresión. Los cálculos se hicieron manualmente iterando sobre los datos, sin usar funciones R predefinidas como lm(). 
Una vez obtenida la recta, se calcularon los residuos. Para cada, se calculó la temperatura estimada y el residuo. Luego, se calculó la desviación típica de los residuos. Y se estableció el criterio de anomalía: un punto se considera un outlier si su residuo absoluto es mayor que dos veces el error estándar de los residuos. Por último, se iteró sobre los residuos para identificar los puntos que cumplían esta condición.

\subsubsection*{Prompt de IA Utilizado}

\begin{quote}
[Final exercise. We take the 11 pairs of data {Speed, Temperature}: {10, 7.46; 8, 6.77; 13, 12.74; 9, 7.11; 11, 7.81; 14, 8.84; 6, 6.08; 4, 5.39; 12, 8.15; 7, 6.42; 5, 5.73}. Perform an outlier analysis based on regression residuals, where Speed is the independent variable and Temperature is the dependent variable.
Please follow these steps manually, without using functions such as lm or summary: first, calculate the mean of Speed and Temperature; then obtain the linear regression coefficients a and b (b = Sxy / Sx²). For each of the 11 points, compute its residual and the standard deviation of the residuals. Finally, the code must indicate which points are outliers according to the condition |yi − ŷi| > d × sr”]
\end{quote}



<<regresion_outliers, echo=TRUE, results=verbatim>>=
x <- datos$Velocidad
y <- datos$Temperatura
media_x <- media_manual(x)
media_y <- media_manual(y)

# Cálculo manual de la pendiente y el intercepto
num <- 0
den <- 0
for (i in seq(contar_elementos(x))) {
  num <- num + (x[i] - media_x) * (y[i] - media_y)
  den <- den + (x[i] - media_x)^2
}
b1 <- num / den
b0 <- media_y - b1 * media_x

cat("\nEcuación de regresión manual:\n")
cat("Temperatura = ", round(b0, 4), " + ", round(b1, 4), " * Velocidad\n")

# Residuos y error estándar
residuos <- numeric(n)
for (i in seq(n)) {
  y_est <- b0 + b1 * x[i]
  residuos[i] <- y[i] - y_est
}

suma_res <- 0
for (r in residuos) suma_res <- suma_res + r^2
error_est <- sqrt(suma_res / n)
cat("\nError estándar de los residuos:", round(error_est, 4), "\n")

cat("\n--- OUTLIERS en REGRESIÓN ---\n")
i <- 1
for (r in residuos) {
  if (abs(r) > 2 * error_est) {
    cat("Índice:", i, "→ Residuo =", round(r, 4), "es un outlier\n")
  }
  i <- i + 1
}
@

\newpage







\section{Conclusiones}
[...AQUÍ VA TU TEXTO: Escribe tus conclusiones finales sobre el proyecto. Por ejemplo: "La implementación manual del algoritmo Apriori en R, aunque compleja, permite un entendimiento profundo de sus mecanismos internos, como la generación de candidatos y el conteo de soporte. El uso de Sweave ha sido fundamental para crear este documento reproducible..."]

% --- Bibliografía ---
\newpage
% \printbibliography % Descomenta esto para mostrar tu bibliografía

\end{document}