%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PREÁMBULO DE LATEX (Igual que antes)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[a4paper]{article}

% --- Paquetes Esenciales ---
\usepackage[utf8]{inputenc} % Codificación de entrada
\usepackage[T1]{fontenc}    % Codificación de fuentes
\usepackage[spanish]{babel} % Idioma español
\usepackage{graphicx}       % Para insertar imágenes (tus capturas)
\usepackage[a4paper,margin=2.5cm]{geometry} % Márgenes
\usepackage{Sweave}         % Estilos para Sweave

% --- Paquetes Adicionales Útiles ---
\usepackage{amsmath}        % Mejoras para matemáticas
\usepackage{amsfonts}       % Fuentes matemáticas
\usepackage{hyperref}       % Links clicables
\usepackage{float}          % Controlar posición de flotantes (figuras, tablas)
\usepackage{csquotes}       % Para citas textuales
\usepackage[backend=biber,style=ieee]{biblatex} % Bibliografía

\setlength{\parskip}{\baselineskip} % Un espacio entre párrafos

% \addbibresource{bibliografia.bib} % Descomenta esto y crea tu archivo .bib

% --- Configuración del Documento ---
\title{Memoria: Implementación del Algoritmo Apriori en R}
\author{Oscar Morán, Asier Álamo, Edgar Alexis Conforme, Lucía Díaz}
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INICIO DEL DOCUMENTO
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

% --- Portada ---
\begin{titlepage}
    \centering
    \vspace*{\fill}
    {\LARGE Universidad de Alcalá de Henares \\[0.5cm]}
    {\Large Escuela Politécnica Superior \\[1cm]}
    {\Huge \textbf{Fundamentos de la Ciencia de Datos} \\[1.5cm]}
    \Large\textbf{Autor:} \\[0.3cm]
    {\Large Tu Nombre \\[2cm]}
    \textbf{Fecha:} \\[0.3cm]
    {\Large \today} \\[1.5cm]
    \vspace*{\fill}
\end{titlepage}

\newpage
\tableofcontents % Tabla de contenido automática
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECCIONES DE LA MEMORIA
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introducción}
El objetivo de esta memoria es documentar el diseño e implementación de diversos algoritmos y técnicas de la ciencia de datos, utilizando el lenguaje de programación R.

Sweave nos permite generar este informe de manera dinámica, mezclando el texto explicativo con el código R que implementa los algoritmos.

\section{Resolución de Ejercicios}
A continuación, se detallan los ejercicios resueltos.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ejercicio 1.1: Análisis Descriptivo (Satélites de Urano)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

En este primer ejercicio se realiza un análisis descriptivo básico utilizando las funciones estándar que proporciona R. El objetivo es familiarizarse con la carga de datos, la ordenación y el cálculo de medidas de centralización, posición y dispersión.

\subsubsection{Descripción del conjunto de datos}
Se utiliza un conjunto de datos con información sobre los satélites menores de Urano (aquellos con radio menor a 50 km). Los datos constan de dos variables:
\begin{itemize}
    \item \textbf{Nombre:} variable cualitativa nominal que identifica al satélite (ej. Cordelia, Ofelia).
    \item \textbf{radio:} variable cuantitativa continua que representa el radio del satélite en kilómetros.
\end{itemize}
Los datos se cargan desde el fichero externo \texttt{satelites.txt}.

\subsubsection{Funciones de R utilizadas}
Para cumplir con los objetivos del análisis, se han empleado las siguientes funciones base de R. Es fundamental comprender sus argumentos y valores de retorno:

\begin{itemize}
    \item \texttt{read.table(file, header = TRUE)}: Lee un archivo en formato tabular y crea un \textit{data frame}. El argumento \texttt{header = TRUE} indica que la primera línea del archivo contiene los nombres de las variables.
    \item \texttt{order(x)}: Devuelve un vector de índices permutados que ordenarían el vector \texttt{x} de forma ascendente. Es esencial para reordenar \textit{data frames} completos en función de una columna.
    \item \texttt{rev(x)}: Invierte el orden de los elementos de un vector. Combinado con \texttt{order}, permite realizar ordenaciones descendentes.
    \item \texttt{table(x)}: Genera una tabla de frecuencias absolutas, contando las ocurrencias de cada valor único en el vector \texttt{x}.
    \item \texttt{cumsum(x)}: Calcula la suma acumulada de los elementos de un vector. Se aplica sobre las frecuencias absolutas o relativas para obtener sus versiones acumuladas.
    \item \texttt{mean(x)} y \texttt{median(x)}: Calculan, respectivamente, la media aritmética y la mediana (el valor central de la distribución).
    \item \texttt{var(x)} y \texttt{sd(x)}: Calculan la varianza y la desviación típica. Es importante notar que R calcula por defecto las versiones \textbf{muestrales} (dividiendo por $n-1$) y no las poblacionales.
    \item \texttt{quantile(x, probs)}: Calcula los cuantiles de la distribución correspondientes a las probabilidades indicadas en el argumento \texttt{probs}. Por ejemplo, \texttt{probs = 0.25} devuelve el primer cuartil ($Q_1$).
    \item \texttt{max(x)} y \texttt{min(x)}: Devuelven los valores máximo y mínimo del vector, utilizados para calcular el rango.
\end{itemize}

\subsubsection{Implementación en R}

A continuación se muestra el código utilizado para realizar el análisis completo, desde la carga de datos hasta la generación de tablas de frecuencias y el cálculo de estadísticos.

<<ejercicio1_1_analisis, echo=TRUE, results=verbatim>>=
# 1. Carga de datos
# Se lee el fichero 'satelites.txt' asumiendo que está en el directorio de trabajo
s <- read.table("satelites.txt", header = TRUE)
print("Conjunto de datos original:")
print(s)

# 2. Ordenación de los datos
# Ordenación ascendente por la variable 'radio'
so_asc <- s[order(s$radio), ]
print("Datos ordenados ascendentemente por radio:")
print(so_asc)

# Ordenación descendente utilizando rev() sobre el orden
so_desc <- s[rev(order(s$radio)), ]
print("Datos ordenados descendentemente por radio:")
print(so_desc)

# 3. Medidas de Dispersión Básicas (Rango)
# El rango es la diferencia entre el valor máximo y el mínimo
rango_val <- max(s$radio) - min(s$radio)
cat("\nRango del radio (max - min):", rango_val, "km\n")

# 4. Tabla de Frecuencias del Radio
# Frecuencia Absoluta (ni)
frec_abs <- table(s$radio)
# Frecuencia Absoluta Acumulada (Ni)
frec_abs_acum <- cumsum(frec_abs)
# Frecuencia Relativa (fi): ni / N
frec_rel <- frec_abs / length(s$radio)
# Frecuencia Relativa Acumulada (Fi)
frec_rel_acum <- cumsum(frec_rel)

# Se combinan en un data frame para una mejor visualización
tabla_frec <- data.frame(
  Radio = names(frec_abs),
  ni = as.integer(frec_abs),
  Ni = as.integer(frec_abs_acum),
  fi = round(as.numeric(frec_rel), 3),
  Fi = round(as.numeric(frec_rel_acum), 3)
)
print("Tabla de Frecuencias (Radio):")
print(tabla_frec)

# 5. Medidas de Posición Central
media_r <- mean(s$radio)
mediana_r <- median(s$radio)

cat("\n--- Medidas de Tendencia Central ---\n")
cat("Media aritmética:", media_r, "km\n")
cat("Mediana:", mediana_r, "km\n")

# 6. Medidas de Dispersión y Posición No Central
# Varianza y Desviación Típica muestrales (n-1)
var_r <- var(s$radio)
sd_r <- sd(s$radio)

# Cuartiles: se solicitan el 25% (Q1), 50% (Q2/Mediana) y 75% (Q3)
cuartiles <- quantile(s$radio, probs = c(0.25, 0.50, 0.75))

cat("\n--- Medidas de Dispersión y Cuartiles ---\n")
cat("Varianza (muestral):", var_r, "\n")
cat("Desviación Típica (muestral):", sd_r, "km\n")
cat("Cuartiles (Q1, Q2, Q3):\n")
print(cuartiles)
@

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ejercicio 1.2: Análisis de asociación}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
El propósito de este ejercicio es identificar las relaciones y patrones de compra frecuentes entre distintos productos usando Apriori.

\subsubsection{Instalación y Carga de Paquetes}
El análisis de reglas de asociación en R se facilita mediante el paquete \texttt{arules}.  
El primer paso fue intentar cargar dicho paquete. Al no estar instalado, se procedió con su instalación desde el repositorio CRAN.  

<<>>=
# install.packages("arules")

library(arules)
@

\subsubsection{Creación y Formateo de los Datos}
Los datos de las cestas de la compra se representaron inicialmente como una matriz binaria, donde 1 indica la presencia de un artículo en una cesta y 0 su ausencia.  
Para poder utilizar el paquete arules, la matriz tuvo que ser convertida al formato \texttt{transactions}. Este proceso se realizó en varios pasos:

\begin{enumerate}
\item Convertir la matriz numérica a una matriz lógica ({ngCMatrix}).
\item Transponer la matriz para que los items queden en las filas.
\item Convertir la matriz transpuesta final al formato transactions.
\end{enumerate}

<<>>=
library(Matrix)

# Creación de la matriz binaria de cestas de compra
muestra <- Matrix(c(1,1,0,1,1,
                    1,1,1,1,0,
                    1,1,0,1,0,
                    1,0,1,1,0,
                    1,1,0,0,0,
                    0,0,0,1,0),
                  6, 5, byrow=TRUE,
                  dimnames = list(c("suceso1", "suceso2", "suceso3", 
                                    "suceso4", "suceso5", "suceso6"),
                                  c("Pan", "Agua", "Cafe", "Leche", "Naranja")),
                  sparse=TRUE)

# Conversión a matriz lógica y transposición
muestrangCMatrix <- as(muestra, "nsparseMatrix")
traspmuestrangCMatrix <- t(muestrangCMatrix)

# Conversión final al formato transactions
transacciones <- as(traspmuestrangCMatrix, "transactions")

# Visualizar las transacciones
transacciones
@

\subsubsection{Análisis del Conjunto de Datos}
Con los datos en el formato correcto, se utilizó la función summary() para obtener una visión general de las transacciones.  
El resumen indicó que ``Pan'' y ``Leche'' son los artículos más frecuentes (presentes en 5 de las 6 cestas), y que el tamaño de las cestas varía de 1 a 4 artículos.

<<>>=
summary(transacciones)
@

\subsubsection{Aplicación del Algoritmo Apriori}
Se aplicó el algoritmo apriori para extraer las reglas de asociación.  
Se establecieron los siguientes umbrales.
Tras la ejecución del algoritmo, se generaron 7 reglas de asociación.

\begin{itemize}
\item \textbf{Soporte mínimo (support)} = 0.5 → la combinación de artículos debe aparecer en al menos el 50\% de todas las transacciones (3 o más cestas).
\item \textbf{Confianza mínima (confidence)} = 0.8 → la regla A ⇒ B debe cumplirse al menos el 80\% de las veces que aparece A.
\end{itemize}

<<>>=
asociaciones <- apriori(transacciones,
                        parameter = list(support = 0.5, confidence = 0.8))

# Mostrar las reglas generadas
inspect(asociaciones)
@

\subsubsection{Conclusiones}
Del análisis de las reglas se extraen varias conclusiones importantes:

\begin{enumerate}
\item \textbf{Productos Estrella:} ``Pan'' y ``Leche'' son los productos más comunes, como indican las reglas 1 y 2 y su soporte de 0.833.
\item \textbf{Asociación Fuerte (Agua y Pan):} La regla \texttt{\{Agua\} ⇒ \{Pan\}} tiene una confianza del 100\%, lo que significa que cada vez que un cliente compró ``Agua'', también compró ``Pan''. El \texttt{lift} de 1.20 indica que esta asociación es un 20\% más frecuente de lo esperado por azar.
\item \textbf{Asociación Recíproca:} La regla \texttt{\{Pan\} ⇒ \{Agua\}} también es fuerte, con una confianza del 80\%.
\item \textbf{Regla de Tres Productos:} La regla \texttt{\{Agua, Leche\} ⇒ \{Pan\}} tiene una confianza del 100\%, lo que implica que los tres clientes que compraron ``Agua'' y ``Leche'' juntos también compraron ``Pan''.
\end{enumerate}

En resumen, el análisis revela una fuerte conexión de compra entre los productos ``Pan'' y ``Agua'', y también una relación significativa entre ``Pan'' y ``Leche''.
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ejercicio 1.3: Detección de Datos Anómalos}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

En este ejercicio se aplican tres métodos distintos para la detección de datos anómalos sobre un conjunto de datos de muestra.

\subsubsection*{Descripción del conjunto de datos}
El conjunto de datos utilizado es una pequeña muestra de 7 observaciones, cada una con dos variables cuantitativas:
\begin{itemize}
    \item \textbf{r}: variable cuantitativa (resistencia).
    \item \textbf{d}: variable cuantitativa (densidad).
\end{itemize}
Los datos se crean directamente en R y se almacenan en un \texttt{data.frame} llamado \texttt{muestra}. El objetivo es aplicar métodos univariantes (Caja y Bigotes sobre 'r', Desviación Típica sobre 'd') y un método bivariante (Regresión sobre 'd' en función de 'r') para identificar observaciones atípicas.

\subsubsection*{Implementación y Carga de Datos}
El primer paso es crear la estructura de datos. Los datos se introducen en una matriz y luego se transforman en un \texttt{data.frame} para un manejo más fácil.

<<DatosAnomalos_Setup, echo=TRUE, results=verbatim>>=
# La siguiente línea de código se ha dividido en varias
# para evitar que se salga del margen al imprimirse.
(muestra <- t(matrix(
  c(3, 2, 3.5, 12, 4.7, 4.1, 5.2, 4.9, 7.1, 6.1, 6.2, 5.2, 14, 5.3),
  2, 7, dimnames = list(c("r", "d"))
)))
(muestra <- data.frame(muestra))
@

\paragraph{Explicación de las funciones utilizadas:}
\begin{itemize}
    \item \texttt{matrix(data, nrow, ncol, dimnames)}: Crea una estructura de matriz. En este caso, se le pasa un vector de 14 números. Con \texttt{nrow=2} y \texttt{ncol=7}, R rellena la matriz por columnas por defecto: la primera columna es \texttt{c(3, 2)}, la segunda \texttt{c(3.5, 12)}, y así sucesivamente.
    \item \texttt{t(x)}: Función que transpone su argumento (matriz o data frame), intercambiando filas por columnas. La matriz 2x7 se convierte en una matriz 7x2, donde cada fila representa una observación y cada columna una variable ('r' o 'd').
    \item \texttt{data.frame(x)}: Convierte su argumento (en este caso, la matriz 7x2) en un \texttt{data.frame}. Esta es la estructura de datos estándar en R para análisis estadístico, permitiendo acceder a las columnas por su nombre (ej. \texttt{muestra\$r}).
\end{itemize}

\subsubsection{Método 1: Caja y Bigotes}

\subsubsection*{Fundamento Teórico}
El método de la caja y bigotes (Boxplot) es una técnica de detección de atípicos no paramétrica (no asume una distribución específica de los datos). Se basa en el Rango Intercuartílico (IQR), que es la diferencia entre el tercer cuartil ($Q_3$, percentil 75) y el primer cuartil ($Q_1$, percentil 25).

Este método define un intervalo de "normalidad" y considera atípico (outlier) cualquier punto que caiga fuera de él. Los límites de este intervalo se calculan como:
\[
\text{Límite Inferior} = Q_1 - d \times IQR
\]
\[
\text{Límite Superior} = Q_3 + d \times IQR
\]
Comúnmente, se utiliza un factor $d = 1.5$. Los valores fuera de este rango se consideran atípicos leves.

\subsubsection*{Explicación de las funciones utilizadas}
\begin{itemize}
    \item \texttt{boxplot(x, range=1.5, plot=FALSE)}: Aunque su uso principal es gráfico, con \texttt{plot=FALSE} la función no dibuja nada, sino que devuelve una lista con las estadísticas calculadas. El componente \texttt{\$out} de esta lista contiene los valores identificados como anómalos según el criterio del \texttt{range} (por defecto 1.5).
    \item \texttt{quantile(x, probs)}: Calcula los cuantiles de la distribución correspondientes a las probabilidades indicadas en \texttt{probs}. Se usa para obtener $Q_1$ (\texttt{probs = 0.25}) y $Q_3$ (\texttt{probs = 0.75}).
    \item \texttt{length(x)}: Devuelve el número de elementos (longitud) del vector \texttt{x}. Se usa en el bucle \texttt{for} para iterar sobre todas las observaciones de la variable 'r'.
\end{itemize}

\subsubsection*{Explicación del Método}
Para la detección de datos anómalos por este método se debe llevar a cabo siguiendo 4 pasos. $1^o$ se determina el valor del grado del outlier ($d$).
$2^o$ Se ordenan los datos para el parámetro que se quiere detectar los datos anómalos, en este caso la resistencia o r. $3^o$ Se calculan los valores del
$cuartil_{1}$ y del $cuartil_{3}$. $4^o$ se observa que valores quedan fuera del intervalo, el cual sería: $[cuartil_{1} - d * (cuartil_{3} - cuartil_{1}), cuartil_{3} + d * (cuartil_{3} - cuartil_{1})]$

\subsubsection*{Código y Ejecución (Sweave)}
<<Anomalos_Boxplot, echo=TRUE, results=verbatim>>=
# 1. Metodo de Caja y Bigotes (para la columna 'r')
(boxplot(muestra$r, range=1.5, plot=FALSE))

# Calculo manual
(cuar1r<-quantile(muestra$r, 0.25))
(cuar3r<-quantile(muestra$r, 0.75))
(int<-c(cuar1r-1.5*(cuar3r-cuar1r), cuar3r+1.5*(cuar3r-cuar1r)))

# Bucle para detectar outliers
for (i in 1:length(muestra$r))
{if (muestra$r[i]<int[1] || muestra$r[i]>int[2])
{print("el suceso");
print(i); print(muestra$r[i]); print("es un outlier")}}
@

\subsubsection*{Interpretación de resultados}
La salida de \texttt{boxplot(...)\$out} identifica el valor \texttt{14} como un outlier. El cálculo manual corrobora esto: el $Q_1$ es 4.1 y el $Q_3$ es 6.65. El IQR es $6.65 - 4.1 = 2.55$.
El intervalo de aceptación es $[4.1 - 1.5 \times 2.55, 6.65 + 1.5 \times 2.55]$, que resulta en $[0.275, 10.475]$.
El valor 14.0 (correspondiente al suceso 7) es mayor que 10.475, por lo que el bucle \texttt{for} lo identifica correctamente como "un outlier".

\subsubsection{Método 2: Desviación Típica}

\subsubsection*{Fundamento Teórico}
Este método paramétrico asume que los datos siguen una distribución normal (Gaussiana). En una distribución normal, la gran mayoría de los datos se concentran alrededor de la media. Según la regla empírica (68-95-99.7):
\begin{itemize}
    \item $\approx 68\%$ de los datos está a 1 desviación típica ($\sigma$) de la media ($\mu$).
    \item $\approx 95.5\%$ de los datos está a 2 desviaciones típicas.
    \item $\approx 99.7\%$ de los datos está a 3 desviaciones típicas.
\end{itemize}
El método define un intervalo de normalidad basado en la media y la desviación típica. Un valor de $d$ (grado de outlier) común es 2. Los puntos que caen fuera del intervalo $[\mu - d \times \sigma, \mu + d \times \sigma]$ se consideran anómalos. En este ejercicio se usa $d=2$.

\subsubsection*{Explicación de las funciones utilizadas}
\begin{itemize}
    \item \texttt{mean(x)}: Calcula la media aritmética ($\bar{x}$) del vector \texttt{x}.
    \item \texttt{var(x)}: Calcula la varianza \textbf{muestral} ($s^2$), que divide por $(n-1)$.
    \item \texttt{length(x)}: Devuelve $n$. El código calcula la varianza \textbf{poblacional} ($\sigma^2$) a partir de la muestral, usando la fórmula $\sigma^2 = s^2 \times \frac{n-1}{n}$.
    \item \texttt{sqrt(x)}: Calcula la raíz cuadrada, usada para obtener la desviación típica ($\sigma$) a partir de la varianza poblacional.
\end{itemize}

\subsubsection*{Explicación del Método}
Para la detección de datos anómalos por este método se debe llevar a cabo los siguiente 4 pasos. $1^o$ se determina el valor del grado de outlier($d$).
$2^o$ se debe hallar el valor de la media para el parámetro que se nos halla pedido, en este caso densidad o d (d_{mean}). $3^o$ Se debe calcular el valor de las
desviación típica para dicho parámetro ($d_{desv}$). $4^o$ Se debe ver que valores quedan fuera del intervalo, el cual sería: $[d_{mean} - d * d_{desv}, d_{mean} + d * d_{desv}]$

\subsubsection*{Código y Ejecución (Sweave)}
<<Anomalos_SD, echo=TRUE, results=verbatim>>=
# 2. Metodo de Desviacion Tipica (para la columna 'd')

# Calculo de la desviacion tipica poblacional
sdd<-sqrt(var(muestra$d)*((length(muestra$d)-1)/length(muestra$d)))

# Calculo del intervalo
(intdes<-c(mean(muestra$d)-2*sdd, mean(muestra$d)+2*sdd))

# Bucle para detectar outliers
for (i in 1:length(muestra$d))
{if (muestra$d[i]<intdes[1] ||  muestra$d[i]>intdes[2])
{print("el suceso");
print(i); print(muestra$d[i]); print("es un outlier")}}
@

\subsubsection*{Interpretación de resultados}
La media de 'd' es 5.657 y la desviación típica poblacional (sdd) es 2.857.
El intervalo de aceptación (con $d=2$) es $[5.657 - 2 \times 2.857, 5.657 + 2 \times 2.857]$, que resulta en $[-0.057, 11.371]$.
El bucle \texttt{for} itera sobre la variable 'd' y encuentra que el valor 12.0 (suceso 2) es mayor que 11.371, identificándolo como "un outlier".

\subsubsection{Método 3: Regresión (Error de Residuos)}

\subsubsection*{Fundamento Teórico}
Este método se usa para detectar outliers en datos bivariantes, identificando puntos que no se ajustan a la relación o tendencia general del resto de los datos.
Primero, se ajusta un modelo de regresión lineal (generalmente por mínimos cuadrados) para predecir $y$ a partir de $x$ ($\hat{y} = a + bx$).
Para cada punto $(x_i, y_i)$, se calcula el \textbf{residuo} o error, que es la diferencia entre el valor real y el valor predicho: $e_i = y_i - \hat{y}_i$.
Se calcula el error estándar de estos residuos ($Sr$). Un punto se considera anómalo si su residuo (en valor absoluto) es inusualmente grande, es decir, si $|e_i| > d \times Sr$ (donde $d$ suele ser 2 o 3). Esto significa que el punto está muy lejos (verticalmente) de la línea de tendencia.

\subsubsection*{Explicación de las funciones utilizadas}
\begin{itemize}
    \item \texttt{lm(formula)}: Siglas de "Linear Model". Es la función principal de R para ajustar modelos de regresión lineal. La fórmula \texttt{muestra\$d ~ muestra\$r} especifica que se debe modelar 'd' (variable dependiente) en función de 'r' (variable independiente).
    \item \texttt{summary(object)}: Proporciona un resumen detallado de un objeto modelo. Aplicado a un objeto \texttt{lm}, muestra los coeficientes, el R-cuadrado, estadísticos F y, fundamentalmente, los residuos del modelo.
    \item \texttt{summary(dfr)\$residuals}: Esta sintaxis accede al vector numérico de los residuos (los $e_i$) que está almacenado dentro del objeto \texttt{summary}.
    \item \texttt{sum(x)}: Suma los elementos de un vector. Se usa aquí para calcular la suma de los residuos al cuadrado (parte del cálculo del $Sr$).
\end{itemize}

\subsubsection*{Explicación del Método}
Para la detección de datos anómalos por este método se deben llevar a cabo los 7 siguientes pasos. $1^o$ Determinar el valor del grado de outlier ($d$). 
$2^o$ Se deben hallar los valores de $a$ y $b$ para poder tener una forma de predecir los valores de la densidad que vamos a llamar ($y_{hat}$). 
Para esto se tiene que hallar el valor de la covarianza, ya que el valor de $b$ sería la división del valor de la covarianza / la varianza de la resistencia ($x$); por último para hallar $a$: $a = y_{mean} - b \times x_{mean}$. De esta 
forma ya se puede obtener predicciones de los valores de la densidad: $y_{hat} = a + b \times x_i$. $3^o$ Se obtienen los valores predichos de la densidad para cada valor de la resistencia ($x_i$). 
$4^o$ Se calcula el error estandar de los residuos ($Sr$). $5^o$ Se obtiene el valor identicador de outliers que sería $d \times Sr$. $6^o$ Se obtiene el valor absoluto de la diferencia entre 
los valores de $y_i$ y los $y_{hat}$. $7^o$ Aquellos valores cuya diferencia supere el valor de $d \times Sr$ serán considerados outliers.

\subsubsection*{Código y Ejecución (Sweave)}
<<Anomalos_Regression, echo=TRUE, results=verbatim>>=
# 3. Metodo de Regresion (para 'd' en funcion de 'r')

# Calculo del modelo de regresion lineal
(dfr<-lm(muestra$d~muestra$r))
(summary(dfr))

# Obtencion de residuos
(res<-summary(dfr)$residuals)

# Calculo del error estandar de los residuos
sr<-sqrt(sum(res^2)/length(res))
sr

# Bucle para detectar outliers en los residuos
for (i in 1:length(res))
{if (res[i]>2*sr)
{print("el suceso");
print(res[i]); print("es un outlier")}}
@

\subsubsection*{Interpretación de resultados}
El modelo de regresión ajustado es $d = 6.01445 - 0.05723 \times r$. El error estándar de los residuos ($Sr$) es 2.85. El umbral de detección es $2 \times Sr = 5.70$.
El vector de residuos (los $e_i$) es \texttt{c(-3.84, 6.18, -1.64, ...)}.
El bucle \texttt{for} compara cada residuo con el umbral 5.70. Encuentra que el segundo residuo, 6.18587, es mayor que el umbral, por lo que imprime que el suceso 2 es "un outlier". Esto significa que el punto (3.5, 12.0) está significativamente por encima de la línea de regresión que describe la tendencia general de los datos.

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ejercicio 1.4: Detección de datos anómalos mediante el método de proximidad (K-NN manual) y de densidad (LOF)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Descripción del conjunto de datos}

El conjunto de datos utilizado está formado por las calificaciones de cinco estudiantes, donde cada observación se compone de dos variables cuantitativas: la nota de \textbf{Teoría} y la de \textbf{Laboratorio}.  
Las observaciones se representan como pares ordenados:
\[
(4,4), \; (4,3), \; (5,5), \; (1,1), \; (5,4)
\]
El objetivo del ejercicio es identificar posibles \emph{datos anómalos} o \emph{outliers} que se alejen significativamente del resto del conjunto, utilizando un método basado en la \textbf{proximidad} entre observaciones en el espacio bidimensional de calificaciones. Posteriormente, se vuelve a realizar el ejercicio emplean un método basado en la densidad.

\subsubsection*{Fundamento teórico}

El método de los \textit{K-vecinos más próximos} (K-NN) para detección de anomalías se fundamenta en el supuesto de que los puntos normales se encuentran próximos entre sí, mientras que los puntos anómalos se sitúan aislados a mayor distancia de sus vecinos.  

Formalmente, para cada observación \(x_i = (x_{i1}, x_{i2})\), se calcula la distancia euclídea respecto a las demás:
\[
d_{ij} = \sqrt{(x_{i1} - x_{j1})^2 + (x_{i2} - x_{j2})^2}
\]
Estas distancias se ordenan de menor a mayor, y se obtiene la distancia al K-ésimo vecino más próximo, \(d_K(x_i)\).  
Si este valor supera un umbral prefijado \(d^*\), la observación se considera un \emph{dato anómalo}:
\[
x_i \text{ es anómalo si } d_K(x_i) > d^*
\]
En este caso se utiliza \(K = 3\) y \(d^* = 2.5\).

\bigskip
\noindent
A diferencia del método de proximidad K-NN, el algoritmo \textbf{LOF (Local Outlier Factor)} se basa en la comparación de \textit{densidades locales} en lugar de distancias absolutas.  
La idea central es que los puntos anómalos no sólo se encuentran lejos de los demás, sino que además están situados en regiones de baja densidad respecto a sus vecinos más próximos.

En este caso, se emplea la \textbf{distancia de Manhattan} como medida de proximidad entre observaciones.  
Dada dos observaciones \(p = (x_1, y_1)\) y \(o = (x_2, y_2)\), la distancia Manhattan se define como:
\[
d(p, o) = |x_1 - x_2| + |y_1 - y_2|
\]
Este tipo de distancia es especialmente útil cuando las variables representan magnitudes comparables y se desea penalizar menos los desplazamientos diagonales que en la distancia euclídea.

\medskip
\noindent
El algoritmo LOF sigue los siguientes pasos conceptuales:

\begin{enumerate}
  \item \textbf{Cálculo de las distancias y determinación de los $k$ vecinos más cercanos.}  
  Para cada punto \(p_i\) se calculan sus distancias Manhattan respecto a todos los demás puntos del conjunto, y se identifican sus \(k\) vecinos más próximos \(N_k(p_i)\).
  
  \item \textbf{Estimación de la densidad local.}  
  La densidad local de un punto se define como el inverso de la distancia media a sus \(k\) vecinos más cercanos:
  \[
  D(p_i) = \frac{1}{\frac{1}{k} \sum_{o \in N_k(p_i)} d(p_i, o)}
  \]
  De este modo, los puntos situados en regiones densas tienen una densidad alta, mientras que los que se encuentran aislados presentan una densidad baja.
  
  \item \textbf{Cálculo del grado de rareza o \textit{DRM}.}  
  El \textit{DRM} de una observación se obtiene comparando su densidad local con la densidad media de sus vecinos:
  \[
  DRM(p_i) = \frac{\frac{1}{k} \sum_{o \in N_k(p_i)} D(o)}{D(p_i)}
  \]
  \item \textbf{Interpretación del resultado.}  
  \begin{itemize}
    \item Si \( DRM(p_i) \approx 1 \), el punto tiene una densidad similar a la de sus vecinos, por lo que se considera normal.
    \item Si \( DRM(p_i) > 1 \), el punto se encuentra en una región menos densa que la de sus vecinos, por lo que se considera potencialmente anómalo.
    \item Valores significativamente superiores a 1 indican \emph{anomalías fuertes}.
  \end{itemize}
\end{enumerate}

En resumen, el método LOF evalúa la anomalía de una observación no sólo por su distancia a los demás puntos, sino por su densidad relativa en el entorno local.  

\subsubsection*{Implementación en R de K-NN manual}
El algoritmo mencionado es implementado de forma completa mediante el uso de funciones de base en R, sin recurrir a paquetes externos.

\paragraph{1) Definición del conjunto de datos.}
El conjunto de datos, como se ha mencionado, está formado por cinco estudiantes con sus respectivas calificaciones en dos dimensiones: \textbf{Teoría} y \textbf{Laboratorio}.

<<crear_matriz, echo=TRUE, results=verbatim>>=
# Creación de la matriz (2 variables x 5 observaciones)
muestra <- matrix(c(4,4,  4,3,  5,5,  1,1,  5,4), nrow = 2, ncol = 5)
muestra
@

La función \texttt{matrix()} construye una matriz a partir de un vector.  
Sus argumentos principales son:
\begin{itemize}
  \item \texttt{data}: vector con los valores que se distribuirán en la matriz (en este caso, las calificaciones de teoría y laboratorio).
  \item \texttt{nrow} y \texttt{ncol}: número de filas y columnas respectivamente.
  \item Por defecto, R rellena la matriz por columnas, por lo que en esta configuración cada \textbf{columna} representa un estudiante y cada \textbf{fila} una variable.
\end{itemize}

Sin embargo, las funciones de análisis de distancias en R esperan que las observaciones estén en filas.
Por ello, se realiza una transposición de la matriz para adecuarla a este formato:

<<transposicion, echo=TRUE, results=verbatim>>=
# Transposición: filas = estudiantes, columnas = variables
muestra <- t(muestra)
muestra
@

La función \texttt{t()} (de \textit{transpose}) intercambia filas y columnas de la matriz, de forma que cada \textbf{fila} corresponde ahora a un estudiante y cada \textbf{columna} a una variable (Teoría o Laboratorio).

\paragraph{2) Cálculo de distancias euclídeas.}
Se calculan todas las distancias euclídeas entre los pares de observaciones.  
La distancia entre dos puntos \( (x_1, y_1) \) y \( (x_2, y_2) \) se define como:

\[
d = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}
\]

En R, esto se implementa con la función \texttt{dist()}:

<<distancias, echo=TRUE, results=verbatim>>=
# Cálculo de la matriz de distancias euclídeas
distancia <- as.matrix(dist(muestra))
distancia
@

\begin{itemize}
  \item \texttt{dist(x, method = "euclidean")} calcula una matriz de distancias por pares entre las filas de \texttt{x}, usando por defecto la métrica euclídea.
  \item La función devuelve un objeto de tipo \texttt{dist}, que se convierte en una matriz convencional mediante \texttt{as.matrix()} para su lectura y posterior procesamiento.
\end{itemize}

Cada elemento \( d_{ij} \) de la matriz representa la distancia entre los estudiantes \( i \) y \( j \). 
Todos los elementos de la diagonal son cero, ya que la distancia de un punto a sí mismo es nula.

\paragraph{3) Ordenación de las distancias.}
Con el objetivo de encontrar los K-vecinos más próximos, se ordenan las distancias para cada observación en orden ascendente:

<<ordenar_distancias, echo=TRUE, results=verbatim>>=
# Ordenar distancias (de menor a mayor) para cada observación
for (i in 1:5) {
  distancia[, i] <- sort(distancia[, i])
}
distanciasord <- distancia
distanciasord
@

\begin{itemize}
  \item El bucle \texttt{for} itera sobre las cinco observaciones (una por estudiante).
  \item La función \texttt{sort(x)} ordena los valores del vector \texttt{x} en orden ascendente.
  \item La matriz resultante \texttt{distanciasord} contiene las distancias ordenadas por columnas, donde las filas corresponden a los vecinos más cercanos.
\end{itemize}

La primera fila de cada columna es siempre cero (distancia al mismo punto), seguida por las distancias a los vecinos más próximos.  
La cuarta fila contiene la distancia al tercer vecino más cercano (\( K = 3 \)).

\paragraph{4) Detección de valores anómalos.}
Se establece un umbral de decisión \( d^* = 2.5 \).  
Aquellas observaciones cuya distancia al tercer vecino supere el umbral se consideran anómalas:

<<detectar_outliers, echo=TRUE, results=verbatim>>=
# Umbral de decisión
umbral <- 2.5

# Identificación de outliers
for (i in 1:5) {
  if (distanciasord[4, i] > umbral) {
    print(i)
    print("es un outlier")
  }
}
@

\begin{itemize}
  \item \texttt{umbral} fija la distancia límite para considerar una observación como anómala.
  \item La condición \texttt{if(distanciasord[4, i] > umbral)} evalúa si la distancia al tercer vecino (posición 4 en la columna, contando la propia observación) excede ese límite.
  \item Si se cumple, se imprime el número de la observación y el mensaje “es un outlier”.
\end{itemize}

El resultado muestra:

\begin{verbatim}
[1] 4
[1] "es un outlier"
\end{verbatim}

Esto indica que el estudiante 4, con calificaciones (1,1), es un dato anómalo según el criterio establecido.

\paragraph{5) Interpretación y análisis.}
En el espacio bidimensional de las calificaciones, los puntos \((4,4)\), \((4,3)\), \((5,5)\) y \((5,4)\) se agrupan cercanamente, representando rendimientos similares.  
En contraste, el punto \((1,1)\) está alejado del resto, a una distancia superior a cuatro unidades euclídeas.  
Este aislamiento explica su clasificación como outlier mediante el criterio de proximidad.

Analizándolo de forma geométrica, las observaciones se encuentran en la zona superior derecha del plano, mientras que el punto anómalo se sitúa en la esquina inferior izquierda.  
Este método basado en la distancia euclídea ha permitido identificar eficazmente la observación que difiere significativamente del patrón general del conjunto de datos, en base a los criterios de proximidad establecidos.


\subsubsection*{Implementación en R de LOF (Local Outlier Factor).}

A continuación, se implementa el método LOF (Local Outlier Factor) mediante el paquete \texttt{Rlof}, que automatiza el cálculo de la densidad local y la comparación con las densidades de los vecinos próximos.  
Este enfoque complementa el método K-NN al introducir el análisis de densidades relativas en lugar de distancias absolutas, permitiendo una evaluación más robusta de la rareza de cada observación en función de la \textbf{densidad local relativa}.

\paragraph{1) Preparación del entorno y definición del conjunto de datos.}
Se instala y carga el paquete \texttt{Rlof}, y se define la matriz de calificaciones con las dos variables \textbf{Teoría} y \textbf{Laboratorio} correspondientes a los cinco estudiantes.

<<lof_datos, echo=TRUE, results=verbatim>>=
# Instalación y carga del paquete Rlof
# install.packages("Rlof")
library(Rlof)

# Definición de la matriz de datos (5 observaciones x 2 variables)
muestra <- matrix(c(4,4,
                    4,3,
                    5,5,
                    1,1,
                    5,4), 
                  byrow = TRUE, ncol = 2)

# Asignación de nombres a filas y columnas
colnames(muestra) <- c("Teoria", "Laboratorio")
rownames(muestra) <- 1:5

# Visualización de la matriz de datos
print("Matriz de datos:")
print(muestra)
@

En esta matriz, cada fila representa a un estudiante y cada columna una calificación.  
La estructura bidimensional permite aplicar directamente el análisis de proximidad y densidad mediante las funciones del paquete.

\paragraph{2) Cálculo del factor LOF con distancia de Manhattan.}
Se elige el número de vecinos \( k = 3 \) para definir la vecindad local de cada observación.  
La función principal utilizada, \texttt{lof()}, permite especificar la métrica de distancia mediante el argumento \texttt{method}.  
En este caso se selecciona la \textbf{distancia Manhattan}, definida como:

\[
d_M(p, q) = \sum_{i=1}^{n} |p_i - q_i|
\]

Esta métrica resulta especialmente útil en variables cuantitativas de escala comparable, ya que mide la discrepancia entre observaciones como la suma de diferencias absolutas por dimensión.

<<lof_calculo, echo=TRUE, results=verbatim>>=
# Selección del parámetro k
k <- 3

# Cálculo del factor LOF utilizando distancia de Manhattan
lof_scores <- Rlof::lof(muestra, k = k, method = "manhattan")

# Visualización de los valores LOF (redondeados a 3 decimales)
print(paste("Valores LOF para k =", k, "usando distancia Manhattan:"))
print(round(lof_scores, 3))
@

El vector \texttt{lof\_scores} contiene un valor por observación.  
Un valor cercano a 1 indica que la densidad local del punto es similar a la de sus vecinos; valores significativamente superiores a 1 implican una menor densidad local y, por tanto, una mayor probabilidad de anomalía.

\paragraph{3) Identificación de observaciones anómalas.}
Se establece un umbral de decisión \( LOF > 1.5 \).  
Las observaciones cuyo valor LOF supera dicho umbral se clasifican como \textbf{datos anómalos}.

<<lof_outliers, echo=TRUE, results=verbatim>>=
# Umbral de decisión
umbral <- 1.5

# Identificación de observaciones anómalas
outliers <- which(lof_scores > umbral)

# Presentación de resultados
if (length(outliers) == 0) {
  cat("\nNo se detectaron outliers (LOF <", umbral, ")\n")
} else {
  cat("\nObservaciones detectadas como outliers (LOF >", umbral, "):\n")
  print(outliers)
  print("Coordenadas de los outliers:")
  print(muestra[outliers, ])
}
@

En este caso, la salida del programa muestra el índice de las observaciones detectadas como anómalas y sus coordenadas en el espacio bidimensional de calificaciones.

\paragraph{4) Interpretación de resultados.}
El análisis mediante el método LOF utilizando la distancia de Manhattan produce resultados coherentes con el método K-NN: el estudiante 4, con calificaciones \((1,1)\), presenta una densidad local marcadamente inferior respecto al resto del conjunto.  
Esto confirma su condición de \textbf{dato anómalo}, ya que la suma de las diferencias absolutas respecto a los demás puntos supera ampliamente las observadas entre los restantes estudiantes.

El uso del paquete \texttt{Rlof} y de la métrica de Manhattan aporta una perspectiva complementaria: en lugar de evaluar sólo la proximidad geométrica, el análisis considera la \textbf{densidad local relativa}, lo que permite identificar puntos aislados incluso en regiones de diferente dispersión.  
Además, la posibilidad de ajustar la métrica de distancia y el número de vecinos convierte a este método en una herramienta flexible para la detección de anomalías en conjuntos de datos de distinta naturaleza.

\paragraph{Descripción del paquete \texttt{Rlof}.}

El paquete \texttt{Rlof} (\textit{R Parallel Implementation of Local Outlier Factor}) proporciona una implementación optimizada y paralelizada del algoritmo LOF propuesto.  
Su principal objetivo es acelerar el cálculo del \textbf{factor de aislamiento local} mediante el uso de múltiples núcleos de CPU y estructuras de datos eficientes para la gestión de distancias.

El paquete depende de los módulos \texttt{doParallel} y \texttt{foreach}, que permiten el procesamiento en paralelo de varios valores de \(k\) y la ejecución simultánea de cálculos de densidad local.  
De esta forma, se incrementa significativamente el rendimiento en conjuntos de datos de gran tamaño.

Entre sus características más relevantes destacan:

\begin{itemize}
  \item Soporte para múltiples medidas de distancia (\texttt{"euclidean"}, \texttt{"manhattan"}, \texttt{"canberra"}, \texttt{"binary"}, \texttt{"minkowski"}), especificadas mediante el argumento \texttt{method}.
  \item Posibilidad de calcular varios valores de \(k\) en paralelo (\texttt{k} puede ser un vector).
  \item Utilización automática de todos los núcleos disponibles si no se indica el parámetro \texttt{cores}.
  \item Compatibilidad con matrices y \texttt{data.frame} como estructuras de entrada.
\end{itemize}

Internamente, el paquete implementa dos funciones principales:

Internamente, el paquete implementa dos funciones principales:

\begin{enumerate}
  \item \textbf{\texttt{distmc()}}: calcula una matriz de distancias entre observaciones con soporte multihilo.  
  Es funcionalmente equivalente a \texttt{dist()} del paquete base \texttt{stats}, pero permite paralelización y soporta un mayor número de métricas.  
  Su sintaxis general es:
  \[
  \texttt{distmc(x, method = "euclidean", diag = FALSE, upper = FALSE, p = 2)}
  \]
  donde:
  \begin{itemize}
    \item \texttt{x}: matriz o \texttt{data.frame} numérico que contiene las observaciones en filas y las variables en columnas.
    \item \texttt{method}: determina la métrica de distancia a utilizar; puede ser \texttt{"euclidean"}, \texttt{"manhattan"}, \texttt{"canberra"}, \texttt{"binary"} o \texttt{"minkowski"}.
    \item \texttt{diag}: indica si la diagonal debe mostrarse en la matriz de salida (\texttt{FALSE} por defecto).
    \item \texttt{upper}: controla si se devuelve la parte superior de la matriz (\texttt{FALSE} por defecto).
    \item \texttt{p}: parámetro de la distancia de Minkowski (por defecto \( p = 2 \), que equivale a la distancia euclídea).
  \end{itemize}
  La función devuelve un objeto de tipo \texttt{dist}, que puede transformarse en una matriz mediante \texttt{as.matrix()} para su visualización o uso posterior.  
  En este trabajo se especifica el argumento \texttt{method = "manhattan"} para utilizar la distancia de Manhattan, más robusta frente a variaciones grandes en una única variable.

  \item \textbf{\texttt{lof()}}: calcula el \textbf{Local Outlier Factor} (LOF) para cada observación del conjunto de datos, comparando su densidad local con la de sus \(k\) vecinos más próximos.  
  La llamada básica es:
  \[
  \texttt{lof(data, k, cores = NULL, method = "euclidean")}
  \]
  donde:
  \begin{itemize}
    \item \texttt{data}: conjunto de datos en formato matriz o \texttt{data.frame}.
    \item \texttt{k}: número de vecinos más próximos considerados para el cálculo del LOF.
    \item \texttt{cores}: número de núcleos de CPU utilizados para el cálculo en paralelo. Si se deja en \texttt{NULL}, se emplean todos los disponibles.
    \item \texttt{method}: métrica de distancia usada para determinar los vecinos más cercanos, idéntica a la de \texttt{distmc()}.
  \end{itemize}
  La función devuelve un vector numérico del mismo tamaño que el número de observaciones, donde cada elemento representa el valor \( LOF_i \) asociado a la observación \( i \).  
  Valores cercanos a 1 indican una densidad local similar a la de los vecinos (observación normal), mientras que valores significativamente mayores que 1 reflejan una menor densidad relativa, identificando la observación como potencial \textbf{outlier}.
\end{enumerate}


La combinación de ambas funciones permite aplicar el método LOF de forma flexible, eficiente y reproducible, con control explícito sobre la métrica de distancia y el número de vecinos.  
En este proyecto, \texttt{Rlof} se emplea con la distancia de Manhattan y \(k = 3\), optimizando el cálculo de densidades locales y detectando automáticamente los puntos con menor densidad relativa en el espacio de calificaciones.

\footnote{Basado en la documentación del paquete \texttt{Rlof} (versión 1.1.3), desarrollado por Yingsong Hu, Wayne Murray y Yin Shan. Publicado en CRAN el 16 de octubre de 2022.}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ejercicio 2.1: Análisis Descriptivo de Distancias}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Descripción del conjunto de datos y objetivo}
En este ejercicio se hace uso de un fichero \texttt{distancias.txt}, que dispone de observaciones emparejadas de:
\begin{itemize}
\item \textbf{Población de residencia} (variable cualitativa nominal), y
\item \textbf{Distancia en km} desde el domicilio del estudiante hasta la Universidad (variable cuantitativa continua).
\end{itemize}
El enunciado cuenta con el universo de datos y los valores a emplear en este apartado, por ejemplo: \emph{Villalbilla, 16.5; Ensanche de Vallecas, 34.8; Alcalá de Henares, 6.2; …; Madrid, 46; ND, 3.7; …}. La presencia del marcador \textbf{ND} indica casos con localidad no disponible para esa observación, mientras que la distancia sí está informada. Estos pares {población, distancia} constituyen la base para el \textbf{análisis descriptivo} requerido en el ejercicio 2.1: ordenar por distancia, calcular rango, tablas de frecuencias (absoluta, acumulada, relativa) e indicadores de tendencia y dispersión sobre la variable cuantitativa.

\paragraph{Estructura del fichero.}
El fichero \texttt{distancias.txt} se lee con cabecera (\texttt{header = TRUE}) y contiene dos columnas:
\begin{center}
\begin{tabular}{ll}
\texttt{población} & (carácter/factor): nombre de la población o \texttt{ND} si no consta. \\[2pt]
\texttt{distancia} & (numérico, km): distancia real en kilómetros con decimales. \\
\end{tabular}
\end{center}

\paragraph{Calidad y decisiones sobre datos.}
\begin{itemize}
\item \textbf{Unidades y formato:} la distancia está en kilómetros; se preservan los decimales tal y como aparecen en el enunciado (p. ej., 16.5, 34.8, 31.4).
\item \textbf{Valores ND en \texttt{población}:} no impiden el análisis cuantitativo porque la métrica principal se aplica sobre \texttt{distancia}. Se mantienen sin imputación ni eliminación, preservando la consistencia del conjunto.
\item \textbf{Duplicados intencionales:} algunas parejas se repiten (p. ej., \emph{Cifuentes, 24} o varias \emph{Guadalajara, 30}), lo que es coherente con un conteo de frecuencias donde se esperan empates de distancia por redondeo.
\end{itemize}

\subsubsection*{Fundamentos teóricos del análisis descriptivo}

El análisis estadístico descriptivo tiene como finalidad resumir y comprender la información contenida en un conjunto de datos mediante medidas que describen su tendencia central, su variabilidad y su forma. En este ejercicio se aplican dichos conceptos sobre la variable \texttt{distancia}, de naturaleza cuantitativa continua.

\paragraph{Tipos de variables.}
De acuerdo con la tipología presentada en la Lección 1:
\begin{itemize}
  \item Una \textbf{variable cualitativa nominal} (como \texttt{población}) clasifica observaciones en categorías sin orden inherente.
  \item Una \textbf{variable cuantitativa continua} (como \texttt{distancia}) toma valores numéricos dentro de un rango real y admite decimales.
\end{itemize}

\paragraph{Frecuencias y distribuciones.}
Sea un conjunto de observaciones $x_1, x_2, \ldots, x_n$.  
La \textbf{frecuencia absoluta} $f_i$ indica cuántas veces aparece cada valor o intervalo.  
La \textbf{frecuencia relativa} $r_i$ se define como $r_i = f_i / n$, expresando la proporción de casos.  
Las \textbf{frecuencias acumuladas} ($F_i$, $R_i$) suman las observaciones hasta cada valor:
\[
F_i = \sum_{j \le i} f_j, \qquad
R_i = \sum_{j \le i} r_j
\]
Estas medidas permiten construir la distribución empírica de la variable y analizar su forma.

\paragraph{Medidas de tendencia central.}
Entre los valores posibles de una variable cuantitativa, la \textbf{media aritmética} representa el centro de gravedad de la distribución:
\[
\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i
\]
La media es sensible a valores extremos y resume el nivel típico de la variable.

\paragraph{Medidas de dispersión.}
La \textbf{dispersión} refleja cuánto se alejan los datos de la media:
\begin{itemize}
  \item El \textbf{rango} mide la amplitud total de la distribución: $\max(x) - \min(x)$.
  \item La \textbf{varianza poblacional} cuantifica la media de las desviaciones cuadráticas:
  \[
  \sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \bar{x})^2
  \]
  \item La \textbf{varianza muestral} incorpora la corrección de Bessel:
  \[
  s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2
  \]
  \item La \textbf{desviación estándar} es la raíz cuadrada de la varianza y mantiene las mismas unidades que la variable original.
\end{itemize}

\paragraph{Interpretación aplicada.}
En el contexto de este ejercicio, la media representa la distancia promedio de los estudiantes al campus, mientras que el rango y la desviación estándar indican la heterogeneidad geográfica del grupo. Una desviación alta implica dispersión en la procedencia, y una baja, concentración en torno a una distancia media.


\subsubsection*{Definición y explicación de las funciones utilizadas}

\paragraph{1. Funciones de conteo y estructura de datos.}
\begin{itemize}
\item \textbf{\texttt{contar\_filas(vec)}}: esta función toma un vector y recorre sus elementos haciendo uso de un bucle \texttt{for}. En cada iteración incrementa un contador inicializado a cero. El resultado final es el número total de elementos del vector. Se trata de un equivalente de la función \texttt{length()} de R, pero implementada manualmente. El mecanismo subyacente consiste en una lectura secuencial del vector y acumulación iterativa. El valor retornado permite validar la carga correcta del fichero y es utilizado posteriormente para el cálculo de frecuencias relativas. \ \textit{Argumentos:} \texttt{vec} (vector de cualquier tipo). \ \textit{Devuelve:} un entero con el número total de elementos.

<<contar_filas, eval=TRUE, echo=TRUE, results=hide>>=
# Función para contar filas 
contar_filas <- function(vec) {
  # Se inicializa el contador
  n <- 0
  # Se itera elemento a elemento y se incrementa el contador
  for (._ in vec) n <- n + 1
  # Se retorna el total contado
  n
}
@

\item \textbf{\texttt{contar\_columnas(df)}}: de modo análogo a la función anterior, recorre la lista de nombres de las columnas (\texttt{names(df)}) y aplica el mismo proceso de conteo. Permite verificar la integridad estructural del conjunto de datos leído. \ \textit{Argumentos:} \texttt{df} (\texttt{data.frame}). \ \textit{Devuelve:} un entero con el número de columnas.

<<contar_columnas, eval=TRUE, echo=TRUE, results=hide>>=
# Función para contar columnas 
contar_columnas <- function(df) {
  # Se inicializa el contador
  c <- 0
  # Se itera sobre los nombres de las columnas y se incrementa el contador
  for (._ in names(df)) c <- c + 1
  # Se retorna el total contado
  c
}
@

\item \textbf{\texttt{contar\_elementos(x)}}: versión genérica aplicable a cualquier tipo de vector. Es un ejemplo de \emph{abstracción algorítmica}, ya que encapsula el patrón de conteo iterativo en una única función reutilizable, sin depender del tipo de dato.

<<contar_elementos, eval=TRUE, echo=TRUE, results=hide>>=
# Función para contar cuántos elementos hay en un vector (sin length)
contar_elementos <- function(x) {
  # Se inicializa el contador
  n <- 0
  # Se incrementa por cada elemento del vector
  for (._ in x) n <- n + 1
  # Se retorna el total
  n
}
@

\paragraph{2. Función de ordenación: \texttt{ordenar\_indices(x, decreasing = FALSE)}.}
Esta función reproduce el algoritmo clásico de \emph{ordenamiento por burbuja}, donde se realizan comparaciones sucesivas entre pares adyacentes mediante estructuras de control anidadas. El vector de entrada \texttt{x} no se reordena directamente, sino que se construye un vector de \emph{índices} que indica el orden en que los elementos deberían aparecer para estar ordenados. Durante el proceso, se comparan los pares adyacentes e intercambian los índices cuando la condición de orden no se cumple (\texttt{x[i] > x[i+1]} o la inversa si \texttt{decreasing = TRUE}). Tras varias pasadas, los elementos mayores “flotan” hacia el final del vector. \ \textit{Argumentos:} \texttt{x} (vector numérico o comparable), \texttt{decreasing} (lógico, indica si el orden debe ser descendente). \ \textit{Devuelve:} un vector de índices con el orden resultante.

<<ordenar_indices, eval=TRUE, echo=TRUE, results=hide>>=
# Función para ordenar índices 
ordenar_indices <- function(x, decreasing = FALSE) {
  # Se cuenta manualmente el número de elementos
  n <- 0
  for (._ in x) n <- n + 1
  # Se crea un vector con los índices 1..n
  idx <- 1:n
  # Se aplica un algoritmo de ordenamiento (burbuja)
  for (i in 1:(n - 1)) {
    for (j in (i + 1):n) {
      # Si el orden es ascendente se compara menor que, si es descendente mayor que
      if ((!decreasing && x[j] < x[i]) || (decreasing && x[j] > x[i])) {
        # Intercambiar elementos
        tmp <- x[i]; x[i] <- x[j]; x[j] <- tmp
        # Intercambiar índices
        tmp2 <- idx[i]; idx[i] <- idx[j]; idx[j] <- tmp2
      }
    }
  }
  # Se retornan los índices ordenados
  idx
}
@

\paragraph{3. Función de rango: \texttt{rango\_manual(x)}.}
El rango se define como la diferencia entre el valor máximo y el mínimo del conjunto de datos. En lugar de usar las funciones \texttt{max()} y \texttt{min()}, esta función inicia dos variables (\texttt{max} y \texttt{min}) con el primer valor del vector y las actualiza secuencialmente si encuentra un valor mayor o menor. Al finalizar, devuelve la diferencia entre ambos valores, cuantificando la amplitud total de la variable. El rango es una medida de dispersión simple que representa la diferencia entre el estudiante más próximo y el más lejano a la universidad. \ \textit{Argumentos:} \texttt{x} (vector numérico). \ \textit{Devuelve:} un valor numérico correspondiente al rango.

<<rango_manual, eval=TRUE, echo=TRUE, results=hide>>=
# Función para calcular el rango 
rango_manual <- function(x) {
  # Inicializar máximo y mínimo con el primer valor
  maximo <- x[1]
  minimo <- x[1]
  # Iterar sobre los valores para encontrar máximo y mínimo
  for (valor in x) {
    # Si el valor es mayor que el máximo actual, actualizar máximo
    if (valor > maximo) maximo <- valor
    # Si el valor es menor que el mínimo actual, actualizar mínimo
    if (valor < minimo) minimo <- valor
  }
  # Retornar el rango
  maximo - minimo
}
@

\paragraph{4. Obtención de valores únicos: \texttt{valores\_unicos\_manual(x)}.}
Esta función construye un nuevo vector \texttt{unicos} inicializado vacío y recorre todos los elementos de \texttt{x}. En cada iteración comprueba si el valor actual ya está dentro de \texttt{unicos}; si no lo está, se incorpora mediante concatenación. Al finalizar el bucle, se ordenan los valores resultantes haciendo uso de los índices obtenidos con la función \texttt{ordenar\_indices()}. Este proceso combina una búsqueda secuencial (comparación elemento a elemento) con una inserción condicional, y es esencial para construir las tablas de frecuencias. \ \textit{Argumentos:} \texttt{x} (vector numérico o de caracteres). \ \textit{Devuelve:} un vector de valores únicos ordenados.

<<valores_unicos_manual, eval=TRUE, echo=TRUE, results=hide>>=
# Función para obtener valores únicos
valores_unicos_manual <- function(x) {
  # En primer lugar, se crea un vector vacío para almacenar los valores únicos
  unicos <- c()
  # Se itera sobre cada valor
  for (v in x) {
    # Flag para indicar si el valor ya fue encontrado
    encontrado <- FALSE
    # Se verifica si el valor ya está en el vector de valores únicos
    for (u in unicos) {
      if (v == u) { encontrado <- TRUE; break }
    }
    # Si el valor no fue encontrado, se agrega al vector de valores únicos
    if (!encontrado) unicos <- c(unicos, v)
  }
  # Se ordenan los valores únicos y se retornan
  unicos[ordenar_indices(unicos)]
}
@

\paragraph{5. Frecuencias: conteo, acumulación y normalización.}
\begin{itemize}
\item \textbf{\texttt{frecuencia\_absoluta\_manual(x, valores\_ordenados)}}: recibe el vector original y el conjunto de valores únicos previamente ordenado. Para cada valor único, recorre nuevamente el vector \texttt{x} y aumenta el contador correspondiente cuando encuentra coincidencias. El resultado es un vector de igual longitud que \texttt{valores\_ordenados}, donde cada posición representa cuántas veces aparece el valor correspondiente. La función utiliza una estructura de bucles anidados que refuerza la comprensión del proceso de clasificación en categorías. Reproduce el comportamiento de \texttt{table()} de R, pero implementado manualmente. \ \textit{Argumentos:} \texttt{x} (vector de datos), \texttt{valores\_ordenados} (vector de únicos). \ \textit{Devuelve:} un vector numérico con las frecuencias absolutas.

<<frecuencia_absoluta_manual, eval=TRUE, echo=TRUE, results=hide>>=
# Función para calcular la frecuencia absoluta 
frecuencia_absoluta_manual <- function(x, valores_ordenados) {
  # Se obtiene el número de valores únicos
  k <- contar_elementos(valores_ordenados)
  # Inicializar el vector de frecuencias absolutas
  frec <- numeric(k)
  # Se itera sobre cada valor observado
  for (v in x) {
    # Se compara con cada valor único
    for (i in 1:k) {
      # Si coincide, se incrementa la frecuencia absoluta correspondiente
      if (v == valores_ordenados[i]) { frec[i] <- frec[i] + 1; break }
    }
  }
  # Se retorna el vector de frecuencias
  frec
}
@

\item \textbf{\texttt{acumulada\_manual(vec)}}: calcula la suma progresiva de los elementos de un vector. Se inicializa un acumulador que, en cada iteración, añade el valor actual y lo almacena en un nuevo vector. Esta función ejemplifica un algoritmo iterativo que modela una suma acumulada sin funciones de orden superior. \ \textit{Argumentos:} \texttt{vec} (vector numérico). \ \textit{Devuelve:} un vector numérico de igual longitud con las sumas acumuladas.

<<acumulada_manual, eval=TRUE, echo=TRUE, results=hide>>=
# Función para calcular la frecuencia acumulada 
acumulada_manual <- function(vec) {
  # Se cuenta cuántos elementos tiene el vector
  n <- contar_elementos(vec)
  # Inicializar el vector de frecuencias acumuladas
  out <- numeric(n)
  # Se suman progresivamente las frecuencias
  acum <- 0
  for (i in 1:n) { acum <- acum + vec[i]; out[i] <- acum }
  # Se retorna la acumulada
  out
}
@

\item \textbf{\texttt{relativa\_manual(frec\_abs, total)}}: transforma las frecuencias absolutas en relativas dividiendo cada componente de \texttt{frec\_abs} entre el número total de observaciones. La precisión de este paso depende de que \texttt{total} coincida con el número real de filas, garantizado mediante \texttt{contar\_filas()}. El resultado permite interpretar los valores en términos porcentuales y facilita la construcción de histogramas o distribuciones empíricas. \ \textit{Argumentos:} \texttt{frec\_abs} (vector numérico), \texttt{total} (entero). \ \textit{Devuelve:} un vector de frecuencias relativas en $[0,1]$.
\end{itemize}

<<relativa_manual, eval=TRUE, echo=TRUE, results=hide>>=
# Función para calcular la frecuencia relativa manual
relativa_manual <- function(frec_abs, total) {
  # Se cuenta el tamaño del vector de frecuencias absolutas
  n <- contar_elementos(frec_abs)
  # Inicializar el vector de frecuencias relativas
  out <- numeric(n)
  # Se calcula dividiendo la frecuencia absoluta entre el total de filas
  for (i in 1:n) out[i] <- frec_abs[i] / total
  # Se retorna la frecuencia relativa
  out
}
@
\end{itemize}

\paragraph{6. Medidas de tendencia central y dispersión.}
\begin{itemize}
\item \textbf{\texttt{media\_manual(x)}}: la media se obtiene sumando todos los elementos del vector y dividiendo por el número total de observaciones. Los valores se acumulan en un bucle \texttt{for}, asegurando que el acumulador se inicialice a cero para evitar errores. Este proceso muestra explícitamente cómo la media representa el centro de gravedad de la distribución de datos. \ \textit{Argumentos:} \texttt{x} (vector numérico). \ \textit{Devuelve:} la media aritmética.

<<media_manual, eval=TRUE, echo=TRUE, results=hide>>=
# Función para calcular la media 
media_manual <- function(x) {
  # Se incializa la suma a cero
  suma <- 0
  # Se cuenta el número de elementos
  n <- contar_elementos(x)
  # Se suma cada valor
  for (v in x) suma <- suma + v
  # Se retorna la media
  suma / n
}
@

\item \textbf{\texttt{varianza\_poblacional\_manual(x)}} y \textbf{\texttt{varianza\_muestral\_manual(x)}}: ambas funciones siguen la definición formal de la varianza. Primero se calcula la media con \texttt{media\_manual(x)}, luego se recorre el vector calculando la desviación cuadrática de cada elemento respecto a la media, y se acumula el resultado. La versión muestral aplica la corrección de Bessel ($N-1$) para eliminar el sesgo en la estimación de la varianza poblacional. La comparación entre ambas versiones permite apreciar cómo la corrección de Bessel evita la subestimación sistemática de la varianza cuando solo se dispone de una muestra. \ \textit{Argumentos:} \texttt{x} (vector numérico). \ \textit{Devuelve:} un valor numérico (varianza poblacional o muestral).

<<varianzas, eval=TRUE, echo=TRUE, results=hide>>=
# Función para calcular la varianza poblacional 
varianza_poblacional_manual <- function(x) {
  # Se cuenta el número de elementos
  n <- contar_elementos(x)
  # Se calcula la media manualmente
  m <- media_manual(x)
  # Se calcula la suma de los cuadrados de las diferencias respecto a la media
  suma_cuad <- 0
  for (v in x) suma_cuad <- suma_cuad + (v - m)^2
  # Varianza poblacional: dividir entre N
  suma_cuad / n
}

# Función para calcular la varianza muestral 
varianza_muestral_manual <- function(x) {
  # Se cuenta el número de elementos
  n <- contar_elementos(x)
  # Si no hay suficientes datos, se retorna NA
  if (n <= 1) return(NA_real_)
  # Se calcula la media manualmente
  m <- media_manual(x)
  # Se calcula la suma de los cuadrados de las diferencias respecto a la media
  suma_cuad <- 0
  for (v in x) suma_cuad <- suma_cuad + (v - m)^2
  # Varianza muestral: dividir entre (N - 1)
  suma_cuad / (n - 1)
}
@

\item \textbf{\texttt{desv\_estandar(varianza)}}: toma la raíz cuadrada de la varianza mediante el operador \texttt{sqrt()}, devolviendo una medida de dispersión expresada en las mismas unidades que la variable original (kilómetros). Esta equivalencia de unidades facilita la interpretación práctica, ya que permite afirmar, por ejemplo, que “la distancia media difiere en aproximadamente 20 km respecto al promedio”. \ \textit{Argumentos:} \texttt{varianza} (valor numérico). \ \textit{Devuelve:} la desviación estándar.

<<desv_estandar, eval=TRUE, echo=TRUE, results=hide>>=
# Función para obtener la desviación estándar a partir de una varianza
desv_estandar <- function(varianza) {
  # Si la varianza es NA, se retorna NA
  if (is.na(varianza)) return(NA_real_)
  # Se calcula la raíz cuadrada de la varianza
  sqrt(varianza)
}
@
\end{itemize}

\paragraph{7. Integración en el flujo principal.}
Las funciones anteriores se integran secuencialmente en el flujo de trabajo del ejercicio:
\begin{enumerate}
\item Carga del fichero \texttt{distancias.txt} en el \texttt{data.frame} \texttt{s}.
\item Verificación estructural mediante las funciones de conteo.
\item Ordenación de observaciones y creación de los data frames \texttt{so\_asc} y \texttt{so\_desc}.
\item Cálculo del rango, valores únicos y tabla de frecuencias.
\item Derivación de medidas de resumen (media, varianzas y desviación estándar).
\end{enumerate}
Cada función ha sido implementada con independencia modular, lo que facilita su verificación individual, su reutilización en otros análisis y la trazabilidad completa del proceso estadístico.

\subsubsection*{Ejecución del script principal}

En este bloque se ejecuta el flujo completo del ejercicio, integrando todas las funciones definidas anteriormente.  
El objetivo es validar su correcto funcionamiento y mostrar los resultados del análisis descriptivo sobre el conjunto de datos \texttt{distancias.txt}.

<<ejecucion_principal, eval=TRUE, echo=TRUE, results=verbatim>>=
# ============================
# Script principal 
# ============================

# Se lee el archivo de datos
s <- read.table("distancias.txt", header = TRUE)
print(s)

# Contar filas y columnas manualmente
filas <- contar_filas(s$distancia)
columnas <- contar_columnas(s)
cat("Dimensiones (filas x columnas): ", filas, " x ", columnas, "\n", sep = "")

# Ordenación ascendente y descendente
idx_asc  <- ordenar_indices(s$distancia)
idx_desc <- ordenar_indices(s$distancia, decreasing = TRUE)
so_asc  <- s[idx_asc, ]
so_desc <- s[idx_desc, ]
cat("Orden ascendente por distancia:\n"); print(so_asc)
cat("\nOrden descendente por distancia:\n"); print(so_desc); cat("\n")

# Rango
rangor <- rango_manual(s$distancia)
cat("Rango (max - min):", rangor, "\n\n")

# Valores únicos y frecuencias
valores_unicos <- valores_unicos_manual(s$distancia)
n_valores <- contar_elementos(valores_unicos)
frecuencia_abs <- frecuencia_absoluta_manual(s$distancia, valores_unicos)
frecuencia_acum <- acumulada_manual(frecuencia_abs)
frecuencia_rel <- relativa_manual(frecuencia_abs, filas)
frecuencia_rel_acum <- acumulada_manual(frecuencia_rel)

# Mostrar frecuencias
cat("Frecuencia absoluta:\n")
for (i in 1:n_valores) cat(valores_unicos[i], ":", frecuencia_abs[i], "\n")
cat("\nFrecuencia acumulada:\n")
for (i in 1:n_valores) cat(valores_unicos[i], ":", frecuencia_acum[i], "\n")
cat("\nFrecuencia relativa:\n")
for (i in 1:n_valores) cat(valores_unicos[i], ":", frecuencia_rel[i], "\n")
cat("\nFrecuencia relativa acumulada:\n")
for (i in 1:n_valores) cat(valores_unicos[i], ":", frecuencia_rel_acum[i], "\n")

# Medidas descriptivas
media <- media_manual(s$distancia)
varianza_poblacional <- varianza_poblacional_manual(s$distancia)
varianza_muestral <- varianza_muestral_manual(s$distancia)
desv_est_poblacional <- desv_estandar(varianza_poblacional)
desv_est_muestral <- desv_estandar(varianza_muestral)

cat("\nMedia:", media, "\n")
cat("Varianza poblacional:", varianza_poblacional, "\n")
cat("Desviación estándar poblacional:", desv_est_poblacional, "\n")
cat("Varianza muestral:", varianza_muestral, "\n")
cat("Desviación estándar muestral:", desv_est_muestral, "\n")
@

\subsubsection*{AI-assisted development}

The following prompts were used during the development of this exercise to refine the logic of manual implementations, the documentation of each function, and the integration within the \texttt{Sweave} environment.  
All resulting code was subsequently reviewed, tested, and adapted.

\begin{itemize}
  \item \textbf{Prompt 1:}  
  \textit{"How can I implement a manual version of the \texttt{length()} function in R using a for loop?"}  
  Guided the creation of \texttt{contar\_filas()}, \texttt{contar\_columnas()}, and \texttt{contar\_elementos()}, each based on iterative counting.

  \item \textbf{Prompt 2:}  
  \textit{"Show me how to sort numeric data manually in R using nested loops (bubble sort) and return only the index order."}  
  Used to design \texttt{ordenar\_indices()}, which reproduces a bubble sort algorithm returning index positions.

  \item \textbf{Prompt 3:}  
  \textit{"Write an R function that calculates the range of a numeric vector without using \texttt{max()} or \texttt{min()}."}  
  Inspired the implementation of \texttt{rango\_manual()} through iterative comparison of values.

  \item \textbf{Prompt 4:}  
  \textit{"How can I manually find unique values in a numeric vector in R, without using \texttt{unique()}?"}  
  Helped define \texttt{valores\_unicos\_manual()}, combining sequential search and conditional appending.

  \item \textbf{Prompt 5:}  
  \textit{"Explain how to calculate absolute, cumulative, and relative frequencies manually in R using loops."}  
  Used to design \texttt{frecuencia\_absoluta\_manual()}, \texttt{acumulada\_manual()}, and \texttt{relativa\_manual()}, which reproduce frequency analysis logic step by step.

  \item \textbf{Prompt 6:}  
  \textit{"How can I organize multiple R functions with explanations inside a Sweave (.Rnw) document so that each function is displayed with comments but executed only once?"}  
  Clarified the correct use of chunk options (\texttt{eval=TRUE}, \texttt{echo=TRUE}, \texttt{results=hide}) for reproducible reports.

  \item \textbf{Prompt 7:}  
  \textit{"How can I integrate all the defined functions into a final executable section that prints descriptive statistics and frequency tables?"}  
  Used to design the final execution block combining all steps of the descriptive analysis.

  \item \textbf{Prompt 8:}  
  \textit{"How do I fix a Sweave error saying ‘results=markup not recognized’ when executing a chunk?"}  
  Guided the correction by replacing \texttt{results=markup} with \texttt{results=verbatim} for Sweave compatibility.
\end{itemize}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ejercicio 2.2: Fases del Algoritmo Apriori}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

El algoritmo se ha dividido en 7 fases lógicas, que se detallan a continuación.

\subsubsection{Fase 1: Carga de Datos y Sucesos Elementales Candidatos}

\subsubsection*{Explicación de la Fase}
[En esta primera fase el objetivo es ver que sucesos elementales de la muestra superan o igualan el umbral de soporte. 
Para ello se tiene que calcular el $n^o$ de veces que aparece dicho suceso elemental en la muestra, dividido por el núemro de 
elemento de la muestra. Los sucesos elementales que superen o igualen el umbral, son los que en la siguiente 
fase se utilizarán para crear los sucesos candidatos de las diferentes dimensiones. Antes de llevar a cabo estos pasos, se debe
cargar la muestra.]

\subsubsection*{Prompt de IA Utilizado}
\begin{quote}
[From the image i have just sent you i need to use a support threshold of 80% in order 
to decide which of the elements ("Faros de Xenon", "Alarma" and so on) go through and 
are going to be consider in the next step to start creating the candidates subsets. 
So in the image i have just sent you it is clear that i an identify the number of 
times each element appears, so how can i go through them all dividing the number 
of times they appear and dividing it by the amount of "sucesos"
that there are (there are 8) so that i get the percentages of the times they appear
for a subset of 8 elements and can discriminate which elements surpass the 0.5 threshold]
\end{quote}

\subsubsection*{Código y Ejecución (Sweave)}
<<Fase1_codigo, echo=TRUE, results=verbatim>>=
# 1ª FASE ----------------------------------------------------
library(Matrix)
library(arules)

muestra<-Matrix(c(1,1,1,1,0,0, 1,1,0,1,1,0, 1,1,1,0,0,0, 1,0,1,1,1,0, 1,1,0,1,0,0, 0,0,1,0,0,0, 1,1,0,1,0,0, 0,0,0,0,1,1), 8, 6, byrow=TRUE, dimnames = list(c("suceso1", "suceso2", "suceso3", "suceso4", "suceso5", "suceso6", "suceso7", "suceso8"), c("Faros de Xenon", "Control de Velocidad", "Navegador", "Bluetooth", "Techo Solar", "Alarma")),sparse=TRUE)

muestrangCMatrix<-as(muestra, "nsparseMatrix")
trapmuestrangCMatrix<-t(muestrangCMatrix)
transacciones<-as(trapmuestrangCMatrix, "transactions")

summary(transacciones)

frequency <- itemFrequency(transacciones, type ="absolute")
frequency

support_threshold <- 0.5
numero_sucesos <-length(transacciones)
support_sucesos <-frequency/numero_sucesos
support_sucesos

elementos_S_validos <- support_sucesos[support_sucesos >= support_threshold]
elementos_S_validos
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Fase 2: Generación de Sucesos Candidatos (L2, L3, L4)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Explicación de la Fase}
[En esta segunda fase, se van a construir los sucesos candidatos para cada una de las dimensiones necesarias en función del número
de sucesos candidatos elementales que superan el umbral de soporte en la primera fase. En este caso va a ser necesario
crear sucesos candidatos hasta la cuarta dimensión. Para crear estos sucesos candidatos de dimensión K, se deben utilizar sucesos
candidatos de la dimensión K-1 donde los últimos elementos de ambos sean diferentes y el resto de elemetos de ambos sean iguales
de forma que no haya conflicto a la hora de combinar ambos sucesos].

\subsubsection*{Prompt de IA Utilizado}
\begin{quote}
[Okey great lets continue on to the next step. In this next step i need you get
the candidate subsets that are going to form up to 4 dimmensions 
(because 4 elements passed the threshold). 

In case you do not know how this candidates subsets are formed, 
you need to start with the lowest dimmension possible other that K=1 and so 
for this dimmension and the follwing you need to have subsets in which the last 
element of the subset are different and the previous ones are exactly the same 
in order to combine them.

For example, for the dimmension k=2 we need to combine the subsets 
that passed the threshold in a way that the new subsets that are formed with 
to subsets of the k=1 dimmension have the last element different and the rest are the same.
In this dimmension because there are combinations of subsets of 1 elements you basically
need to have all possible combinations of the k=1 subsets .

But for the k=3 and so on you need to combine sets of k=2 o k=k-1 respectively
following the rules I told you so for example if we have {X,C} and {X,N} which
are from the k=2 dimmension then we can combine them because it follows the 
rules i mentioned obtaining anew subset of {X,C,N} for the dimmension k=3.

And for yout information the elements that passed the threshold were Faros de Xenon, 
Control de Velocidad, Navegador, Bluetooth.]
\end{quote}

\subsubsection*{Código y Ejecución (Sweave)}
<<Fase2_codigo, echo=TRUE, results=verbatim>>=
# 2ª FASE ----------------------------------------------------
L1 <- c("Faros de Xenon", "Control de Velocidad", "Navegador", "Bluetooth")
L1_lista <- lapply(L1, function(x) c(x))

generate_candidates <- function(L_prev) {
   n <- length(L_prev)
   if (n <= 1) return(list())     # nothing to join
   
   k <- length(L_prev[[1]]) + 1   # target candidate size
   
   # make sure items inside each subset are sorted
   L_prev <- lapply(L_prev, sort)
   candidates <- list()
   
   for (i in seq_len(n - 1)) {         # iterate all but last
     for (j in seq(i + 1, n)) {         # compare with following sets
       a <- L_prev[[i]]
       b <- L_prev[[j]]
       
       # join rule: all first (k-2) items identical
       if (k - 2 == 0 || all(a[1:(k - 2)] == b[1:(k - 2)])) {
         new_set <- sort(unique(c(a, b)))
         if (length(new_set) == k) {
           candidates <- append(candidates, list(new_set))
         }
       }
     }
   }
   
   # remove duplicates (if any)
   if (length(candidates) > 0) {
     candidates <- unique(lapply(candidates, function(x) paste(sort(x), collapse = ",")))
     candidates <- lapply(candidates, function(x) strsplit(x, ",")[[1]])
   }
   return(candidates)
}

L2 <- generate_candidates(L1_lista)
print("Candidatos L2:")
print(L2)

L3 <- generate_candidates(L2)
print("Candidatos L3:")
print(L3)

L4 <- generate_candidates(L3)
print("Candidatos L4:")
print(L4)
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Fase 3: Hash-Tree de sucesos candidatos para cada dimensión}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Explicación de la Fase}
[En esta fase se van a crear los hash-trees de los sucesos candidatos. Para llevar a cabo este paso se debe llevar a cabo una 
transformación numérica de los sucesos candidatos. Una vez se tenga hecho, se podrá crear un hash tree por 
cada dimensión (L2, L3, L4)]

\subsubsection*{Prompt de IA Utilizado}
\begin{quote}
[Okey great, thanks, lets move on to the next step, now we need to start working 
with tree structures, first you are going to focus on the candidates subsets, 
which first need to be transformed the words into numbers, using this transformation: 
\{ Faros de Xenon =1, Control de Velocidad = 2, Navegador =3, Bluetooth =4, Techo Solar =5,
Alarma =6). Once you transform the candidates to the numbers, this numbers must be sorted, 
for example if you have a subset that ends up being 243, it has to trasform to 234.]
\end{quote}

\subsubsection*{Código y Ejecución (Sweave)}
<<Fase3_codigo, echo=TRUE, results=verbatim>>=
# 3º FASE ----------------------------------------------------
item_map <- c(
   "Faros de Xenon" = 1,
   "Control de Velocidad" = 2,
   "Navegador" = 3,
   "Bluetooth" = 4,
   "Techo Solar" = 5,
   "Alarma" = 6
)

encode_candidates <- function(candidates, mapping) {
   lapply(candidates, function(subset) {
     nums <- unname(mapping[subset])     # map words → numbers
     sort(nums)
   })
}

L2_numerico <- encode_candidates(L2, item_map)
L3_numerico <- encode_candidates(L3, item_map)
L4_numerico <- encode_candidates(L4, item_map)

print("Candidatos L2 Numéricos:")
print(L2_numerico)
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Fase 4: Construcción de Hash Trees para cada suceso de la muestra para cada dimensión}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Explicación de la Fase}
[En esta fase se van a construir los hash trees de cada uno de los sucesos de la muestra para cada una de las dimensiones previas
(L2, L3, L4). Para llevar a cabo esta tarea se deben llevar a cabo los siguientes pasos. $1^o$ Se deben transformar los sucesos de la
muestra a los valores númericos de la misma forma que se hizo en la fase previa. $2^o$ Una vez se tengan los valores númericos se deben
crear todas las combinaciones de cada suceso de la muestra para cada una de las dimensiones (L2, L3, L4). $3^o$ Se deben crear
los hash-trees para cada dimensión a partir de todas las combinaciones posibles creadas.]

\subsubsection*{Prompt de IA Utilizado}
\begin{quote}
[okey, so the next step is to transform all the elementos of the sample to the numbers
following this mapping function, so that as we have done for the candidate
 subsets we can transform the sample into trees of k=2, 3 and 4 dimmensions .

The way of getting the subsets of those dimmensions if basically combining all possible
 ways the values of the sample subsets so that you get subsets of size =2, 3 or 4. 
Another critical thing is that you need to have the numbers sorted ascendingly so 
if for example one subset is {2,3,1} it has to be {1,2,3} before it gets transformed 
to a tree. So with that being said, this is the mapping:
\begin{verbatim}
item_map <- c( "Faros de Xenon" = 1, "Control de Velocidad" = 2, "Navegador" = 3,
"Bluetooth" = 4, "Techo Solar" = 5, "Alarma" = 6 )
\end{verbatim}
And this is sample that needs to be transformed first into numbers and then into subsets
of size = 2, 3 and 4 so that the trees of this samples are created:
\begin{verbatim}
muestra <- Matrix(c(1,1,1,1,0,0, 1,1,0,1,1,0, 1,1,1,0,0,0, 1,0,1,1,1,0, 1,1,0,1,0,0, 
0,0,1,0,0,0, 1,1,0,1,0,0, 0,0,0,0,1,1), nrow = 8, ncol = 6, byrow=TRUE, 
dimnames = list(c("suceso1", "suceso2", "suceso3", "suceso4", "suceso5", "suceso6", 
"suceso7", "suceso8"), c("Faros de Xenon", "Control de Velocidad", "Navegador", 
"Bluetooth", "Techo Solar", "Alarma")),sparse=TRUE).
\end{verbatim}
The trees need to be done for every level for every subset of the sample.]
\end{quote}

\subsubsection*{Código y Ejecución (Sweave)}
<<Fase4_codigo, echo=TRUE, results=verbatim>>=
# 4º FASE ----------------------------------------------------
all_candidates <- c(L2_numerico, L3_numerico, L4_numerico)

L2_candidatos <- Filter(function(x) length(x) == 2, all_candidates)
L3_candidatos <- Filter(function(x) length(x) == 3, all_candidates)
L4_candidatos <- Filter(function(x) length(x) == 4, all_candidates)



make_node <- function(item = NULL, is_end = FALSE) {
    # Create a new environment, not a list
    node <- new.env(parent = emptyenv()) 
    node$item <- item
    node$children <- list() # We can still use a list to hold the child envs
    node$is_end <- is_end
    node # Return the environment
}

insert_candidate <- function(root, candidate) {
    current <- root
    for (it in candidate) {
      key <- as.character(it)
      # create child if it doesn't exist
      if (!(key %in% names(current$children))) {
        current$children[[key]] <- make_node(item = it, is_end = FALSE)
      }
      # move to the child node
      current <- current$children[[key]]
    }
    # mark end of candidate
    current$is_end <- TRUE
    invisible(NULL)
}
 
 
build_tree <- function(candidates) {
    root <- make_node(item = NULL, is_end = FALSE)
    for (cand in candidates) {
      insert_candidate(root, cand)
    }
    root
}
 
 
 
 
print_tree <- function(node, depth = 0) {
    indent <- paste(rep("  ", depth), collapse = "")
    if (is.null(node$item)) {
      cat("(root)\n")
    } else {
      cat(indent, "- ", node$item, if (node$is_end) " [end]\n" else "\n", sep = "")
    }
    # iterate children in numeric order of keys
    if (length(node$children) > 0) {
      keys <- as.integer(names(node$children))
      keys <- sort(keys, na.last = TRUE)
      for (k in as.character(keys)) {
        print_tree(node$children[[k]], depth + 1)
      }
    }
}
 
 
arbol_L2_candidatos <- build_tree(L2_candidatos)
arbol_L3_candidatos <- build_tree(L3_candidatos)
arbol_L4_candidatos <- build_tree(L4_candidatos) 

print_tree(arbol_L2_candidatos)


# for the sample trees

transactions_num <- apply(muestra, 1, function(row) {
    items <- names(row[row == 1])
    nums <- unname(item_map[items])
    sort(nums)
})


# Function to generate subsets of a given size k
generate_subsets <- function(items, k) {
    if (length(items) < k) return(list())
    combn(items, k, simplify = FALSE)
}


transaction_subsets <- lapply(transactions_num, function(items) {
    list(
      C2 = generate_subsets(items, 2),
      C3 = generate_subsets(items, 3),
      C4 = generate_subsets(items, 4)
    )
})
 
all_trees <- lapply(transaction_subsets, function(subs) {
   list(
      tree_C2 = if (length(subs$C2) > 0) build_tree(subs$C2) else NULL,
      tree_C3 = if (length(subs$C3) > 0) build_tree(subs$C3) else NULL,
      tree_C4 = if (length(subs$C4) > 0) build_tree(subs$C4) else NULL
    )
  }) 
print_tree(all_trees$suceso1$tree_C2)
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Fase 5: Conteo sucesos candidatos para analizar la asociación de los que superen el umbral de soporte}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Explicación de la Fase}
[En esta fase se cuenta el número de veces que aparecen los sucesos candidatos en los árboles de los sucesos de la muestra. Este paso es importante ya 
que solo aquellos sucesos que superen el umbral de soporte serán considerados en la fase de análisis de asociación. De forma que se contabiliza si el 
$n^o$ de veces que aparecen los sucesos candidatos/$n^o$ de sucesos de la muestra es superior o igual al umbral de soporte]

\subsubsection*{Prompt de IA Utilizado}
\begin{quote}
[okey great, we can now move on to the next phase which consists on counting 
how many times do the leafs of the candidates tree in each level appear 
in each of the sample trees as this values need to be counted in order 
to see which of the candidates subsets move on to the next phase.

okey, great no that i have the supports for every tree subset, 
we need to filter by threshold that i have already defined previously 
support_threshold <- 0.5. The ones that pass the cut will pass on to 
the next phase.]
\end{quote}

\subsubsection*{Código y Ejecución (Sweave)}
<<Fase5_codigo, echo=TRUE, results=verbatim>>=
# 5º FASE ----------------------------------------------------
# (Incluimos las funciones de la Fase 4 que se necesitan aquí)


get_leaf_paths <- function(node, current_path = integer()) {
    paths <- list()
    if (!is.null(node$item)) {
      current_path <- c(current_path, node$item)
    }
    if (node$is_end) {
      paths <- append(paths, list(current_path))
    }
    if (length(node$children) > 0) {
      for (child in node$children) {
        paths <- append(paths, get_leaf_paths(child, current_path))
      }
    }
    paths
}

path_in_tree <- function(tree, path) {
    current <- tree
    for (item in path) {
      key <- as.character(item)
      if (!(key %in% names(current$children))) {
        return(FALSE)
      }
      current <- current$children[[key]]
    }
   return(TRUE)
}

count_supports <- function(candidate_tree, sample_trees_by_level) {
    candidate_paths <- get_leaf_paths(candidate_tree)
    supports <- numeric(length(candidate_paths))
    
    for (i in seq_along(candidate_paths)) {
      path <- candidate_paths[[i]]
      supports[i] <- sum(sapply(sample_trees_by_level, function(trees) {
        tree_level <- NULL
        k <- length(path)
        if (k == 2) tree_level <- trees$tree_C2
        else if (k == 3) tree_level <- trees$tree_C3
        else if (k == 4) tree_level <- trees$tree_C4
        if (is.null(tree_level)) return(FALSE)
        path_in_tree(tree_level, path)
      }))
    }
    data.frame(
      itemset = sapply(candidate_paths, function(p) paste(p, collapse = ",")),
      support = supports
    )
}

soporte_arbol_L2 <- count_supports(arbol_L2_candidatos, all_trees)
soporte_arbol_L3 <- count_supports(arbol_L3_candidatos, all_trees)
soporte_arbol_L4 <- count_supports(arbol_L4_candidatos, all_trees)

print("Soporte L2 (Antes de filtrar):")
print(soporte_arbol_L2)
print("Soporte L3 (Antes de filtrar):")
print(soporte_arbol_L3)
print("Soporte L4 (Antes de filtrar):")
print(soporte_arbol_L4)

filter_by_threshold <- function(support_df, threshold, total_tx) {
    support_df$support_ratio <- support_df$support / total_tx
    subset(support_df, support_ratio >= threshold)
}

L2_candidatos_post_arbol <- filter_by_threshold(soporte_arbol_L2, support_threshold, numero_sucesos)
L3_candidatos_post_arbol <- filter_by_threshold(soporte_arbol_L3, support_threshold, numero_sucesos)
L4_candidatos_post_arbol <- filter_by_threshold(soporte_arbol_L4, support_threshold, numero_sucesos)

print("Itemsets Frecuentes L2 (Post-Filtro):")
print(L2_candidatos_post_arbol)
print("Itemsets Frecuentes L3 (Post-Filtro):")
print(L3_candidatos_post_arbol)
print("Itemsets Frecuentes L4 (Post-Filtro):")
print(L4_candidatos_post_arbol)
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Fase 6: Generación de Reglas de Asociación}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Explicación de la Fase}
[En esta fase a partir de los sucesos candidatos que superaron el umbral de soporte se van a crear las diversas reglas de asociación
para cada uno de ellos. Para saber cuantas reglas de asociación se deben crear por cada suceso de cada dimensión, se debe considerar el cálculo de
$2^K-2$. De forma que si por ejemplo $K=3$, habrá 6 reglás de asociación que se deben crear y analizar. Para la creación de las reglas de asociación
se debe entender que se forman con la siguiente estructura $A \to B-A$, donde $B$ podría ser $\{Co, Li\}$ y $A$ sería $\{Co\}$ o $\{Li\}$.]

\subsubsection*{Prompt de IA Utilizado}
\begin{quote}
[Okay, we can move onto the next phase, no we have to create associations 
from every level (L2, L3, L4) . The amount of associations that can be 
created for each subset of each level follows the following formula: $2^k-2$. 
So for example for every subset of $k=2$, there are two possible associations. 

Before you start creating every association for each subset you have to reverse 
the numeric values for the words that we previously changed.]
\end{quote}

\subsubsection*{Código y Ejecución (Sweave)}
<<Fase6_codigo, echo=TRUE, results=verbatim>>=
# 6ª FASE ----------------------------------------------------
item_map_rev <- setNames(names(item_map), item_map)

generate_associations <- function(itemset) {
    k <- length(itemset)
    associations <- list()
    if (k < 2) return(associations)
 
   all_subsets <- unlist(lapply(1:(k - 1), function(i) combn(itemset, i, simplify = FALSE)), recursive = FALSE)
 
   for (lhs in all_subsets) {
     rhs <- setdiff(itemset, lhs)
     associations <- append(associations, list(list(lhs = lhs, rhs = rhs)))
   }
   associations
}

# Helper: convert "1,2,3" -> c(1,2,3)
split_itemset <- function(s) as.integer(unlist(strsplit(s, ",")))

create_associations_from_L <- function(L_df) {
    if (nrow(L_df) == 0) return(data.frame())
    
    all_associations <- list()
    for (i in seq_len(nrow(L_df))) {
      itemset <- split_itemset(L_df$itemset[i])
      associations <- generate_associations(itemset)
      all_associations <- append(all_associations, lapply(associations, function(r) {
        data.frame(
          lhs = paste(item_map_rev[as.character(sort(r$lhs))], collapse = ", "),
          rhs = paste(item_map_rev[as.character(sort(r$rhs))], collapse = ", "),
          k = length(itemset),
          support = L_df$support[i],
          support_ratio = L_df$support_ratio[i],
          stringsAsFactors = FALSE
        )
      }))
    }
    
   do.call(rbind, all_associations)
}

associaciones_L2 <- create_associations_from_L(L2_candidatos_post_arbol)
associaciones_L3 <- create_associations_from_L(L3_candidatos_post_arbol)
associaciones_L4 <- create_associations_from_L(L4_candidatos_post_arbol)

print("Reglas generadas desde L2:")
print(associaciones_L2)
print("Reglas generadas desde L3:")
print(associaciones_L3)
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Fase 7: Cálculo de Confianza y Filtrado Final}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection*{Explicación de la Fase}
[En esta fase final se va a realizar el cálculo de confianza de cada una de las reglas de asociación para así poder sacar conclusiones de la muestra con
cierto grado de confianza. Para llevar a cabo este cálculo se deberá dividir el $n^o$ de veces que aparece B en la muestra/$n^o$ de veces que aparece A en la muestra.
Si por ejemplo se tiene que $B=\{Co,Li\}$ y se tiene la siguiente regla de asociación $Co \to Li$ en este caso $A=Co$. Y esos son los conjuntos que se deben
contabilizar de la muestra. Aquellas reglas de asociación que superen dicho umbral de confianza, se podrá afirmar con el umbral de confianza que si sucede
una, la otra también sucede con ese grado de confianza.]

\subsubsection*{Prompt de IA Utilizado}
\begin{quote}
[so now we enter the last phase, we need to evaluate the confidence threshold 
for every association we created. In my case the confidence threshold will be of 
0.8 can you help me analyze this last phase for all the associations in every 
level please.

And also, do not incorporate prunning to the code.]
\end{quote}

\subsubsection*{Código y Ejecución (Sweave)}
<<Fase7_codigo, echo=TRUE, results=verbatim>>=
# 7ª FASE ----------------------------------------------------
confidence_threshold <- 0.8

convert_numeric_str_to_text_str <- function(num_str, map_rev) {
    nums_numeric <- split_itemset(num_str) 
    nums_char <- as.character(nums_numeric)
    names <- unname(map_rev[nums_char])
    paste(names, collapse = ", ") 
}

soporte_L1_map_df <- data.frame(
    lhs_str = names(frequency),
    lhs_support = as.integer(frequency),
    stringsAsFactors = FALSE
)

soporte_L2_map_df <- data.frame(
    lhs_str = sapply(soporte_arbol_L2$itemset, convert_numeric_str_to_text_str, map_rev = item_map_rev),
    lhs_support = soporte_arbol_L2$support,
    stringsAsFactors = FALSE
)

soporte_L3_map_df <- data.frame(
    lhs_str = sapply(soporte_arbol_L3$itemset, convert_numeric_str_to_text_str, map_rev = item_map_rev),
    lhs_support = soporte_arbol_L3$support,
    stringsAsFactors = FALSE
)

master_support_lookup <- rbind(soporte_L1_map_df, soporte_L2_map_df, soporte_L3_map_df)
master_support_lookup <- na.omit(master_support_lookup)

calculate_and_filter_confidence <- function(associations_df, support_lookup, threshold) {
    if (nrow(associations_df) == 0) {
      return(data.frame(
        lhs = character(),
        rhs = character(),
        k = integer(),
        support_ratio = numeric(),
        confidence = numeric(),
        stringsAsFactors = FALSE
      ))
    }
    
    merged_df <- merge(associations_df, support_lookup, by.x = "lhs", by.y = "lhs_str", all.x = TRUE)
    
    merged_df$lhs_support[is.na(merged_df$lhs_support)] <- 0
    
    merged_df$confidence <- merged_df$support / merged_df$support
    
    merged_df$confidence[is.nan(merged_df$confidence)] <- 0
    
    strong_rules <- merged_df[, c("lhs", "rhs", "k", "support_ratio", "confidence")]
    
    # Modificado: No filtrar por threshold aquí, solo ordenar
    strong_rules[order(-strong_rules$confidence), ]
}

reglas_fuertes_L2 <- calculate_and_filter_confidence(associaciones_L2, master_support_lookup, confidence_threshold)
reglas_fuertes_L3 <- calculate_and_filter_confidence(associaciones_L3, master_support_lookup, confidence_threshold)
reglas_fuertes_L4 <- calculate_and_filter_confidence(associaciones_L4, master_support_lookup, confidence_threshold)

# Filtrado final
reglas_finales_L2 <- subset(reglas_fuertes_L2, confidence >= confidence_threshold)
reglas_finales_L3 <- subset(reglas_fuertes_L3, confidence >= confidence_threshold)
reglas_finales_L4 <- subset(reglas_fuertes_L4, confidence >= confidence_threshold)

print("--- Reglas que superan el umbral de confianza (0.8) ---")
print("Reglas Finales L2:")
print(reglas_finales_L2)
print("Reglas Finales L3:")
print(reglas_finales_L3)
print("Reglas Finales L4:")
print(reglas_finales_L4)
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FIN DE LAS FASES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ejercicio 2.3: Detección de Datos Anómalos}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

En este ejercicio se implementaron tres métodos distintos para la detección de datos anómalos, utilizando técnicas estadísticas básicas.  
Se trabajó con dos variables: \texttt{Velocidad} y \texttt{Temperatura}.  

<<setup, echo=TRUE, message=FALSE>>=
# Creación del conjunto de datos
datos <- data.frame(
  Velocidad = c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5),
  Temperatura = c(7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73)
)

# Funciones auxiliares del ejercicio 2.1
contar_elementos <- function(x) {
  n <- 0
  for (._ in x) n <- n + 1
  n
}

media_manual <- function(x) {
  suma <- 0
  n <- contar_elementos(x)
  for (v in x) suma <- suma + v
  suma / n
}
@

\subsubsection{Medidas de ordenación (Velocidad), Método Caja y Bigotes}

En esta parte del trabajo se buscó identificar posibles valores atípicos en la variable \texttt{Velocidad}.  
Se calcularon los cinco números resumen (mínimo, Q1, mediana, Q3 y máximo) mediante la función \texttt{fivenum()}.  
Posteriormente, se aplicó la regla del rango intercuartílico (IQR) para detectar outliers, considerando un dato como atípico si cumple:
\[
x < Q_1 - 1.5 \times IQR \quad \text{o} \quad x > Q_3 + 1.5 \times IQR
\]. Finalmente, se iteró manualmente sobre todos los valores de Velocidad, comparando cada uno con estos límites para identificar y reportar aquellos que quedaban fuera del intervalo.

\subsubsection*{Prompt de IA Utilizado}

\begin{quote}
[I have the following dataset of Speed values: {10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5}. I want you to apply the box-and-whisker outlier detection method. To do this, follow these steps: 
sort the data, calculate the first and third quartiles, compute the interquartile range (IQR) using the formula provided earlier, determine the detection limits, and finally indicate which
 of the original Speed values fall outside this interval. You are not allowed to use: matrix, boxplot, quantile, length, mean, or sd.]
\end{quote}



<<ordenacion_velocidad, echo=TRUE, results=verbatim>>=
# Resumen de cinco números para Velocidad
resumen_vel <- fivenum(datos$Velocidad)
names(resumen_vel) <- c("Mínimo", "Q1", "Mediana", "Q3", "Máximo")
print(resumen_vel)

# Cálculo del IQR y límites
IQRv <- resumen_vel["Q3"] - resumen_vel["Q1"]
lim_inf_v <- resumen_vel["Q1"] - 1.5 * IQRv
lim_sup_v <- resumen_vel["Q3"] + 1.5 * IQRv

cat("\n--- OUTLIERS en VELOCIDAD ---\n")
i <- 1
for (v in datos$Velocidad) {
  if (v < lim_inf_v || v > lim_sup_v) {
    cat("Índice:", i, "→ Velocidad =", v, "es un outlier\n")
  }
  i <- i + 1
}
if (all(datos$Velocidad >= lim_inf_v & datos$Velocidad <= lim_sup_v)) {
  cat("No se detectaron outliers en velocidad.\n")
}
@



\subsubsection{Medidas de dispersión (Temperatura), Método Desviación Típica}

Se aplicó el método de la desviación típica para detectar valores atípicos en la variable \texttt{Temperatura}.  
Asumiendo distribución normal, se calculó la media y varianza de forma manual, y se definieron los límites inferior y superior como:
\[
L_i = \bar{x} - 2\sigma \quad ; \quad L_s = \bar{x} + 2\sigma
\]
Los valores fuera de este intervalo se consideraron atípicos.

\subsubsection*{Prompt de IA Utilizado}
\begin{quote}
[Now you have to do the same as before but with different data and another method. Analyze the following Temperature dataset: 
{7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73}. I want you to identify outliers using the standard deviation method. 
It is important to follow this procedure: calculate the arithmetic mean of all values; compute the standard deviation; then apply the 
outlier detection rule using the formulas: lower limit = mean(x) − d × standard deviation, and upper limit = mean(x) + d × standard deviation. 
The code must check the data and indicate which values are outliers because they do not meet this condition. You are not allowed to use: lm, summary, 
or length.]
\end{quote}

<<dispersión_temperatura, echo=TRUE, results=verbatim>>=
n <- contar_elementos(datos$Temperatura)
media_temp <- media_manual(datos$Temperatura)

# Varianza y desviación típica manuales
var_temp <- 0
for (t in datos$Temperatura) {
  var_temp <- var_temp + (t - media_temp)^2
}
var_temp <- var_temp / n
desv_temp <- sqrt(var_temp)

# Límites de outliers
lim_inf_t <- media_temp - 2 * desv_temp
lim_sup_t <- media_temp + 2 * desv_temp

cat("\n--- OUTLIERS en TEMPERATURA ---\n")
i <- 1
for (t in datos$Temperatura) {
  if (t < lim_inf_t || t > lim_sup_t) {
    cat("Índice:", i, "→ Temperatura =", t, "es un outlier\n")
  }
  i <- i + 1
}
@

\subsubsection{Detección por regresión lineal}

Finalmente, se utilizó un enfoque basado en regresión lineal entre \texttt{Velocidad} (variable independiente) y \texttt{Temperatura} (variable dependiente).  
Se calcularon las medias de Velocidad y Temperatura con las funciones del ejercicio 2.1. Después, se calcularon los coeficientes de la recta de regresión. Los cálculos se hicieron manualmente iterando sobre los datos, sin usar funciones R predefinidas como lm(). 
Una vez obtenida la recta, se calcularon los residuos. Para cada, se calculó la temperatura estimada y el residuo. Luego, se calculó la desviación típica de los residuos. Y se estableció el criterio de anomalía: un punto se considera un outlier si su residuo absoluto es mayor que dos veces el error estándar de los residuos. Por último, se iteró sobre los residuos para identificar los puntos que cumplían esta condición.

\subsubsection*{Prompt de IA Utilizado}

\begin{quote}
[Final exercise. We take the 11 pairs of data {Speed, Temperature}: {10, 7.46; 8, 6.77; 13, 12.74; 9, 7.11; 11, 7.81; 14, 8.84; 6, 6.08; 4, 5.39; 12, 8.15; 7, 6.42; 5, 5.73}. Perform an outlier analysis based on regression residuals, where Speed is the independent variable and Temperature is the dependent variable.
Please follow these steps manually, without using functions such as lm or summary: first, calculate the mean of Speed and Temperature; then obtain the linear regression coefficients a and b (b = Sxy / Sx²). For each of the 11 points, compute its residual and the standard deviation of the residuals. Finally, the code must indicate which points are outliers according to the condition |yi − ŷi| > d × sr”]
\end{quote}



<<regresion_outliers, echo=TRUE, results=verbatim>>=
x <- datos$Velocidad
y <- datos$Temperatura
media_x <- media_manual(x)
media_y <- media_manual(y)

# Cálculo manual de la pendiente y el intercepto
num <- 0
den <- 0
for (i in seq(contar_elementos(x))) {
  num <- num + (x[i] - media_x) * (y[i] - media_y)
  den <- den + (x[i] - media_x)^2
}
b1 <- num / den
b0 <- media_y - b1 * media_x

cat("\nEcuación de regresión manual:\n")
cat("Temperatura = ", round(b0, 4), " + ", round(b1, 4), " * Velocidad\n")

# Residuos y error estándar
residuos <- numeric(n)
for (i in seq(n)) {
  y_est <- b0 + b1 * x[i]
  residuos[i] <- y[i] - y_est
}

suma_res <- 0
for (r in residuos) suma_res <- suma_res + r^2
error_est <- sqrt(suma_res / n)
cat("\nError estándar de los residuos:", round(error_est, 4), "\n")

cat("\n--- OUTLIERS en REGRESIÓN ---\n")
i <- 1
for (r in residuos) {
  if (abs(r) > 2 * error_est) {
    cat("Índice:", i, "→ Residuo =", round(r, 4), "es un outlier\n")
  }
  i <- i + 1
}
@

\newpage







\section{Conclusiones}
[...AQUÍ VA TU TEXTO: Escribe tus conclusiones finales sobre el proyecto. Por ejemplo: "La implementación manual del algoritmo Apriori en R, aunque compleja, permite un entendimiento profundo de sus mecanismos internos, como la generación de candidatos y el conteo de soporte. El uso de Sweave ha sido fundamental para crear este documento reproducible..."]

% --- Bibliografía ---
\newpage
% \printbibliography % Descomenta esto para mostrar tu bibliografía

\end{document}